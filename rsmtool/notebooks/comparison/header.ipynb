{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting options for the plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats={'retina', 'svg'}\n",
    "%config InlineBackend.rc={'savefig.dpi': 150}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import base64\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from os.path import exists, join\n",
    "\n",
    "from IPython import sys_info\n",
    "from IPython.display import display, HTML, Image, Markdown, SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".alternate_colors3 tr:nth-of-type(3n+1) {background-color: #ffffff;}\n",
    ".alternate_colors3 tr:nth-of-type(3n+2){background-color: #dddddd;}\n",
    ".alternate_colors3 tr:nth-of-type(3n){background-color: #cccccc;}\n",
    "\n",
    ".alternate_colors3_groups tr:nth-of-type(6n+1){background-color: #ffffff;}\n",
    ".alternate_colors3_groups tr:nth-of-type(6n+2){background-color: #ffffff;}\n",
    ".alternate_colors3_groups tr:nth-of-type(6n+3){background-color: #ffffff;}\n",
    ".alternate_colors3_groups tr:nth-of-type(6n+4){background-color: #dddddd;}\n",
    ".alternate_colors3_groups tr:nth-of-type(6n+5){background-color: #dddddd;}\n",
    ".alternate_colors3_groups tr:nth-of-type(6n){background-color: #dddddd;}\n",
    "\n",
    ".alternate_colors2 tr:nth-of-type(2n+1){background-color: #ffffff;}\n",
    ".alternate_colors2 tr:nth-of-type(2n){background-color: #dddddd;}\n",
    "\n",
    "td, th {\n",
    "        padding-left: 5px;\n",
    "        padding-right: 5px;\n",
    "        border-top:0;\n",
    "        border-bottom: 0;\n",
    "    }\n",
    "\n",
    "table {\n",
    "    border: 0;\n",
    "    border-collapse: collapse;\n",
    "    text-align: right;\n",
    "    font-size: 11pt;\n",
    "}\n",
    "\n",
    ".chunk {\n",
    "    page-break-inside: avoid;\n",
    "    position: relative;\n",
    "    margin-top: 4em;\n",
    "}\n",
    "\n",
    "div.prompt.output_prompt { color: white; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOTE: you will need to set the following manually\n",
    "# if you are using this notebook interactively.\n",
    "experiment_id_old = os.environ.get('EXPERIMENT_ID_OLD')\n",
    "description_old = os.environ.get('DESCRIPTION_OLD')\n",
    "output_dir_old = os.environ.get('OUTPUT_DIR_OLD')\n",
    "figure_dir_old = os.environ.get('FIGURE_DIR_OLD')\n",
    "scaled_old = os.environ.get('SCALED_OLD')\n",
    "score_prefix_old = 'scale' if scaled_old == '1' else 'raw'\n",
    "\n",
    "experiment_id_new = os.environ.get('EXPERIMENT_ID_NEW')\n",
    "description_new = os.environ.get('DESCRIPTION_NEW')\n",
    "output_dir_new = os.environ.get('OUTPUT_DIR_NEW')\n",
    "figure_dir_new = os.environ.get('FIGURE_DIR_NEW')\n",
    "scaled_new = os.environ.get('SCALED_NEW')\n",
    "score_prefix_new = 'scale' if scaled_new == '1' else 'raw'\n",
    "\n",
    "# groups for subgroup analysis.\n",
    "# example: 'prompt%%subgroup1%%subgroup2' \n",
    "groups_desc_string = os.environ.get('GROUPS_FOR_DESCRIPTIVES') \n",
    "groups_desc = groups_desc_string.split('%%')\n",
    "groups_eval_string = os.environ.get('GROUPS_FOR_EVALUATIONS') \n",
    "groups_eval = groups_eval_string.split('%%')\n",
    "\n",
    "if len(groups_desc) == 1 and groups_desc[0] == '':\n",
    "    groups_desc = []\n",
    "\n",
    "if len(groups_eval) == 1 and groups_eval[0] == '':\n",
    "    groups_eval = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_strs = ['This report presents a comparison of the following two experiments']\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('  Old Experiment ID: **{}**'.format(experiment_id_old))\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('  Description: {}'.format(description_old))\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('  New Experiment ID: **{}**'.format(experiment_id_new))\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('  Description: {}'.format(description_new))\n",
    "markdown_strs.append('')\n",
    "Markdown('\\n'.join(markdown_strs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML(time.strftime('%c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_df_eval_columns_existing_raw = [\"N\", \"h_mean\", \"h_sd\", \n",
    "                                 \"sys_mean.raw_trim\", \n",
    "                                 \"sys_sd.raw_trim\", \n",
    "                                 \"corr.raw_trim\", \n",
    "                                 \"SMD.raw_trim\", \n",
    "                                 \"sys_mean.raw_trim_round\", \n",
    "                                 \"sys_sd.raw_trim_round\", \n",
    "                                 \"exact_agr.raw_trim_round\", \n",
    "                                 \"kappa.raw_trim_round\", \n",
    "                                 \"wtkappa.raw_trim_round\", \n",
    "                                 \"adj_agr.raw_trim_round\", \n",
    "                                 \"SMD.raw_trim_round\",\n",
    "                                 \"r2.raw_trim\",\n",
    "                                 \"RMSE.raw_trim\"] \n",
    "\n",
    "_df_eval_columns_existing_scale = [\"N\", \"h_mean\", \"h_sd\", \n",
    "                                   \"sys_mean.scale_trim\", \n",
    "                                   \"sys_sd.scale_trim\", \n",
    "                                   \"corr.scale_trim\", \n",
    "                                   \"SMD.scale_trim\", \n",
    "                                   \"sys_mean.scale_trim_round\", \n",
    "                                   \"sys_sd.scale_trim_round\", \n",
    "                                   \"exact_agr.scale_trim_round\", \n",
    "                                   \"kappa.scale_trim_round\", \n",
    "                                   \"wtkappa.scale_trim_round\", \n",
    "                                   \"adj_agr.scale_trim_round\", \n",
    "                                   \"SMD.scale_trim_round\",\n",
    "                                   \"r2.scale_trim\",\n",
    "                                   \"RMSE.scale_trim\"] \n",
    "\n",
    "\n",
    "_df_eval_columns_renamed = [\"N\", \"H1 mean\", \"H1 SD\", \n",
    "                            \"score mean(b)\", \n",
    "                            \"score SD(b)\", \n",
    "                            \"Pearson(b)\", \n",
    "                            \"SMD(b)\", \n",
    "                            \"score mean(br)\", \n",
    "                            \"score SD(br)\", \n",
    "                            \"Agmt.(br)\", \n",
    "                            \"K(br)\", \n",
    "                            \"QWK(br)\", \n",
    "                            \"Adj. Agmt.(br)\", \n",
    "                            \"SMD(br)\",\n",
    "                            \"R2(b)\",\n",
    "                            \"RMSE(b)\"]\n",
    "raw_renamedict = dict(zip(_df_eval_columns_existing_raw, _df_eval_columns_renamed))\n",
    "scale_renamedict = dict(zip(_df_eval_columns_existing_scale, _df_eval_columns_renamed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def float_format_func(x):\n",
    "    return '{:.3f}'.format(x)\n",
    " \n",
    "def int_or_float_format_func2(x):\n",
    "    if float.is_integer(x):\n",
    "        ans = '{}'.format(int(x))\n",
    "    else:\n",
    "        ans = '{:.2f}'.format(x)\n",
    "    return ans\n",
    "\n",
    "def int_or_float_format_func3(x):\n",
    "    if float.is_integer(x):\n",
    "        ans = '{}'.format(int(x))\n",
    "    else:\n",
    "        ans = '{:.3f}'.format(x)\n",
    "    return ans\n",
    "\n",
    "def corr_formatter2(x):\n",
    "    rx = '{:.2f}'.format(x)\n",
    "    ans = rx if x < 0.9 else '<span style=\"color: #FF0000\">{}</span>'.format(rx)\n",
    "    return ans\n",
    "\n",
    "def corr_formatter3(x):\n",
    "    rx = '{:.3f}'.format(x)\n",
    "    ans = rx if x < 0.9 else '<span style=\"color: #FF0000\">{}</span>'.format(rx)\n",
    "    return ans\n",
    "\n",
    "def factor_formatter3(x):\n",
    "    rx = '{:.3f}'.format(x)\n",
    "    ans = rx if abs(x) < 0.1 else '<span style=\"font-weight: bold;\">{}</span>'.format(rx)\n",
    "    return ans\n",
    "\n",
    "def make_summary_stat_df(df):\n",
    "    series = []\n",
    "    for summary_func in [np.mean, np.std, np.median, np.min, np.max]:\n",
    "        series.append(df.apply(summary_func))\n",
    "    res = pd.concat(series, axis=1)\n",
    "    res.columns = ['MEAN', 'SD', 'MEDIAN', 'MIN', 'MAX']\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_rsmtool_output(csvdir, figdir, experiment_id, prefix):\n",
    "\n",
    "    res = {}\n",
    "\n",
    "    # feature distributions and the inter-feature correlations\n",
    "    with open(join(figdir, '{}_distrib.svg'.format(experiment_id)), 'rb') as f:\n",
    "        res['feature_distplots'] = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "    res['df_feature_cors'] = pd.read_csv(join(csvdir, '{}_cors_processed.csv'.format(experiment_id)), index_col=0)\n",
    "        \n",
    "    # df_scores\n",
    "    df_scores = pd.read_csv(join(csvdir, '{}_pred_processed.csv'.format(experiment_id)),\n",
    "                        converters = {'spkitemid':str})\n",
    "\n",
    "    res['df_scores'] = df_scores[['spkitemid', 'sc1', prefix]]\n",
    "    \n",
    "    # model coefficients if present\n",
    "    betas_file = join(csvdir, '{}_betas.csv'.format(experiment_id))\n",
    "    if exists(betas_file):\n",
    "        res['df_coef'] = pd.read_csv(betas_file, index_col=0)\n",
    "        res['df_coef'].index.name = None\n",
    "    \n",
    "            \n",
    "    # read in the model fit files if present\n",
    "    model_fit_file = join(csvdir, '{}_model_fit.csv'.format(experiment_id))\n",
    "    if exists(model_fit_file):\n",
    "        res['df_model_fit'] = pd.read_csv(model_fit_file)\n",
    "\n",
    "    # human human agreement\n",
    "    consistency_file = join(csvdir, '{}_consistency.csv'.format(experiment_id))\n",
    "\n",
    "    # load if consistency file is present \n",
    "    if exists(consistency_file):\n",
    "        df_consistency = pd.read_csv(consistency_file, index_col=0)\n",
    "        res['df_consistency'] = df_consistency\n",
    "    \n",
    "    # degradation\n",
    "    degradation_file = join(csvdir, \"{}_degradation.csv\".format(experiment_id))\n",
    "\n",
    "    # load if degradation file is present\n",
    "    if exists(degradation_file):\n",
    "        df_degradation = pd.read_csv(degradation_file, index_col=0)\n",
    "        res['df_degradation'] = df_degradation\n",
    "        \n",
    "        # df_eval without renaming for degradation\n",
    "        df_eval_for_degradation = pd.read_csv(join(csvdir, \"{}_eval.csv\".format(experiment_id)), index_col = 0)\n",
    "        res['df_eval_for_degradation'] = df_eval_for_degradation\n",
    "        \n",
    "    # use the raw columns or the scale columns depending on the prefix\n",
    "    existing_eval_cols = _df_eval_columns_existing_raw if prefix == 'raw' else _df_eval_columns_existing_scale\n",
    "    renamedict = raw_renamedict if prefix == 'raw' else scale_renamedict\n",
    "\n",
    "    # read in the short version of the evaluation metrics for all data\n",
    "    short_metrics_list = [\"N\", \"Adj. Agmt.(br)\", \"Agmt.(br)\", \"K(br)\", \"Pearson(b)\", \"QWK(br)\", \"R2(b)\", \"RMSE(b)\"]\n",
    "    df_eval = pd.read_csv(join(csvdir, '{}_eval_short.csv'.format(experiment_id)), index_col=0)\n",
    "    df_eval = df_eval[existing_eval_cols]\n",
    "    df_eval = df_eval.rename(columns=renamedict)\n",
    "    res['df_eval'] = df_eval[short_metrics_list]\n",
    "    res['df_eval'].index.name = None\n",
    "    \n",
    "    # read in the evaluation metrics by subgroup, if we are asked to\n",
    "    for group in groups_eval:\n",
    "        df_eval = pd.read_csv(join(csvdir, '{}_eval_by_{}.csv'.format(experiment_id, group)), index_col=0)\n",
    "        df_eval = df_eval[existing_eval_cols]\n",
    "        df_eval = df_eval.rename(columns=renamedict)\n",
    "        res['df_eval_by_{}'.format(group)] = df_eval[short_metrics_list]\n",
    "        res['df_eval_by_{}'.format(group)].index.name = None\n",
    "        res['df_eval_by_{}_overview'.format(group)] = make_summary_stat_df(res['df_eval_by_{}'.format(group)])\n",
    "    \n",
    "        # set the ordering of mean/SD/SMD statistics \n",
    "        res['df_eval_by_{}_m_sd'.format(group)] = df_eval[['N', 'H1 mean', 'H1 SD', 'score mean(br)', 'score SD(br)', 'score mean(b)', 'score SD(b)', 'SMD(br)', 'SMD(b)']]\n",
    "        res['df_eval_by_{}_m_sd'.format(group)].index.name = None\n",
    "\n",
    "    # read in the partial correlations vs. score for all data\n",
    "    res['df_pcor_sc1'] = pd.read_csv(join(csvdir, '{}_pcor_score_all_data.csv'.format(experiment_id)), index_col=0)\n",
    "    res['df_pcor_sc1_overview'] = make_summary_stat_df(res['df_pcor_sc1'])\n",
    "    \n",
    "    # read in the partial correlations by subgroups, if we are asked to\n",
    "    for group in groups_eval:\n",
    "        res['df_pcor_sc1_by_{}'.format(group)] = pd.read_csv(join(csvdir, '{}_pcor_score_by_{}.csv'.format(experiment_id, group)), index_col=0)\n",
    "        res['df_pcor_sc1_{}_overview'.format(group)] = make_summary_stat_df(res['df_pcor_sc1_by_{}'.format(group)])\n",
    "\n",
    "    # read in the marginal correlations vs. score for all data\n",
    "    res['df_mcor_sc1'] = pd.read_csv(join(csvdir, '{}_margcor_score_all_data.csv'.format(experiment_id)), index_col=0)\n",
    "    res['df_mcor_sc1_overview'] = make_summary_stat_df(res['df_mcor_sc1'])\n",
    "\n",
    "    # read in the partial correlations by subgroups, if we are asked to\n",
    "    for group in groups_eval:\n",
    "        res['df_mcor_sc1_by_{}'.format(group)] = pd.read_csv(join(csvdir, '{}_margcor_score_by_{}.csv'.format(experiment_id, group)), index_col=0)\n",
    "        res['df_mcor_sc1_{}_overview'.format(group)] = make_summary_stat_df(res['df_mcor_sc1_by_{}'.format(group)])\n",
    "\n",
    "    res['df_pca'] = pd.read_csv(join(csvdir, '{}_pca.csv'.format(experiment_id)), index_col=0)\n",
    "    res['df_pcavar'] = pd.read_csv(join(csvdir, '{}_pcavar.csv'.format(experiment_id)), index_col=0)\n",
    "    res['df_descriptives'] = pd.read_csv(join(csvdir, '{}_feature_descriptives.csv'.format(experiment_id)), index_col=0)\n",
    "\n",
    "    # this df contains only the number of features. this is used later for another two tables to show the number of features\n",
    "    df_features_n_values = res['df_descriptives'][['N', 'min', 'max']]\n",
    "\n",
    "    res['df_descriptives'] = res['df_descriptives'][['N', 'mean', 'std. dev.', 'skewness', 'kurtosis']]\n",
    "\n",
    "    df_outliers = pd.read_csv(join(csvdir, '{}_feature_outliers.csv'.format(experiment_id)), index_col=0)\n",
    "    df_outliers = df_outliers.rename(columns={'upper': 'Upper',\n",
    "                                              'lower': 'Lower',\n",
    "                                              'both': 'Both',\n",
    "                                              'upperperc': 'Upper %',\n",
    "                                              'lowerperc': 'Lower %',\n",
    "                                              'bothperc': 'Both %'})\n",
    "    df_outliers_columns = df_outliers.columns.tolist()\n",
    "    res['df_outliers'] = df_outliers\n",
    "\n",
    "    # join with df_features_n_values to get the value of N\n",
    "    res['df_outliers'] = pd.merge(res['df_outliers'], df_features_n_values, left_index=True, right_index=True)[['N'] + df_outliers_columns]\n",
    "\n",
    "    # join with df_features_n_values to get the value of N\n",
    "    res['df_percentiles'] = pd.read_csv(join(csvdir, '{}_feature_descriptivesExtra.csv'.format(experiment_id)), index_col=0)\n",
    "    res['df_percentiles'] = pd.merge(res['df_percentiles'], df_features_n_values, left_index=True, right_index=True)\n",
    "\n",
    "    res['df_percentiles'][\"Mild outliers (%)\"] = res['df_percentiles'][\"Mild outliers\"]/res['df_percentiles'][\"N\"].astype(float)*100\n",
    "    res['df_percentiles'][\"Extreme outliers (%)\"] = res['df_percentiles'][\"Extreme outliers\"]/res['df_percentiles'][\"N\"].astype(float)*100\n",
    "\n",
    "    res['df_percentiles'] = res['df_percentiles'][['N', 'min', 'max', '1%', '5%', '25%', '50%', '75%', '95%', '99%', 'IQR', 'Mild outliers', 'Mild outliers (%)', 'Extreme outliers', 'Extreme outliers (%)']]\n",
    "\n",
    "    res['df_confmatrix'] = pd.read_csv(join(csvdir, '{}_confMatrix.csv'.format(experiment_id)), index_col=0)\n",
    "    confmatrix_size = res['df_confmatrix'].shape[0]\n",
    "    res['df_confmatrix'].index = ['machine {}'.format(n) for n in range(1, confmatrix_size + 1)]\n",
    "    res['df_confmatrix'].columns = ['human {}'.format(x) for x in range(1, confmatrix_size + 1)]\n",
    "\n",
    "    df_score_dist = pd.read_csv(join(csvdir, '{}_score_dist.csv'.format(experiment_id)), index_col=1)\n",
    "    df_score_dist.rename(columns={'sys_{}'.format(prefix): 'sys'}, inplace=True)\n",
    "    res['df_score_dist'] = df_score_dist[['human', 'sys', 'difference']]\n",
    "\n",
    "    # read in the feature boxplots by subgroup, if we were asked to\n",
    "    for group in groups_eval:\n",
    "        feature_boxplot_prefix = join(figdir, '{}_feature_boxplot_by_{}'.format(experiment_id, group))\n",
    "        svg_file = join(feature_boxplot_prefix + '.svg')\n",
    "        png_file = join(feature_boxplot_prefix + '.png')\n",
    "        if exists(svg_file):\n",
    "            with open(svg_file, 'rb') as f:\n",
    "                res['feature_boxplots_by_{}_svg'.format(group)] = base64.b64encode(f.read()).decode('utf-8')\n",
    "        elif exists(png_file):\n",
    "            with open(png_file, 'rb') as f:\n",
    "                res['feature_boxplots_by_{}_png'.format(group)] = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "    # read in the betas image if exists\n",
    "    betas_svg = join(figdir, '{}_betas.svg'.format(experiment_id))\n",
    "    if exists(betas_svg):\n",
    "        with open(betas_svg, 'rb') as f:\n",
    "            res['betas'] = base64.b64encode(f.read()).decode('utf-8')    \n",
    "\n",
    "    # read in the evaluation barplots by subgroup, if we were asked to\n",
    "    for group in groups_eval:\n",
    "        eval_barplot_svg_file = join(figdir, '{}_eval_by_{}.svg'.format(experiment_id, group))\n",
    "        with open(eval_barplot_svg_file, 'rb') as f:\n",
    "                res['eval_barplot_by_{}'.format(group)] = base64.b64encode(f.read()).decode('utf-8')\n",
    "        \n",
    "    with open(join(figdir, '{}_pca.svg'.format(experiment_id)), 'rb') as f:\n",
    "        res['pca_scree_plot'] = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the two sets of RSMTool outputs\n",
    "outputs_old = load_rsmtool_output(output_dir_old, figure_dir_old, experiment_id_old, prefix=score_prefix_old)\n",
    "outputs_new = load_rsmtool_output(output_dir_new, figure_dir_new, experiment_id_new, prefix=score_prefix_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_old_new_results(df_new, df_old, name):\n",
    "    \n",
    "    # combine the two dataframes and compute the difference\n",
    "    df_diff = df_new - df_old\n",
    "\n",
    "    # if the dataframe pertains to features, then add a fake column\n",
    "    # to the old dataframe if the feature was added and, conversely,\n",
    "    # to the new dataframe if the feature was removed.\n",
    "    if name in ['descriptives', 'outliers', 'percentiles', 'coefs', 'feature_cors', 'eval_by_prompt_overview', 'pcor_sc1_overview', 'mcor_sc1_overview', 'pcor_log_dta_dtu_overview']:\n",
    "        added_features = list(set(df_new.index).difference(df_old.index))\n",
    "        removed_features = list(set(df_old.index).difference(df_new.index))\n",
    "        for af in added_features:\n",
    "            df_old.loc[af] = '-'\n",
    "        for rf in removed_features:\n",
    "            df_new.loc[rf] = '-'\n",
    "\n",
    "    df_old['version'] = 'old'\n",
    "    df_new['version'] = 'new'\n",
    "    df_diff['version'] = 'change'\n",
    "\n",
    "    tmp_df = pd.DataFrame(df_old, copy=True)\n",
    "    tmp_df = tmp_df.append(df_new)\n",
    "    tmp_df = tmp_df.append(df_diff)\n",
    "    tmp_df.index.name = 'for'  \n",
    "    tmp_df = tmp_df.reset_index().sort_values(by=['for', 'version'], ascending=[True, False]).set_index(tmp_df.index.names)\n",
    "    tmp_df.index.name = None\n",
    "\n",
    "    # put version first\n",
    "    tmp_df = tmp_df[['version'] + [x for x in tmp_df.columns if x != 'version']]\n",
    "    return tmp_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_dfs = {}\n",
    "\n",
    "name_old_new = [('descriptives', outputs_old['df_descriptives'], outputs_new['df_descriptives']),\n",
    "                ('outliers', outputs_old['df_outliers'], outputs_new['df_outliers']),\n",
    "                ('feature_cors', outputs_old['df_feature_cors'], outputs_new['df_feature_cors']),\n",
    "                ('percentiles', outputs_old['df_percentiles'], outputs_new['df_percentiles']),\n",
    "                ('eval_overview', outputs_old['df_eval'], outputs_new['df_eval']),\n",
    "                ('mcor_sc1', outputs_old['df_mcor_sc1'], outputs_new['df_mcor_sc1']),\n",
    "                ('mcor_sc1_overview', outputs_old['df_mcor_sc1_overview'], outputs_new['df_mcor_sc1_overview']),\n",
    "                ('pcor_sc1', outputs_old['df_pcor_sc1'], outputs_new['df_pcor_sc1']),\n",
    "                ('pcor_sc1_overview', outputs_old['df_pcor_sc1_overview'], outputs_new['df_pcor_sc1_overview']),\n",
    "                ('score_dist', outputs_old['df_score_dist'], outputs_new['df_score_dist'])]\n",
    "\n",
    "# add the subgroup differences\n",
    "for group in groups_eval:\n",
    "    name_old_new.extend([('eval_by_{}'.format(group), outputs_old['df_eval_by_{}'.format(group)], outputs_new['df_eval_by_{}'.format(group)]),\n",
    "                         ('eval_by_{}_m_sd'.format(group), outputs_old['df_eval_by_{}_m_sd'.format(group)], outputs_new['df_eval_by_{}_m_sd'.format(group)]),\n",
    "                         ('eval_by_{}_overview'.format(group), outputs_old['df_eval_by_{}_overview'.format(group)], outputs_new['df_eval_by_{}_overview'.format(group)]),\n",
    "                         ('mcor_sc1_by_{}'.format(group), outputs_old['df_mcor_sc1_by_{}'.format(group)], outputs_new['df_mcor_sc1_by_{}'.format(group)]),\n",
    "                         ('pcor_sc1_by_{}'.format(group), outputs_old['df_pcor_sc1_by_{}'.format(group)], outputs_new['df_pcor_sc1_by_{}'.format(group)])])\n",
    "for name, df_old, df_new in name_old_new:\n",
    "\n",
    "    out_dfs[name] = combine_old_new_results(df_old, df_new, name)\n",
    "    \n",
    "# WARN IF THE OLD AND NEW DATASETS ARE NOT THE SAME SIZE\n",
    "log_msgs = []\n",
    "oldTrainN = outputs_old['df_descriptives']['N'][0]  # take the N from the descriptive stats for the first feature\n",
    "newTrainN = outputs_new['df_descriptives']['N'][0]\n",
    "if oldTrainN != newTrainN:\n",
    "    log_msg = \"WARNING: the training sets were different sizes.  old N: {}, new N: {}.\".format(oldTrainN, newTrainN)\n",
    "    log_msgs.append(log_msg)\n",
    "\n",
    "if 'prompt' in groups_eval:\n",
    "    oldTestN = np.sum(outputs_old['df_eval_by_prompt']['N'])  # sum N across prompts\n",
    "    newTestN = np.sum(outputs_new['df_eval_by_prompt']['N'])\n",
    "    if not np.all(oldTestN == newTestN):\n",
    "        log_msg = \"WARNING: the testing sets were different sizes.  old N: {}, new N: {}.\".format(oldTestN, newTestN)\n",
    "        log_msgs.append(log_msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
