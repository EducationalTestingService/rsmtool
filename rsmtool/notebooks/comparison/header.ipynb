{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting options for the plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats={'retina', 'svg'}\n",
    "%config InlineBackend.rc={'savefig.dpi': 150}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import base64\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import platform\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",

    "from functools import partial\n",
    "from os.path import abspath, exists, join\n",
    "\n",
    "from IPython import sys_info\n",
    "from IPython.display import display, HTML, Image, Javascript, Markdown, SVG\n",
    "\n",
    "from rsmtool.comparison import load_rsmtool_output\n",
    "from rsmtool.version import VERSION as rsmtool_version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "  .alternate_colors3 tr:nth-of-type(3n+1) {background-color: #ffffff;}\n",
    "  .alternate_colors3 tr:nth-of-type(3n+2){background-color: #dddddd;}\n",
    "  .alternate_colors3 tr:nth-of-type(3n){background-color: #cccccc;}\n",
    "\n",
    "  .alternate_colors3_groups tr:nth-of-type(6n+1){background-color: #ffffff;}\n",
    "  .alternate_colors3_groups tr:nth-of-type(6n+2){background-color: #ffffff;}\n",
    "  .alternate_colors3_groups tr:nth-of-type(6n+3){background-color: #ffffff;}\n",
    "  .alternate_colors3_groups tr:nth-of-type(6n+4){background-color: #dddddd;}\n",
    "  .alternate_colors3_groups tr:nth-of-type(6n+5){background-color: #dddddd;}\n",
    "  .alternate_colors3_groups tr:nth-of-type(6n){background-color: #dddddd;}\n",
    "\n",
    "  .alternate_colors2 tr:nth-of-type(2n+1){background-color: #ffffff;}\n",
    "  .alternate_colors2 tr:nth-of-type(2n){background-color: #dddddd;}\n",
    "\n",
    "  div.prompt.output_prompt { \n",
    "    color: white; \n",
    "  }\n",
    "  span.highlight_color {\n",
    "    color: red;\n",
    "  }\n",
    "    \n",
    "  @media print {\n",
    "    @page {\n",
    "      size: landscape;\n",
    "      margin: 0cm 0cm 0cm 0cm;\n",
    "      font-size: 12px;\n",
    "    }\n",
    "\n",
    "    * {\n",
    "      margin: 0px;\n",
    "      padding: 0px;\n",
    "    }\n",
    "\n",
    "    #toc {\n",
    "      display: none;\n",
    "    }\n",
    "    \n",
    "    span.highlight_color {\n",
    "        font-weight: bolder;\n",
    "        text-decoration: underline;\n",
    "    }\n",
    "    div.prompt.output_prompt {\n",
    "      display: none;\n",
    "    }\n",
    "\n",
    "    h3#Python-packages, div#packages {\n",
    "      display: none;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOTE: you will need to set the following manually\n",
    "# if you are using this notebook interactively.\n",
    "experiment_id_old = os.environ.get('EXPERIMENT_ID_OLD')\n",
    "description_old = os.environ.get('DESCRIPTION_OLD')\n",
    "output_dir_old = os.environ.get('OUTPUT_DIR_OLD')\n",
    "figure_dir_old = os.environ.get('FIGURE_DIR_OLD')\n",
    "scaled_old = os.environ.get('SCALED_OLD')\n",
    "score_prefix_old = 'scale' if scaled_old == '1' else 'raw'\n",
    "\n",
    "experiment_id_new = os.environ.get('EXPERIMENT_ID_NEW')\n",
    "description_new = os.environ.get('DESCRIPTION_NEW')\n",
    "output_dir_new = os.environ.get('OUTPUT_DIR_NEW')\n",
    "figure_dir_new = os.environ.get('FIGURE_DIR_NEW')\n",
    "scaled_new = os.environ.get('SCALED_NEW')\n",
    "score_prefix_new = 'scale' if scaled_new == '1' else 'raw'\n",
    "\n",
    "# javascript path\n",
    "javascript_path = os.environ.get(\"JAVASCRIPT_PATH\")\n",
    "\n",
    "# groups for subgroup analysis.\n",
    "# example: 'prompt%%subgroup1%%subgroup2' \n",
    "groups_desc_string = os.environ.get('GROUPS_FOR_DESCRIPTIVES') \n",
    "groups_desc = groups_desc_string.split('%%')\n",
    "groups_eval_string = os.environ.get('GROUPS_FOR_EVALUATIONS') \n",
    "groups_eval = groups_eval_string.split('%%')\n",
    "\n",
    "if len(groups_desc) == 1 and groups_desc[0] == '':\n",
    "    groups_desc = []\n",
    "\n",
    "if len(groups_eval) == 1 and groups_eval[0] == '':\n",
    "    groups_eval = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(Javascript(filename=join(javascript_path, \"sort.js\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_strs = ['This report presents a comparison of the following two experiments']\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('  Old Experiment ID: **{}**'.format(experiment_id_old))\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('  Description: {}'.format(description_old))\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('  New Experiment ID: **{}**'.format(experiment_id_new))\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('  Description: {}'.format(description_new))\n",
    "markdown_strs.append('')\n",
    "Markdown('\\n'.join(markdown_strs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML(time.strftime('%c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def float_format_func(x):\n",
    "    return '{:.3f}'.format(x)\n",
    " \n",
    "def int_or_float_format_func2(x):\n",
    "    if float.is_integer(x):\n",
    "        ans = '{}'.format(int(x))\n",
    "    else:\n",
    "        ans = '{:.2f}'.format(x)\n",
    "    return ans\n",
    "\n",
    "def int_or_float_format_func3(x):\n",
    "    if float.is_integer(x):\n",
    "        ans = '{}'.format(int(x))\n",
    "    else:\n",
    "        ans = '{:.3f}'.format(x)\n",
    "    return ans\n",
    "\n",
    "def corr_formatter2(x):\n",
    "    rx = '{:.2f}'.format(x)\n",
    "    ans = rx if x < 0.9 else '<span style=\"color: #FF0000\">{}</span>'.format(rx)\n",
    "    return ans\n",
    "\n",
    "def corr_formatter3(x):\n",
    "    rx = '{:.3f}'.format(x)\n",
    "    ans = rx if x < 0.9 else '<span style=\"color: #FF0000\">{}</span>'.format(rx)\n",
    "    return ans\n",
    "\n",
    "def factor_formatter3(x):\n",
    "    rx = '{:.3f}'.format(x)\n",
    "    ans = rx if abs(x) < 0.1 else '<span style=\"font-weight: bold;\">{}</span>'.format(rx)\n",
    "    return ans\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the two sets of RSMTool outputs\n",
    "(outputs_old, figures_old) = load_rsmtool_output(output_dir_old, \n",
    "                                                 figure_dir_old,\n",
    "                                                 experiment_id_old,\n",
    "                                                 prefix=score_prefix_old,\n",
    "                                                 groups_eval=groups_eval)\n",
    "(outputs_new, figures_new) = load_rsmtool_output(output_dir_new,\n",
    "                                                 figure_dir_new,\n",
    "                                                 experiment_id_new,\n",
    "                                                 prefix=score_prefix_new,\n",
    "                                                 groups_eval=groups_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_old_new_results(df_old, df_new, name):\n",
    "    \n",
    "    # check that both data frames are not empty and return an empty data frame if this is the case\n",
    "    if df_old.empty and df_new.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # if only one data frame is empty, keep the one that exists and substitute '-' for missing data\n",
    "    elif df_old.empty:\n",
    "        df_old = pd.DataFrame(columns=df_new.columns, index=df_new.index, data='-')\n",
    "        df_diff = pd.DataFrame(columns=df_new.columns, index=df_new.index, data='-')\n",
    "    \n",
    "    elif df_new.empty:\n",
    "        df_new = pd.DataFrame(columns=df_old.columns, index=df_old.index, data='-')\n",
    "        df_diff = pd.DataFrame(columns=df_old.columns, index=df_old.index, data='-')\n",
    "    \n",
    "    else:\n",
    "        # combine the two dataframes and compute the difference\n",
    "        df_diff = df_new - df_old\n",
    "\n",
    "        # if the dataframe pertains to features, then add a fake column\n",
    "        # to the old dataframe if the feature was added and, conversely,\n",
    "        # to the new dataframe if the feature was removed.\n",
    "        if name in ['descriptives', 'outliers', 'percentiles', 'coefs', 'feature_cors', 'eval_by_prompt_overview', 'pcor_sc1_overview', 'mcor_sc1_overview', 'pcor_log_dta_dtu_overview']:\n",
    "            added_features = list(set(df_new.index).difference(df_old.index))\n",
    "            removed_features = list(set(df_old.index).difference(df_new.index))\n",
    "            for af in added_features:\n",
    "                df_old.loc[af] = '-'\n",
    "            for rf in removed_features:\n",
    "                df_new.loc[rf] = '-'\n",
    "\n",
    "    df_old['version'] = 'old'\n",
    "    df_new['version'] = 'new'\n",
    "    df_diff['version'] = 'change'\n",
    "\n",
    "    tmp_df = pd.DataFrame(df_old, copy=True)\n",
    "    tmp_df = tmp_df.append(df_new)\n",
    "    tmp_df = tmp_df.append(df_diff)\n",
    "    tmp_df.index.name = 'for'  \n",
    "    tmp_df = tmp_df.reset_index().sort_values(by=['for', 'version'], ascending=[True, False]).set_index(tmp_df.index.names)\n",
    "    tmp_df.index.name = None\n",
    "\n",
    "    # put version first\n",
    "    tmp_df = tmp_df[['version'] + [x for x in tmp_df.columns if x != 'version']]\n",
    "    return tmp_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_dfs = {}\n",
    "\n",
    "name_old_new = [('descriptives', outputs_old['df_descriptives'], outputs_new['df_descriptives']),\n",
    "                ('outliers', outputs_old['df_outliers'], outputs_new['df_outliers']),\n",
    "                ('feature_cors', outputs_old['df_feature_cors'], outputs_new['df_feature_cors']),\n",
    "                ('percentiles', outputs_old['df_percentiles'], outputs_new['df_percentiles']),\n",
    "                ('eval_overview', outputs_old['df_eval'], outputs_new['df_eval']),\n",
    "                ('mcor_sc1', outputs_old['df_mcor_sc1'], outputs_new['df_mcor_sc1']),\n",
    "                ('mcor_sc1_overview', outputs_old['df_mcor_sc1_overview'], outputs_new['df_mcor_sc1_overview']),\n",
    "                ('pcor_sc1', outputs_old['df_pcor_sc1'], outputs_new['df_pcor_sc1']),\n",
    "                ('pcor_sc1_overview', outputs_old['df_pcor_sc1_overview'], outputs_new['df_pcor_sc1_overview']),\n",
    "                ('score_dist', outputs_old['df_score_dist'], outputs_new['df_score_dist']),\n",
    "                ('consistency', outputs_old['df_consistency'], outputs_new['df_consistency'])]\n",
    "\n",
    "# add the subgroup differences\n",
    "for group in groups_eval:\n",
    "    name_old_new.extend([('eval_by_{}'.format(group), outputs_old['df_eval_by_{}'.format(group)], outputs_new['df_eval_by_{}'.format(group)]),\n",
    "                         ('eval_by_{}_m_sd'.format(group), outputs_old['df_eval_by_{}_m_sd'.format(group)], outputs_new['df_eval_by_{}_m_sd'.format(group)]),\n",
    "                         ('eval_by_{}_overview'.format(group), outputs_old['df_eval_by_{}_overview'.format(group)], outputs_new['df_eval_by_{}_overview'.format(group)]),\n",
    "                         ('mcor_sc1_by_{}'.format(group), outputs_old['df_mcor_sc1_by_{}'.format(group)], outputs_new['df_mcor_sc1_by_{}'.format(group)]),\n",
    "                         ('pcor_sc1_by_{}'.format(group), outputs_old['df_pcor_sc1_by_{}'.format(group)], outputs_new['df_pcor_sc1_by_{}'.format(group)])])\n",
    "\n",
    "# combine the old and new data and compute the difference\n",
    "for name, df_old, df_new in name_old_new:\n",
    "    out_dfs[name] = combine_old_new_results(df_old, df_new, name)\n",
    "\n",
    "    \n",
    "# define the message for missing information\n",
    "no_info_str = 'This information is not available for either of the models.'\n",
    "no_plot_old_str = 'This figure is not available for the old model.'\n",
    "no_plot_new_str = 'This figure is not available for the new model.'\n",
    "    \n",
    "# WARN IF THE OLD AND NEW DATASETS ARE NOT THE SAME SIZE\n",
    "\n",
    "log_msgs = []\n",
    "# first check the training set\n",
    "if not (outputs_old['df_descriptives'].empty or outputs_new['df_descriptives'].empty):\n",
    "    oldTrainN = outputs_old['df_descriptives']['N'][0]  # take the N from the descriptive stats for the first feature\n",
    "    newTrainN = outputs_new['df_descriptives']['N'][0]\n",
    "    if oldTrainN != newTrainN:\n",
    "        log_msg = \"WARNING: the training sets were different sizes.  old N: {}, new N: {}.\".format(oldTrainN, newTrainN)\n",
    "        log_msgs.append(log_msg)\n",
    "\n",
    "# the check the test set\n",
    "\n",
    "if not (outputs_old['df_eval'].empty or outputs_new['df_eval'].empty): \n",
    "    oldTestN = outputs_old['df_eval']['N']  \n",
    "    newTestN = outputs_new['df_eval']['N']\n",
    "    if not np.all(oldTestN == newTestN):\n",
    "        log_msg = \"WARNING: the testing sets were different sizes.  old N: {}, new N: {}.\".format(oldTestN, newTestN)\n",
    "        log_msgs.append(log_msg)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
