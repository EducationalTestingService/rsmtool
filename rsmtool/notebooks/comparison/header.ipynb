{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting options for the plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats={'retina', 'svg'}\n",
    "%config InlineBackend.rc={'savefig.dpi': 150}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import base64\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import platform\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from os.path import abspath, exists, join\n",
    "\n",
    "from IPython import sys_info\n",
    "from IPython.display import display, HTML, Image, Javascript, Markdown, SVG\n",
    "\n",
    "from rsmtool.comparer import Comparer\n",
    "\n",
    "from rsmtool.reader import DataReader\n",
    "from rsmtool.writer import DataWriter\n",
    "\n",
    "from rsmtool.utils import (float_format_func,\n",
    "                           int_or_float_format_func,\n",
    "                           parse_json_with_comments,\n",
    "                           bold_highlighter,\n",
    "                           color_highlighter,\n",
    "                           show_thumbnail)\n",
    "\n",
    "from rsmtool.version import VERSION as rsmtool_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_report_dir = os.environ.get('RSM_REPORT_DIR', None)\n",
    "if rsm_report_dir is None:\n",
    "    rsm_report_dir = os.getcwd()\n",
    "\n",
    "rsm_environ_config = join(rsm_report_dir, '.environ.json')\n",
    "if not exists(rsm_environ_config):\n",
    "    raise FileNotFoundError('The file {} cannot be located. '\n",
    "                            'Please make sure that either (1) '\n",
    "                            'you have set the correct directory with the `RSM_REPORT_DIR` '\n",
    "                            'environment variable, or (2) that your `.environ.json` '\n",
    "                            'file is in the same directory as your notebook.'.format(rsm_environ_config))\n",
    "    \n",
    "environ_config = parse_json_with_comments(rsm_environ_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "  .alternate_colors3 tr:nth-of-type(3n+1) {background-color: #ffffff;}\n",
    "  .alternate_colors3 tr:nth-of-type(3n+2){background-color: #dddddd;}\n",
    "  .alternate_colors3 tr:nth-of-type(3n){background-color: #cccccc;}\n",
    "\n",
    "  .alternate_colors3_groups tr:nth-of-type(6n+1){background-color: #ffffff;}\n",
    "  .alternate_colors3_groups tr:nth-of-type(6n+2){background-color: #ffffff;}\n",
    "  .alternate_colors3_groups tr:nth-of-type(6n+3){background-color: #ffffff;}\n",
    "  .alternate_colors3_groups tr:nth-of-type(6n+4){background-color: #dddddd;}\n",
    "  .alternate_colors3_groups tr:nth-of-type(6n+5){background-color: #dddddd;}\n",
    "  .alternate_colors3_groups tr:nth-of-type(6n){background-color: #dddddd;}\n",
    "\n",
    "  .alternate_colors2 tr:nth-of-type(2n+1){background-color: #ffffff;}\n",
    "  .alternate_colors2 tr:nth-of-type(2n){background-color: #dddddd;}\n",
    "\n",
    "  div.prompt.output_prompt { \n",
    "    color: white; \n",
    "  }\n",
    "  span.highlight_color {\n",
    "    color: red;\n",
    "  }\n",
    "    \n",
    "  @media print {\n",
    "    @page {\n",
    "      size: landscape;\n",
    "      margin: 0cm 0cm 0cm 0cm;\n",
    "      font-size: 12px;\n",
    "    }\n",
    "\n",
    "    * {\n",
    "      margin: 0px;\n",
    "      padding: 0px;\n",
    "    }\n",
    "\n",
    "    #toc {\n",
    "      display: none;\n",
    "    }\n",
    "    \n",
    "    span.highlight_color {\n",
    "        font-weight: bolder;\n",
    "        text-decoration: underline;\n",
    "    }\n",
    "    div.prompt.output_prompt {\n",
    "      display: none;\n",
    "    }\n",
    "\n",
    "    h3#Python-packages, div#packages {\n",
    "      display: none;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: you will need to set the following manually\n",
    "# if you are using this notebook interactively.\n",
    "experiment_id_old = environ_config.get('EXPERIMENT_ID_OLD')\n",
    "description_old = environ_config.get('DESCRIPTION_OLD')\n",
    "output_dir_old = environ_config.get('OUTPUT_DIR_OLD')\n",
    "figure_dir_old = environ_config.get('FIGURE_DIR_OLD')\n",
    "scaled_old = environ_config.get('SCALED_OLD')\n",
    "score_prefix_old = 'scale' if scaled_old else 'raw'\n",
    "use_thumbnails = environ_config.get('USE_THUMBNAILS')\n",
    "\n",
    "experiment_id_new = environ_config.get('EXPERIMENT_ID_NEW')\n",
    "description_new = environ_config.get('DESCRIPTION_NEW')\n",
    "output_dir_new = environ_config.get('OUTPUT_DIR_NEW')\n",
    "figure_dir_new = environ_config.get('FIGURE_DIR_NEW')\n",
    "scaled_new = environ_config.get('SCALED_NEW')\n",
    "score_prefix_new = 'scale' if scaled_new else 'raw'\n",
    "\n",
    "# javascript path\n",
    "javascript_path = environ_config.get(\"JAVASCRIPT_PATH\")\n",
    "\n",
    "# groups for subgroup analysis.\n",
    "groups_desc = environ_config.get('GROUPS_FOR_DESCRIPTIVES') \n",
    "groups_eval = environ_config.get('GROUPS_FOR_EVALUATIONS') \n",
    "\n",
    "if len(groups_desc) == 1 and groups_desc[0] == '':\n",
    "    groups_desc = []\n",
    "\n",
    "if len(groups_eval) == 1 and groups_eval[0] == '':\n",
    "    groups_eval = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize id generator for thumbnails\n",
    "id_generator = itertools.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(join(javascript_path, \"sort.js\"), \"r\", encoding=\"utf-8\") as sortf:\n",
    "    display(Javascript(data=sortf.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_strs = ['This report presents a comparison of the following two experiments']\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('  Old Experiment ID: **{}**'.format(experiment_id_old))\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('  Description: {}'.format(description_old))\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('  New Experiment ID: **{}**'.format(experiment_id_new))\n",
    "markdown_strs.append('')\n",
    "markdown_strs.append('  Description: {}'.format(description_new))\n",
    "markdown_strs.append('')\n",
    "Markdown('\\n'.join(markdown_strs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_thumbnails:\n",
    "    display(Markdown(\"\"\"***Note: Images in this report have been converted to \"\"\"\n",
    "                     \"\"\"clickable thumbnails***\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML(time.strftime('%c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the two sets of RSMTool outputs\n",
    "comparer = Comparer()\n",
    "\n",
    "(outputs_old, figures_old,\n",
    " file_format_old) = comparer.load_rsmtool_output(output_dir_old, \n",
    "                                                 figure_dir_old,\n",
    "                                                 experiment_id_old,\n",
    "                                                 prefix=score_prefix_old,\n",
    "                                                 groups_eval=groups_eval)\n",
    "(outputs_new, figures_new,\n",
    " file_format_new) = comparer.load_rsmtool_output(output_dir_new,\n",
    "                                                 figure_dir_new,\n",
    "                                                 experiment_id_new,\n",
    "                                                 prefix=score_prefix_new,\n",
    "                                                 groups_eval=groups_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_old_new_results(df_old, df_new, name):\n",
    "    \n",
    "    # check that both data frames are not empty and return an empty data frame if this is the case\n",
    "    if df_old.empty and df_new.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # if only one data frame is empty, keep the one that exists and substitute '-' for missing data\n",
    "    elif df_old.empty:\n",
    "        df_old = pd.DataFrame(columns=df_new.columns, index=df_new.index, data='-')\n",
    "        df_diff = pd.DataFrame(columns=df_new.columns, index=df_new.index, data='-')\n",
    "    \n",
    "    elif df_new.empty:\n",
    "        df_new = pd.DataFrame(columns=df_old.columns, index=df_old.index, data='-')\n",
    "        df_diff = pd.DataFrame(columns=df_old.columns, index=df_old.index, data='-')\n",
    "    \n",
    "    else:\n",
    "        # combine the two dataframes and compute the difference\n",
    "        df_diff = df_new - df_old\n",
    "\n",
    "        # if the dataframe pertains to features or scores, then add a fake column\n",
    "        # to the old dataframe if the feature was added and, conversely,\n",
    "        # to the new dataframe if the feature was removed.\n",
    "        if name in ['descriptives',\n",
    "                    'outliers',\n",
    "                    'percentiles',\n",
    "                    'coefs',\n",
    "                    'feature_cors',\n",
    "                    'eval_by_prompt_overview',\n",
    "                    'pcor_sc1_overview',\n",
    "                    'mcor_sc1_overview',\n",
    "                    'pcor_log_dta_dtu_overview',\n",
    "                    'score_dist']:\n",
    "            added_features_or_scores = list(set(df_new.index).difference(df_old.index))\n",
    "            removed_features_or_scores = list(set(df_old.index).difference(df_new.index))\n",
    "            for afs in added_features_or_scores:\n",
    "                df_old.loc[afs] = '-'\n",
    "            for rfs in removed_features_or_scores:\n",
    "                df_new.loc[rfs] = '-'\n",
    "\n",
    "    df_old['version'] = 'old'\n",
    "    df_new['version'] = 'new'\n",
    "    df_diff['version'] = 'change'\n",
    "\n",
    "    tmp_df = pd.DataFrame(df_old, copy=True)\n",
    "    tmp_df = tmp_df.append(df_new, sort=True)\n",
    "    tmp_df = tmp_df.append(df_diff, sort=True)\n",
    "    tmp_df.index.name = 'for'  \n",
    "    tmp_df = tmp_df.reset_index().sort_values(by=['for', 'version'], ascending=[True, False]).set_index(tmp_df.index.names)\n",
    "    tmp_df.index.name = None\n",
    "\n",
    "    # put version first\n",
    "    tmp_df = tmp_df[['version'] + [x for x in tmp_df.columns if x != 'version']]\n",
    "    return tmp_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_dfs = {}\n",
    "\n",
    "name_old_new = [('descriptives', outputs_old['df_descriptives'], outputs_new['df_descriptives']),\n",
    "                ('outliers', outputs_old['df_outliers'], outputs_new['df_outliers']),\n",
    "                ('feature_cors', outputs_old['df_feature_cors'], outputs_new['df_feature_cors']),\n",
    "                ('percentiles', outputs_old['df_percentiles'], outputs_new['df_percentiles']),\n",
    "                ('eval_overview', outputs_old['df_eval'], outputs_new['df_eval']),\n",
    "                ('mcor_sc1', outputs_old['df_mcor_sc1'], outputs_new['df_mcor_sc1']),\n",
    "                ('mcor_sc1_overview', outputs_old['df_mcor_sc1_overview'], outputs_new['df_mcor_sc1_overview']),\n",
    "                ('pcor_sc1', outputs_old['df_pcor_sc1'], outputs_new['df_pcor_sc1']),\n",
    "                ('pcor_sc1_overview', outputs_old['df_pcor_sc1_overview'], outputs_new['df_pcor_sc1_overview']),\n",
    "                ('score_dist', outputs_old['df_score_dist'], outputs_new['df_score_dist']),\n",
    "                ('consistency', outputs_old['df_consistency'], outputs_new['df_consistency']),\n",
    "                ('disattenuated_correlations', outputs_old['df_disattenuated_correlations'], outputs_new['df_disattenuated_correlations'])]\n",
    "\n",
    "# add the subgroup differences\n",
    "for group in groups_eval:\n",
    "    name_old_new.extend([('eval_by_{}'.format(group), outputs_old['df_eval_by_{}'.format(group)], outputs_new['df_eval_by_{}'.format(group)]),\n",
    "                         ('eval_by_{}_m_sd'.format(group), outputs_old['df_eval_by_{}_m_sd'.format(group)], outputs_new['df_eval_by_{}_m_sd'.format(group)]),\n",
    "                         ('eval_by_{}_overview'.format(group), outputs_old['df_eval_by_{}_overview'.format(group)], outputs_new['df_eval_by_{}_overview'.format(group)]),\n",
    "                         ('mcor_sc1_by_{}'.format(group), outputs_old['df_mcor_sc1_by_{}'.format(group)], outputs_new['df_mcor_sc1_by_{}'.format(group)]),\n",
    "                         ('pcor_sc1_by_{}'.format(group), outputs_old['df_pcor_sc1_by_{}'.format(group)], outputs_new['df_pcor_sc1_by_{}'.format(group)]),\n",
    "                         ('disattenuated_correlations_by_{}'.format(group), \n",
    "                           outputs_old['df_disattenuated_correlations_by_{}'.format(group)],\n",
    "                           outputs_new['df_disattenuated_correlations_by_{}'.format(group)]),\n",
    "                         ('disattenuated_correlations_by_{}_overview'.format(group),\n",
    "                           outputs_old['df_disattenuated_correlations_by_{}_overview'.format(group)],\n",
    "                           outputs_new['df_disattenuated_correlations_by_{}_overview'.format(group)])])\n",
    "\n",
    "# combine the old and new data and compute the difference\n",
    "for name, df_old, df_new in name_old_new:\n",
    "    out_dfs[name] = combine_old_new_results(df_old, df_new, name)\n",
    "\n",
    "    \n",
    "# define the message for missing information\n",
    "no_info_str = 'This information is not available for either of the models.'\n",
    "no_plot_old_str = 'This figure is not available for the old model.'\n",
    "no_plot_new_str = 'This figure is not available for the new model.'\n",
    "    \n",
    "# WARN IF THE OLD AND NEW DATASETS ARE NOT THE SAME SIZE\n",
    "\n",
    "log_msgs = []\n",
    "# first check the training set\n",
    "if not (outputs_old['df_descriptives'].empty or outputs_new['df_descriptives'].empty):\n",
    "    oldTrainN = outputs_old['df_descriptives']['N'][0]  # take the N from the descriptive stats for the first feature\n",
    "    newTrainN = outputs_new['df_descriptives']['N'][0]\n",
    "    if oldTrainN != newTrainN:\n",
    "        log_msg = \"WARNING: the training sets were different sizes.  old N: {}, new N: {}.\".format(oldTrainN, newTrainN)\n",
    "        log_msgs.append(log_msg)\n",
    "\n",
    "# the check the test set\n",
    "\n",
    "if not (outputs_old['df_eval'].empty or outputs_new['df_eval'].empty): \n",
    "    oldTestN = outputs_old['df_eval']['N']  \n",
    "    newTestN = outputs_new['df_eval']['N']\n",
    "    if not np.all(oldTestN == newTestN):\n",
    "        log_msg = \"WARNING: the testing sets were different sizes.  old N: {}, new N: {}.\".format(oldTestN, newTestN)\n",
    "        log_msgs.append(log_msg)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
