{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True score evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not out_dfs['true_score_evaluations'].empty:\n",
    "    variance_columns = ['N','N_single','N_double','h1_var_single','h1_var_double', 'h2_var_double','true_var']\n",
    "    prmse_columns = ['N','N_single', 'N_double','sys_var_single','sys_var_double','mse_true','prmse_true']\n",
    "    markdown_strs = []\n",
    "    markdown_strs.append(\"The tables in this section show how well system scores can \"\n",
    "                        \"predict *true* scores. According to Test theory, a *true* score \"\n",
    "                        \"is a score that would have been obtained if there were no errors \"\n",
    "                        \"in measurement. While true scores cannot be observed, the variance \"\n",
    "                        \"of true scores and the prediction error can be estimated using observed \"\n",
    "                        \"human scores when multiple human ratings are available for a subset of \"\n",
    "                        \"responses. In this notebook these are estimated using human scores for \"\n",
    "                        \"responses in the evaluation set.\")\n",
    "    markdown_strs.append(\"#### Variance of human scores\")\n",
    "    markdown_strs.append(\"The table below shows variance of both sets of human scores \"\n",
    "                        \"for the whole evaluation set and for the subset of responses \"\n",
    "                        \"that were double-scored. Large differences in variance between \"\n",
    "                        \"the two human scores require further investigation. The last column \"\n",
    "                        \"shows estimated true score variance. \")\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    pd.options.display.width=10\n",
    "    df_human_variance =  out_dfs['true_score_evaluations'][variance_columns].copy()\n",
    "                # replace nans with \"-\"\n",
    "    df_human_variance.replace({np.nan: '-'}, inplace=True)\n",
    "    display(HTML('<span style=\"font-size:95%\">'+ df_human_variance.to_html(classes=['sortable'], \n",
    "                                                               escape=False,\n",
    "                                                               float_format=float_format_func) + '</span>'))\n",
    "    \n",
    "    markdown_strs = [\"#### Proportional reduction in mean squared error (PRMSE)\"]\n",
    "    markdown_strs.append(\"The table shows the variance of system scores for single-scored \"\n",
    "                        \"and double-scored responses, and mean squared error (MSE) and \"\n",
    "                        \"proportional reduction in mean squared error (PRMSE) for \"\n",
    "                        \"predicting a true score with system score.\")\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    pd.options.display.width=10\n",
    "    df_prmse = out_dfs['true_score_evaluations'][prmse_columns].copy()\n",
    "    df_prmse.replace({np.nan: '-'}, inplace=True)\n",
    "    display(HTML('<span style=\"font-size:95%\">'+ df_prmse.to_html(classes=['sortable'], \n",
    "                                                               escape=False,\n",
    "                                                               float_format=float_format_func) + '</span>'))\n",
    "else:\n",
    "    display(Markdown(no_info_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
