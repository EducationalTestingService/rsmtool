{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    num_excluded_test = len(df_test_responses_with_excluded_flags)\n",
    "except NameError:\n",
    "    num_excluded_test = 0\n",
    "\n",
    "pct_excluded_test = round(100*num_excluded_test/len(df_test_orig), 2)\n",
    "\n",
    "if num_excluded_test != 0:\n",
    "    display(Markdown(\"### Responses excluded due to flags\"))\n",
    "\n",
    "    display(Markdown(\"Total number of responses excluded due to flags:\"))\n",
    "    display(Markdown(\"{} responses ({:.1f}% of the original {} responses)\".format(num_excluded_test, pct_excluded_test, len(df_test_orig))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses excluded due to non-numeric system or human scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    num_missing_rows_test = len(df_test_excluded)\n",
    "except:\n",
    "    num_missing_rows_test = 0\n",
    "pct_missing_rows_test = 100*num_missing_rows_test/len(df_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('Total number of excluded responses: {} ({:.1f}% of the original {})'.format(num_missing_rows_test, pct_missing_rows_test, len(df_test_orig))))\n",
    "if num_missing_rows_test != 0:\n",
    "        df_test_excluded_analysis = pd.read_csv(join(output_dir, '{}_test_excluded_composition.csv'.format(experiment_id)))\n",
    "        display(HTML(df_test_excluded_analysis.to_html(classes=['sortable'], float_format=float_format_func, index=False))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of this report is based only on the responses used to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition of the evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the table showing candidate (speaker), prompt \n",
    "# and responses stats for training and test\n",
    "\n",
    "# feature descriptives extra table\n",
    "df_data_desc = pd.read_csv(join(output_dir, '{}_data_composition.csv'.format(experiment_id)))\n",
    "display(HTML(df_data_desc.to_html(classes=['sortable'], float_format=float_format_func, index=False)))\n",
    "        \n",
    "try:\n",
    "    num_double_scored_responses = len(df_test_human_scores[df_test_human_scores['sc2'].notnull()])\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    zeros_included_or_excluded = 'excluded' if exclude_zero_scores else 'included'\n",
    "    display(Markdown(\"Total number of double scored responses\" \n",
    "                     \" used: {} (zeros {})\".format(num_double_scored_responses,\n",
    "                                                   zeros_included_or_excluded)))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
