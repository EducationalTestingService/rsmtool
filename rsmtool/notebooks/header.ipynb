{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting options for the plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats={'retina', 'svg'}\n",
    "%config InlineBackend.rc={'savefig.dpi': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter out warnings from third-party libraries to prevent them\n",
    "# from showing up in the notebooks in multiple places\n",
    "import warnings\n",
    "\n",
    "# warning from shap about tqdm \n",
    "from tqdm import TqdmExperimentalWarning\n",
    "warnings.filterwarnings(\"ignore\", \n",
    "                        category=TqdmExperimentalWarning, \n",
    "                        message=\"Using `tqdm.autonotebook.tqdm` .*\", \n",
    "                        module=\"shap.explainers._linear\")\n",
    "\n",
    "# warning from matplotlib -> seaborn when figure layouts are changed to \"tight\"\n",
    "warnings.filterwarnings(\"ignore\", \n",
    "                        category=UserWarning, \n",
    "                        message=\".*figure layout.*\", \n",
    "                        module=\"seaborn.axisgrid\")\n",
    "\n",
    "# warning from seaborn about a deprecated method in pandas\n",
    "warnings.filterwarnings(\"ignore\", \n",
    "                        category=FutureWarning, \n",
    "                        message=\".*is_categorical_dtype is deprecated.*\", \n",
    "                        module=\"seaborn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import platform\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from os.path import abspath, relpath, exists, join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "from textwrap import wrap\n",
    "\n",
    "# allow older versions of pandas to work\n",
    "try:\n",
    "    from pandas.io.common import DtypeWarning\n",
    "except ImportError:\n",
    "    from pandas.errors import DtypeWarning\n",
    "\n",
    "from IPython import sys_info\n",
    "from IPython.display import display, HTML, Image, Javascript, Markdown, SVG\n",
    "from rsmtool.reader import DataReader\n",
    "from rsmtool.writer import DataWriter\n",
    "from rsmtool.utils.files import parse_json_with_comments\n",
    "from rsmtool.utils.notebook import (float_format_func,\n",
    "                                    int_or_float_format_func,\n",
    "                                    compute_subgroup_plot_params,\n",
    "                                    bold_highlighter,\n",
    "                                    color_highlighter,\n",
    "                                    show_thumbnail)\n",
    "\n",
    "from rsmtool.fairness_utils import (get_fairness_analyses,\n",
    "                                    write_fairness_results)\n",
    "\n",
    "from rsmtool.version import VERSION as rsmtool_version\n",
    "\n",
    "# turn off interactive plotting\n",
    "plt.ioff()\n",
    "\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_report_dir = os.environ.get('RSM_REPORT_DIR', None)\n",
    "if rsm_report_dir is None:\n",
    "    rsm_report_dir = os.getcwd()\n",
    "\n",
    "rsm_environ_config = join(rsm_report_dir, '.environ.json')\n",
    "if not exists(rsm_environ_config):\n",
    "    raise FileNotFoundError('The file {} cannot be located. '\n",
    "                            'Please make sure that either (1) '\n",
    "                            'you have set the correct directory with the `RSM_REPORT_DIR` '\n",
    "                            'environment variable, or (2) that your `.environ.json` '\n",
    "                            'file is in the same directory as your notebook.'.format(rsm_environ_config))\n",
    "    \n",
    "environ_config = parse_json_with_comments(rsm_environ_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "  div.prompt.output_prompt { \n",
    "    color: white; \n",
    "  }\n",
    "  \n",
    "  span.highlight_color {\n",
    "    color: red;\n",
    "  }\n",
    "  \n",
    "  span.highlight_bold {\n",
    "    font-weight: bold;  \n",
    "  }\n",
    "    \n",
    "  @media print {\n",
    "    @page {\n",
    "      size: landscape;\n",
    "      margin: 0cm 0cm 0cm 0cm;\n",
    "    }\n",
    "\n",
    "    * {\n",
    "      margin: 0px;\n",
    "      padding: 0px;\n",
    "    }\n",
    "\n",
    "    #toc {\n",
    "      display: none;\n",
    "    }\n",
    "\n",
    "    span.highlight_color, span.highlight_bold {\n",
    "        font-weight: bolder;\n",
    "        text-decoration: underline;\n",
    "    }\n",
    "\n",
    "    div.prompt.output_prompt {\n",
    "      display: none;\n",
    "    }\n",
    "    \n",
    "    h3#Python-packages, div#packages {\n",
    "      display: none;\n",
    "  }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you will need to set the following manually\n",
    "# if you are using this notebook interactively.\n",
    "experiment_id = environ_config.get('EXPERIMENT_ID')\n",
    "description = environ_config.get('DESCRIPTION')\n",
    "context = environ_config.get('CONTEXT')\n",
    "train_file_location = environ_config.get('TRAIN_FILE_LOCATION')\n",
    "test_file_location = environ_config.get('TEST_FILE_LOCATION')\n",
    "output_dir = environ_config.get('OUTPUT_DIR')\n",
    "figure_dir = environ_config.get('FIGURE_DIR')\n",
    "model_name = environ_config.get('MODEL_NAME')\n",
    "model_type = environ_config.get('MODEL_TYPE')\n",
    "skll_fixed_parameters = environ_config.get('SKLL_FIXED_PARAMETERS')\n",
    "skll_objective = environ_config.get('SKLL_OBJECTIVE')\n",
    "file_format = environ_config.get('FILE_FORMAT')\n",
    "length_column = environ_config.get('LENGTH_COLUMN')\n",
    "second_human_score_column = environ_config.get('H2_COLUMN')\n",
    "use_scaled_predictions = environ_config.get('SCALED')\n",
    "min_score = environ_config.get(\"MIN_SCORE\")\n",
    "max_score = environ_config.get(\"MAX_SCORE\")\n",
    "standardize_features = environ_config.get('STANDARDIZE_FEATURES')\n",
    "exclude_zero_scores = environ_config.get('EXCLUDE_ZEROS')\n",
    "feature_subset_file = environ_config.get('FEATURE_SUBSET_FILE', ' ')\n",
    "min_items = environ_config.get('MIN_ITEMS')\n",
    "use_thumbnails = environ_config.get('USE_THUMBNAILS')\n",
    "predict_expected_scores = environ_config.get('PREDICT_EXPECTED_SCORES')\n",
    "rater_error_variance = environ_config.get(\"RATER_ERROR_VARIANCE\")\n",
    "\n",
    "# groups for analysis by prompt or subgroup.\n",
    "groups_desc = environ_config.get('GROUPS_FOR_DESCRIPTIVES') \n",
    "groups_eval = environ_config.get('GROUPS_FOR_EVALUATIONS') \n",
    "\n",
    "# min number of n for group to be displayed in the report\n",
    "min_n_per_group = environ_config.get('MIN_N_PER_GROUP')\n",
    "\n",
    "if min_n_per_group is None:\n",
    "    min_n_per_group = {}\n",
    "\n",
    "# javascript path\n",
    "javascript_path = environ_config.get(\"JAVASCRIPT_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize counter for thumbnail IDs\n",
    "id_generator = itertools.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(javascript_path, \"sort.js\"), \"r\", encoding=\"utf-8\") as sortf:\n",
    "    display(Javascript(data=sortf.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown('''This report presents the analysis for **{}**: {}'''.format(experiment_id, description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = ''\n",
    "if use_thumbnails:\n",
    "    markdown_str += (\"\"\"\\n  - Images in this report have been converted to \"\"\"\n",
    "                     \"\"\"clickable thumbnails.\"\"\")\n",
    "if predict_expected_scores:\n",
    "    markdown_str += (\"\"\"\\n  - Predictions analyzed in this report are *expected scores*, \"\"\"\n",
    "                     \"\"\"i.e., probability-weighted averages over all score points.\"\"\")\n",
    "\n",
    "if markdown_str:\n",
    "    markdown_str = '**Notes**:' + markdown_str\n",
    "    display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(time.strftime('%c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training and testing features, both raw and pre-processed\n",
    "# Make sure that the `spkitemid` and `candidate` columns are read as strings \n",
    "# to preserve any leading zeros\n",
    "# We filter DtypeWarnings that pop up mostly in very large files\n",
    "\n",
    "string_columns = ['spkitemid', 'candidate']\n",
    "converter_dict = {column: str for column in string_columns}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore', category=DtypeWarning)\n",
    "    if exists(train_file_location):\n",
    "        df_train_orig = DataReader.read_from_file(train_file_location)\n",
    "\n",
    "    train_file = join(output_dir, '{}_train_features.{}'.format(experiment_id,\n",
    "                                                                file_format))\n",
    "    if exists(train_file):\n",
    "        df_train = DataReader.read_from_file(train_file, converters=converter_dict)\n",
    "\n",
    "    train_metadata_file = join(output_dir, '{}_train_metadata.{}'.format(experiment_id,\n",
    "                                                                         file_format))    \n",
    "    if exists(train_metadata_file):\n",
    "        df_train_metadata = DataReader.read_from_file(train_metadata_file, converters=converter_dict)\n",
    "\n",
    "    train_other_columns_file = join(output_dir, '{}_train_other_columns.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(train_other_columns_file):\n",
    "        df_train_other_columns = DataReader.read_from_file(train_other_columns_file, converters=converter_dict)\n",
    "\n",
    "    train_length_file = join(output_dir, '{}_train_response_lengths.{}'.format(experiment_id,\n",
    "                                                                               file_format))\n",
    "    if exists(train_length_file):\n",
    "        df_train_length = DataReader.read_from_file(train_length_file, converters=converter_dict)\n",
    "\n",
    "    train_excluded_file = join(output_dir, '{}_train_excluded_responses.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(train_excluded_file):\n",
    "        df_train_excluded = DataReader.read_from_file(train_excluded_file, converters=converter_dict)\n",
    "\n",
    "    train_responses_with_excluded_flags_file = join(output_dir, '{}_train_responses_with_excluded_flags.{}'.format(experiment_id,\n",
    "                                                                                                                   file_format))\n",
    "    if exists(train_responses_with_excluded_flags_file):\n",
    "        df_train_responses_with_excluded_flags = DataReader.read_from_file(train_responses_with_excluded_flags_file,\n",
    "                                                                           converters=converter_dict)\n",
    "\n",
    "    train_preproc_file = join(output_dir, '{}_train_preprocessed_features.{}'.format(experiment_id,\n",
    "                                                                                     file_format))    \n",
    "    if exists(train_preproc_file):\n",
    "        df_train_preproc = DataReader.read_from_file(train_preproc_file, converters=converter_dict)\n",
    "\n",
    "    if exists(test_file_location):\n",
    "        df_test_orig = DataReader.read_from_file(test_file_location)\n",
    "\n",
    "    test_file = join(output_dir, '{}_test_features.{}'.format(experiment_id,\n",
    "                                                              file_format))\n",
    "    if exists(test_file):\n",
    "        df_test = DataReader.read_from_file(test_file, converters=converter_dict)\n",
    "\n",
    "    test_metadata_file = join(output_dir, '{}_test_metadata.{}'.format(experiment_id,\n",
    "                                                                       file_format))    \n",
    "    if exists(test_metadata_file):\n",
    "        df_test_metadata = DataReader.read_from_file(test_metadata_file, converters=converter_dict)\n",
    "\n",
    "    test_other_columns_file = join(output_dir, '{}_test_other_columns.{}'.format(experiment_id,\n",
    "                                                                                 file_format))\n",
    "    if exists(test_other_columns_file):\n",
    "        df_test_other_columns = DataReader.read_from_file(test_other_columns_file, converters=converter_dict)\n",
    "\n",
    "    test_human_scores_file = join(output_dir, '{}_test_human_scores.{}'.format(experiment_id,\n",
    "                                                                               file_format))\n",
    "    if exists(test_human_scores_file):\n",
    "        df_test_human_scores = DataReader.read_from_file(test_human_scores_file, converters=converter_dict)\n",
    "\n",
    "    test_excluded_file = join(output_dir, '{}_test_excluded_responses.{}'.format(experiment_id,\n",
    "                                                                                 file_format))\n",
    "    if exists(test_excluded_file):\n",
    "        df_test_excluded = DataReader.read_from_file(test_excluded_file, converters=converter_dict)\n",
    "\n",
    "    test_responses_with_excluded_flags_file = join(output_dir, '{}_test_responses_with_excluded_flags.{}'.format(experiment_id,\n",
    "                                                                                                                 file_format))\n",
    "    if exists(test_responses_with_excluded_flags_file):\n",
    "        df_test_responses_with_excluded_flags = DataReader.read_from_file(test_responses_with_excluded_flags_file,\n",
    "                                                                          converters=converter_dict)\n",
    "\n",
    "    test_preproc_file = join(output_dir, '{}_test_preprocessed_features.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(test_preproc_file):\n",
    "        df_test_preproc = DataReader.read_from_file(test_preproc_file, converters=converter_dict)\n",
    "\n",
    "    pred_preproc_file = join(output_dir, '{}_pred_processed.{}'.format(experiment_id,\n",
    "                                                                       file_format))\n",
    "    if exists(pred_preproc_file):\n",
    "        df_pred_preproc = DataReader.read_from_file(pred_preproc_file, converters=converter_dict)\n",
    "\n",
    "    feature_file = join(output_dir, '{}_feature.{}'.format(experiment_id,\n",
    "                                                           file_format))\n",
    "    if exists(feature_file):\n",
    "        df_features = DataReader.read_from_file(feature_file, converters=converter_dict)\n",
    "        features_used = [c for c in df_features.feature.values]\n",
    "        # compute the longest feature name: we'll need if for the plots\n",
    "        longest_feature_name = max(map(len, features_used))\n",
    "\n",
    "    betas_file = join(output_dir, '{}_betas.{}'.format(experiment_id,\n",
    "                                                       file_format))\n",
    "    if exists(betas_file):\n",
    "        df_betas = DataReader.read_from_file(betas_file)\n",
    "\n",
    "    if exists(feature_subset_file):\n",
    "        df_feature_subset_specs = DataReader.read_from_file(feature_subset_file)\n",
    "    else:\n",
    "        df_feature_subset_specs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for continuous human scores in the evaluation set\n",
    "continuous_human_score = False\n",
    "\n",
    "if exists(pred_preproc_file):\n",
    "    if not df_pred_preproc['sc1'].equals(np.round(df_pred_preproc['sc1'])):\n",
    "        continuous_human_score = True"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
