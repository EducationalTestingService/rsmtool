{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rsmtool.analysis import compute_metrics\n",
    "from rsmtool.form_level_scores import compute_form_level_predictions\n",
    "from rsmtool.tpo_scores import convert_tpo_scores \n",
    "from rsmtool.utils import write_experiment_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check to make sure we have the information about candidate\n",
    "skip_section = 'candidate' not in df_test_metadata.columns\n",
    "\n",
    "if skip_section:\n",
    "    display(Markdown(\"Section skipped because the data did not contain a column with candidate ID.\"))\n",
    "else:\n",
    "    display(Markdown('## Form-level evaluations using TOEFLiBT/TPO aggregation rules'))\n",
    "    display(Markdown(\"This section shows the form-level (speaker-level) evaluations on the evaluation set. \"\n",
    "                     \"Note that for system scores (as elsewhere in this report) 'scale' refers to scores \"\n",
    "                     \"that have been re-scaled to the range of human scores. This is different from \"\n",
    "                     \"re-scaling the final scores to 1-30 range which is done using a look-up table \"\n",
    "                     \"at a later stage\"))\n",
    "    \n",
    "    # merge the data and compute form level predictions for system scores\n",
    "    \n",
    "    df_pred_with_metadata = pd.merge(df_pred_preproc, df_test_metadata)\n",
    "    df_form_level_predictions = compute_form_level_predictions(df_pred_with_metadata, 'TPO_speaking')\n",
    "\n",
    "    # determine which predictions we are using\n",
    "    if use_scaled_predictions:\n",
    "        system_score = 'scale'\n",
    "    else:\n",
    "        system_score = 'raw'\n",
    "        \n",
    "    # select speakers with valid numeric score\n",
    "    df_valid_predictions = df_form_level_predictions.ix[df_form_level_predictions[system_score].notnull()]\n",
    "    \n",
    "    score_columns = [system_score, 'sc1']\n",
    "    \n",
    "    if not df_valid_predictions.empty:\n",
    "    \n",
    "        # if we have second human score, compute the second form-level score\n",
    "        if second_human_score_column:\n",
    "            df_human_scores_with_metadata = pd.merge(df_test_human_scores, df_test_metadata)\n",
    "\n",
    "            # only select responses with valid sc2\n",
    "            df_human_scores_which_have_sc2 = df_human_scores_with_metadata.ix[df_human_scores_with_metadata['sc2'].notnull()]\n",
    "            df_human_form_level_predictions = compute_form_level_predictions(df_human_scores_which_have_sc2,\n",
    "                                                                             'TPO_speaking',\n",
    "                                                                              human_scores_only=True)\n",
    "            # select candidates which have form-level score for sc2\n",
    "            df_valid_human_predictions = df_human_form_level_predictions.ix[df_human_form_level_predictions['sc2'].notnull()]\n",
    "        else:\n",
    "            df_human_form_level_predictions = pd.DataFrame()\n",
    "            df_valid_human_predictions = pd.DataFrame()\n",
    "\n",
    "\n",
    "        # if we have valid human predictions, add them to system predictions and compute evaluations\n",
    "        if not df_valid_human_predictions.empty:\n",
    "\n",
    "            df_valid_human_and_system_predictions = pd.merge(df_valid_predictions, \n",
    "                                                             df_valid_human_predictions[['candidate', 'sc2']], \n",
    "                                                             how='left')\n",
    "            score_columns.append('sc2')\n",
    "            (df_human_machine_eval, \n",
    "             _, \n",
    "             df_human_human_eval) = compute_metrics(df_valid_human_and_system_predictions[score_columns],\n",
    "                                                    include_second_score=True)\n",
    "        else:\n",
    "            (df_human_machine_eval, \n",
    "             _, \n",
    "             df_human_human_eval) = compute_metrics(df_valid_predictions[score_columns],\n",
    "                                                    include_second_score=False)\n",
    "\n",
    "\n",
    "        df_human_machine_eval.insert(0, 'score', df_human_machine_eval.index)\n",
    "        df_human_human_eval.insert(0, 'score', ['human'])\n",
    "\n",
    "        # compute separate evaluation only for speakers who had scores for all six items\n",
    "\n",
    "        df_clean_predictions = df_valid_predictions.ix[df_valid_predictions['numeric_scores'] == 6]\n",
    "\n",
    "        # if we have a second score, find speakers who had 6 numeric scores from both raters and add them to \n",
    "        # system predictions\n",
    "        if second_human_score_column:\n",
    "            df_clean_human_predictions = df_valid_human_predictions[df_valid_human_predictions['numeric_scores'] == 6]\n",
    "            df_clean_human_and_system_predictions = pd.merge(df_clean_predictions, \n",
    "                                                             df_clean_human_predictions[['candidate', 'sc2']],\n",
    "                                                             how = 'left')\n",
    "            (df_human_machine_clean_eval, \n",
    "             _, \n",
    "             df_human_human_clean_eval) = compute_metrics(df_clean_human_and_system_predictions[score_columns],\n",
    "                                                          include_second_score=True)\n",
    "        else:\n",
    "            df_clean_human_and_system_predictions = df_clean_predictions\n",
    "            (df_human_machine_clean_eval, \n",
    "             _, \n",
    "             df_human_human_clean_eval) = compute_metrics(df_clean_human_and_system_predictions[score_columns],\n",
    "                                                          include_second_score=False)\n",
    "\n",
    "        # compute evaluation metrics\n",
    "        df_human_machine_clean_eval.insert(0, 'score', df_human_machine_clean_eval.index)\n",
    "        df_human_human_clean_eval.insert(0, 'score', ['human'])\n",
    "    \n",
    "      \n",
    "        # display the results\n",
    "\n",
    "        markdown_strs = ['### No imputation']\n",
    "        markdown_strs.append(\"The following table shows the agreement for all speakers who \"\n",
    "                             \"received numeric scores for all 6 items.\")\n",
    "\n",
    "        if second_human_score_column:\n",
    "            markdown_strs.append('#### Human-human agreement')\n",
    "            if not df_human_human_clean_eval['N'].isnull().values.all():\n",
    "                display(Markdown('\\n'.join(markdown_strs)))\n",
    "                display(HTML(df_human_human_clean_eval.to_html(index=False,\n",
    "                                               escape=False,\n",
    "                                               float_format=float_format_func)))\n",
    "            else:\n",
    "                markdown_strs.append(\"None of the speakers who had 6 numeric system and human scores also had \"\n",
    "                                     \" 6 numeric scores from the second rater.\")\n",
    "                display(Markdown('\\n'.join(markdown_strs)))\n",
    "\n",
    "                # set the data frame to empty so that it is not saved\n",
    "                df_human_human_clean_eval = pd.DataFrame()\n",
    "        else:\n",
    "            display(Markdown('\\n'.join(markdown_strs)))\n",
    "\n",
    "        markdown_strs = ['#### Human-system agreement']     \n",
    "        if len(df_clean_predictions) > 0:\n",
    "            display(Markdown('\\n'.join(markdown_strs)))\n",
    "            display(HTML(df_human_machine_clean_eval.to_html(index=False,\n",
    "                                           escape=False,\n",
    "                                           float_format=float_format_func)))\n",
    "        else:\n",
    "            markdown_strs.append(\"None of the speakers had 6 numeric system and human scores\")\n",
    "            # set the data frame to empty so that it is not saved\n",
    "            df_human_machine_clean_eval = pd.DataFrame()\n",
    "\n",
    "\n",
    "        markdown_strs = ['### All data']\n",
    "        markdown_strs.append(\"The following table shows the agreement for all speakers \"\n",
    "                             \" who received 5 or more \"\n",
    "                             \"numeric scores. The missing scores were imputed using median \"\n",
    "                             \"value of the other 5 scores.\")\n",
    "\n",
    "        if second_human_score_column:\n",
    "            markdown_strs.append('#### Human-human agreement')\n",
    "            if not df_human_human_eval['N'].isnull().values.all():\n",
    "                display(Markdown('\\n'.join(markdown_strs)))\n",
    "                display(HTML(df_human_human_eval.to_html(index=False,\n",
    "                                                     escape=False,\n",
    "                                                     float_format=float_format_func)))\n",
    "            else:\n",
    "                markdown_strs.append(\"None of the speakers who had 5 or more numeric system and human scores also \"\n",
    "                                     \" had 5 or more numeric scores from the second rater.\")\n",
    "                display(Markdown('\\n'.join(markdown_strs)))\n",
    "                df_human_human_eval = pd.DataFrame()\n",
    "        else:\n",
    "            display(Markdown('\\n'.join(markdown_strs)))\n",
    "\n",
    "        markdown_strs = ['#### Human-system agreement']  \n",
    "        display(Markdown('\\n'.join(markdown_strs)))\n",
    "        display(HTML(df_human_machine_eval.to_html(index=False,\n",
    "                                                   escape=False,\n",
    "                                                   float_format=float_format_func)))\n",
    "        # write out the outputs\n",
    "        write_experiment_output([df_form_level_predictions,\n",
    "                                 df_human_form_level_predictions,\n",
    "                                 df_human_human_clean_eval,\n",
    "                                 df_human_machine_clean_eval,\n",
    "                                 df_human_human_eval,\n",
    "                                 df_human_machine_eval],\n",
    "                                ['pred_form_level',\n",
    "                                 'test_human_scores_form_level',\n",
    "                                 'consistency_form_level_no_imputation',\n",
    "                                 'eval_form_level_no_imputation',\n",
    "                                 'consistency_form_level',\n",
    "                                 'eval_form_level'], \n",
    "                               experiment_id,\n",
    "                               output_dir)\n",
    "    else:\n",
    "        display(Markdown(\"None of the speakers had 5 or more numeric system and human scores necessary to \"\n",
    "                         \"compute form-level scores\") )\n",
    "        write_experiment_output([df_form_level_predictions],\n",
    "                                ['pred_form_level'], \n",
    "                               experiment_id,\n",
    "                               output_dir)\n",
    "         \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
