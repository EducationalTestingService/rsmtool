{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting options for the plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats={'retina', 'svg'}\n",
    "%config InlineBackend.rc={'savefig.dpi': 150}\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    div.prompt.output_prompt { color: white; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you are running this ipython notebook interactively, you will \n",
    "# need to set the following variables manually below.\n",
    "\n",
    "experiment_id = '' # the experiment id\n",
    "description = '' # experiment description\n",
    "train_file_location = '' # location of the training set feature file\n",
    "test_file_location = '' # location of the test set feature file\n",
    "output_dir = '' # the experiment output directory\n",
    "figure_dir = '' # the experiment figure directory>\n",
    "model_name = '' # the name of the model that was trained\n",
    "scaled='' # set this to '1' if you want to use scaled predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "\n",
    "from os.path import abspath, exists, join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython import sys_info\n",
    "from IPython.display import display, HTML, Image, Markdown, SVG\n",
    "\n",
    "if not experiment_id:\n",
    "    experiment_id = os.environ.get('EXPERIMENT_ID')\n",
    "if not description:\n",
    "    description = os.environ.get('DESCRIPTION')\n",
    "if not train_file_location:\n",
    "    train_file_location = os.environ.get('TRAIN_FILE_LOCATION')\n",
    "if not test_file_location:\n",
    "    test_file_location = os.environ.get('TEST_FILE_LOCATION')\n",
    "if not output_dir:\n",
    "    output_dir = os.environ.get('OUTPUT_DIR')\n",
    "if not figure_dir:\n",
    "    figure_dir = os.environ.get('FIGURE_DIR')\n",
    "if not model_name:\n",
    "    model_name = os.environ.get('MODEL_NAME')\n",
    "if not scaled:\n",
    "    scaled = os.environ.get('SCALED')\n",
    "\n",
    "use_scaled_predictions = scaled == '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the training and testing features, both raw and pre-processed\n",
    "df_train_orig = pd.read_csv(train_file_location)\n",
    "df_train = pd.read_csv(join(output_dir, '{}_train.csv'.format(experiment_id)))\n",
    "df_train_preproc = pd.read_csv(join(output_dir, '{}_train_preprocessed.csv'.format(experiment_id)))\n",
    "df_test_orig = pd.read_csv(test_file_location)\n",
    "df_test = pd.read_csv(join(output_dir, '{}_test.csv'.format(experiment_id)))\n",
    "df_test_preproc = pd.read_csv(join(output_dir, '{}_test_preprocessed.csv'.format(experiment_id)))\n",
    "df_features = pd.read_csv(join(output_dir, '{}_feature.csv'.format(experiment_id)))\n",
    "features_used = [c for c in df_features.feature.values if c not in ['spkitemid', 'sc1']]\n",
    "\n",
    "# define a float formatting function\n",
    "def float_format_func(x):\n",
    "    if float.is_integer(x):\n",
    "        ans = '{}'.format(int(x))\n",
    "    else:\n",
    "        ans = '{:.3f}'.format(x)\n",
    "    return ans\n",
    "\n",
    "def float_format_func2(x):\n",
    "    if float.is_integer(x):\n",
    "        ans = '{}'.format(int(x))\n",
    "    else:\n",
    "        ans = '{:.2f}'.format(x)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html_strings = []\n",
    "html_strings.append('<strong>Experiment ID</strong>: {}'.format(experiment_id))\n",
    "html_strings.append('<strong>Description</strong>: {}'.format(description))\n",
    "html_strings.append('<strong>Output directory</strong>: {}'.format(os.path.abspath(output_dir)))\n",
    "html_strings.append('<strong>Figure directory</strong>: {}'.format(os.path.abspath(figure_dir)))\n",
    "html_strings.append('<strong>Model</strong>: {}'.format(model_name))\n",
    "html_strings.append('<strong>Scaled predictions</strong>: {}'.format(use_scaled_predictions))\n",
    "HTML('<br/>'.join(html_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses excluded due to non-numeric feature values or scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_missing_rows_train = len(df_train_orig) - len(df_train)\n",
    "pct_missing_rows_train = round(num_missing_rows_train/len(df_train_orig))\n",
    "num_missing_rows_test = len(df_test_orig) - len(df_test)\n",
    "pct_missing_rows_test = round(num_missing_rows_test/len(df_test_orig)*100, 1)\n",
    "\n",
    "html_strings = []\n",
    "html_strings.append('<i>Training set: </i>{} ({}% of the original {})'.format(num_missing_rows_train, pct_missing_rows_train, len(df_train_orig)))\n",
    "html_strings.append('<br/>')\n",
    "html_strings.append('<i>Evaluation set: </i>{} ({}% of the original {})'.format(num_missing_rows_test, pct_missing_rows_test, len(df_test_orig)))\n",
    "html_strings.append('<br/><br/>')\n",
    "html_strings.append('The rest of this report is based only on the responses used to build and train the model.')\n",
    "HTML(''.join(html_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make the table showing candidate (speaker), prompt \n",
    "# and responses stats for training and test\n",
    "\n",
    "train_responses = set(df_train['spkitemid'])\n",
    "train_ids_prompts_lists = df_train['spkitemid'].apply(lambda x: x.split('-'))\n",
    "train_candidates = set(map(operator.itemgetter(0), train_ids_prompts_lists))\n",
    "train_prompts = set(map(operator.itemgetter(1), train_ids_prompts_lists))\n",
    "\n",
    "test_responses = set(df_test['spkitemid'])\n",
    "test_ids_prompts_lists = df_test['spkitemid'].apply(lambda x: x.split('-'))\n",
    "test_candidates = set(map(operator.itemgetter(0), test_ids_prompts_lists))\n",
    "test_prompts = set(map(operator.itemgetter(1), test_ids_prompts_lists))\n",
    "\n",
    "rows = [{'partition': 'Training', 'responses': len(train_responses), \n",
    "         'prompts': len(train_prompts), 'candidates': len(train_candidates)},\n",
    "        {'partition': 'Evaluation', 'responses': len(test_responses), \n",
    "         'prompts': len(train_prompts), 'candidates': len(train_candidates)},\n",
    "        {'partition': 'Overlapping', 'responses': len(train_responses & test_responses), \n",
    "         'prompts': len(train_prompts & test_prompts), 'candidates': len(train_candidates & test_candidates)},\n",
    "        {'partition': 'Total', 'responses': len(train_responses | test_responses), \n",
    "         'prompts': len(train_prompts | test_prompts), 'candidates': len(train_candidates | test_candidates)}]\n",
    "\n",
    "df = pd.DataFrame.from_dict(rows)\n",
    "df = df[['partition', 'responses', 'prompts', 'candidates']]\n",
    "HTML(df.to_html(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html_strings = []\n",
    "avg_resp_prompt_train, avg_resp_prompt_test = len(train_responses) / len(train_prompts), len(test_responses) / len(test_prompts)\n",
    "avg_resp_cands_train, avg_resp_cands_test = len(train_responses) / len(train_candidates), len(test_responses) / len(test_candidates)\n",
    "html_strings.append('Average responses per candidate: {} for training, {} for evaluation'.format(avg_resp_cands_train, avg_resp_cands_test))\n",
    "html_strings.append('<br/>')\n",
    "html_strings.append('Average responses per prompt: {} for training, {} for evaluation'.format(avg_resp_prompt_train, avg_resp_prompt_test))\n",
    "HTML(''.join(html_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics for the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feature values are reported before transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature descriptives table\n",
    "df_desc = pd.read_csv(join(output_dir, '{}_feature_descriptives.csv'.format(experiment_id)), index_col=0)\n",
    "HTML(df_desc.to_html(float_format=lambda x: '{:.3f}'.format(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows additional statistics for the data. Quantiles are computed using type=3 method used in SAS. The mild outliers are defined as data points between [1.5, 3) \\* IQR away from the nearest quartile. Extreme outliers are the data points >= 3 * IQR away from the nearest quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature descriptives extra table\n",
    "df_desce = pd.read_csv(join(output_dir, '{}_feature_descriptivesExtra.csv'.format(experiment_id)), index_col=0)\n",
    "HTML(df_desce.to_html(float_format=lambda x: '{:.3f}'.format(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature values by prompt or item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are reported before transformations/truncation. The lines indicate the threshold for truncation (mean +/- 4\\*SD)\n",
    "\n",
    "**Note**: If the training data has more than 20 features, a low-resolution plot is generated to conserve memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train_orig_feats = df_train_orig.copy()\n",
    "df_train_orig_feats['prompt'] = df_train_orig_feats['spkitemlab'].apply(lambda x: x.split('-')[1])\n",
    "df_train_orig_feats = df_train_orig_feats[features_used + ['prompt']]\n",
    "\n",
    "df_train_orig_feats_all = df_train_orig_feats.copy()\n",
    "df_train_orig_feats_all['prompt'] = 'All data'\n",
    "\n",
    "df_train_combined = pd.concat([df_train_orig_feats, df_train_orig_feats_all])\n",
    "df_train_combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# decide on the the height per plot\n",
    "height_per_plot = 2 if len(features_used) > 15 else 4\n",
    "\n",
    "# create the faceted boxplots\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, len(features_used)*height_per_plot)\n",
    "with sns.axes_style('white'):\n",
    "    for i, varname in enumerate(sorted(features_used)):\n",
    "        df_plot = df_train_combined[['prompt', varname]]\n",
    "        min_value = df_plot.mean() - 4 * df_plot.std()\n",
    "        max_value = df_plot.mean() + 4 * df_plot.std()\n",
    "        ax = fig.add_subplot(len(features_used), 1, i + 1)\n",
    "        ax.axhline(y=float(min_value), linestyle='--', linewidth=0.5, color='r')\n",
    "        ax.axhline(y=float(max_value), linestyle='--', linewidth=0.5, color='r')\n",
    "        sns.boxplot(df_plot[varname], df_plot.prompt, color='grey', alpha=0.5, ax=ax)\n",
    "        labels = sorted(df_plot.prompt.unique())\n",
    "        ax.set_xticklabels(labels, rotation=90) \n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_title(varname)\n",
    "plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "# if there are > 20 features, then save the \n",
    "# figure as a low resolution .png file and display\n",
    "# that instead of rendering the SVG file\n",
    "if len(features_used) > 20:\n",
    "    imgfile = '/tmp/feature_boxplot_{}.png'.format(experiment_id)\n",
    "    plt.savefig(imgfile)\n",
    "    display(Image(imgfile))\n",
    "    plt.close();    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage of cases truncated to mean +/- 4 SD for each feature\n",
    "\n",
    "**Note: See `*_outliers.csv` for more details**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_outliers = pd.read_csv(join(output_dir, '{}_feature_outliers.csv'.format(experiment_id)), index_col=0)\n",
    "df_outliers.index.name = 'feature'\n",
    "df_outliers = df_outliers.reset_index()\n",
    "df_outliers = pd.melt(df_outliers, id_vars=['feature'])\n",
    "df_outliers = df_outliers[df_outliers.variable.str.contains(r'[ul].*?perc')]\n",
    "\n",
    "# we need a higher aspect if we have more than 40 features\n",
    "aspect = 3 if len(features_used) > 40 else 2\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "    p = sns.factorplot(\"feature\", \"value\", \"variable\", kind=\"bar\", \n",
    "                       palette=sns.color_palette(\"Greys\", 2),\n",
    "                       data=df_outliers, size=3, aspect=aspect, legend=False)\n",
    "    p.set_axis_labels('', 'Percent cases truncated to mean +/- 4sd')\n",
    "    p.set_xticklabels(rotation=90)\n",
    "    axis = p.axes[0][0]\n",
    "    axis.legend(('lower', 'upper'), title='', frameon=True, fancybox=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A multi-variable view using psych package's pairs function. \n",
    "The features values are shown after the truncation and standardization.  \n",
    "For datasets with more than 5000 responses the plots only show the correlation lines. \n",
    "A textual version of this table can be found in `output/*_cors_orig.csv` for original feature values \n",
    "and `output/*_cors_processed.csv` for standardized and truncated feature values. \n",
    "\n",
    "**Note**: No plot is generated for models with more than 20 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrixfile = join(figure_dir, '{}_psy.png'.format(experiment_id))\n",
    "if exists(matrixfile):\n",
    "    display(Image(matrixfile, width=800, height=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal and partial correlations\n",
    "\n",
    "The plot shows correlations between truncated and standardized values of each feature and human score. The first bar in each case shows Pearson's correlation, the second bar shows partial correlations after controlling for all other variables. The correlations are computed using the `ppcor` package. Correlations for each prompt are saved in .csv files `*_pcor.csv` and `*_margcor_csv`. ERR means that there was singularity in the data and partial correlations could not be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in and merge the correlations \n",
    "df_margcor = pd.read_csv(join(output_dir, '{}_margcor.csv'.format(experiment_id)), index_col=0)\n",
    "df_pcor = pd.read_csv(join(output_dir, '{}_pcor.csv'.format(experiment_id)), index_col=0)\n",
    "df_mpcor = pd.DataFrame([df_margcor.loc['All data'], df_pcor.loc['All data']]).transpose()\n",
    "df_mpcor.index.name = 'feature'\n",
    "df_mpcor.columns = ['marginal', 'partial']\n",
    "df_mpcor = df_mpcor.reset_index()\n",
    "df_mpcor = pd.melt(df_mpcor, id_vars=['feature'])\n",
    "\n",
    "# we need a higher aspect if we have more than 40 features\n",
    "aspect = 3 if len(features_used) > 40 else 2\n",
    "\n",
    "# check for any negative correlations\n",
    "limits = (0, 1)\n",
    "if len(df_mpcor[df_mpcor.value < 0]):\n",
    "    limits = (-1, 1)\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "    p = sns.factorplot(\"feature\", \"value\", \"variable\", kind=\"bar\",\n",
    "                       palette=sns.color_palette(\"Greys\", 2), \n",
    "                       data=df_mpcor, size=3, aspect=aspect, legend=False)\n",
    "    p.set_axis_labels('', 'Correlation coefficient')\n",
    "    p.set_xticklabels(rotation=90)\n",
    "    p.set(ylim=limits)\n",
    "    axis = p.axes[0][0]\n",
    "    axis.legend(labels=('Marginal', 'Partial'), title='', frameon=True, fancybox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For e-rater data we also compute partial correlations after controlling for LOGDTA and LOGDTU. These are stored in `*_pcor_no_LOGDTA.csv` and also shown in the following plot (if available). For the time being no such analysis is conducted for SpeechRater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcor_nologdta_file = join(output_dir, '{}_pcor_no_LOGDTA.csv'.format(experiment_id))\n",
    "if exists(pcor_nologdta_file):\n",
    "    df_pcor_nologdta = pd.read_csv(pcor_nologdta_file, index_col=0)\n",
    "    df_mpcor2 = pd.DataFrame([df_margcor.loc['All data'], \n",
    "                              df_pcor_nologdta.loc['All data'], \n",
    "                              df_pcor.loc['All data']]).transpose()\n",
    "    df_mpcor2.index.name = 'feature'\n",
    "    df_mpcor2.columns = ['marginal', 'partial_nodta', 'partial']\n",
    "    df_mpcor2 = df_mpcor2.reset_index()\n",
    "    df_mpcor2 = pd.melt(df_mpcor2, id_vars=['feature'])\n",
    "\n",
    "    # check for any negative correlations\n",
    "    limits = (0, 1)\n",
    "    if len(df_mpcor2[df_mpcor2.value < 0]):\n",
    "        limits = (-1, 1)\n",
    "\n",
    "    with sns.axes_style('whitegrid'):\n",
    "        p = sns.factorplot(\"feature\", \"value\", \"variable\", kind=\"bar\",\n",
    "                           palette=sns.color_palette(\"Greys\", 3),\n",
    "                           data=df_mpcor2, size=3, aspect=2, legend=False)\n",
    "        p.set_axis_labels('', 'Correlation coefficient')\n",
    "        p.set(ylim=limits)\n",
    "        p.set_xticklabels(rotation=90)\n",
    "        axis = p.axes[0][0]\n",
    "        axis.legend(labels=('Marginal', 'Excl. LOGDTA/LOGDTU', 'Excl. all features'), title='', frameon=True, fancybox=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Markdown('Model used: **{}**'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if model_name == 'empWt':\n",
    "    display(HTML('<h3>Weights assigned to each feature</h3>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if we used an R linear model, then we first just show a summary of that model\n",
    "if model_name == 'empWt':\n",
    "    summ = %R -i experiment_id,output_dir library(etsmodels); modelfile <- paste0(output_dir, \"/\", experiment_id, \"_\", \"empWt.Rmodel\"); load(modelfile); summ <- summary(fit)\n",
    "    print(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_str = \"\"\"### Standardized and Relative Regression Coefficients (Betas)\n",
    "\n",
    "The relative coefficients are intended to show relative contribution of different feature and their primary purpose is to indentify whether one of the features has an unproportionate effect over the final score. They are computed as standardized/(sum of absolute values of standardized coefficients). \n",
    "\n",
    "**Note**: if the model contains negative coefficients, relative values will not sum up to one and their interpretation is generally questionable. \"\"\"\n",
    "\n",
    "if model_name == 'empWt':\n",
    "    display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if model_name == 'empWt':\n",
    "    df_weights = pd.read_csv(join(output_dir, '{}_{}_coefficients.csv'.format(experiment_id, model_name)), index_col=0)\n",
    "    df_weights.drop('Intercept', inplace=True)\n",
    "    df_betas = df_weights.multiply(df_train_preproc[features_used].std(), axis='index') / df_train['sc1'].std()\n",
    "    df_betas.columns = ['standardized']\n",
    "    df_betas['relative'] = df_betas / sum(abs(df_betas['standardized']))\n",
    "    df_betas.reset_index(inplace=True)\n",
    "    df_betas.sort('feature', inplace=True)\n",
    "    display(HTML(df_betas.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if model_name == 'empWt':\n",
    "    display(Markdown('Here are the same values, shown graphically.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this cell is if we have less than 15 features\n",
    "if model_name == 'empWt':\n",
    "    df_betas_sorted = df_betas.sort('standardized', ascending=False)\n",
    "    df_betas_sorted.reset_index(drop=True, inplace=True)\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(8, 2.5)\n",
    "    grey_colors = sns.color_palette('Greys', len(features_used))[::-1]\n",
    "    with sns.axes_style('whitegrid'):\n",
    "        ax1=fig.add_subplot(121)\n",
    "        sns.barplot(\"feature\",\"standardized\", data=df_betas_sorted, \n",
    "                    x_order=df_betas_sorted['feature'].values,\n",
    "                    palette=sns.color_palette(\"Greys\", 1), ax=ax1)\n",
    "        ax1.set_xticklabels(df_betas_sorted['feature'].values, rotation=90)\n",
    "        ax1.set_title('Values of standardized coefficients')\n",
    "        ax1.set_xlabel('')\n",
    "        ax1.set_ylabel('')\n",
    "        if len(features_used) < 15:\n",
    "            ax2=fig.add_subplot(133, aspect=True)\n",
    "            ax2.pie(abs(df_betas_sorted['relative'].values), colors=grey_colors, \n",
    "                labels=df_betas_sorted['feature'].values)\n",
    "            ax2.set_title('Proportional contribution of each feature')\n",
    "        else:\n",
    "            fig.set_size_inches(0.35*len(features_used), 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this cell is if we have more than 15 features (no pie chart)\n",
    "if model_name == 'empWt and len(features_used) > 15':\n",
    "    df_betas_sorted = df_betas.sort('standardized', ascending=False)\n",
    "    df_betas_sorted.reset_index(drop=True, inplace=True)\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(8, 2.5)\n",
    "    grey_colors = sns.color_palette('Greys', len(features_used))[::-1]\n",
    "    with sns.axes_style('whitegrid'):\n",
    "        ax1=fig.add_subplot(121)\n",
    "        sns.barplot(\"feature\",\"standardized\", data=df_betas_sorted, \n",
    "                    x_order=df_betas_sorted['feature'].values,\n",
    "                    palette=sns.color_palette(\"Greys\", 1), ax=ax1)\n",
    "        ax1.set_xticklabels(df_betas_sorted['feature'].values, rotation=90)\n",
    "        ax1.set_title('Values of standardized coefficients')\n",
    "        ax1.set_ylabel('')\n",
    "        ax2=fig.add_subplot(133, aspect=True)\n",
    "        ax2.pie(abs(df_betas_sorted['relative'].values), colors=grey_colors, \n",
    "                labels=df_betas_sorted['feature'].values)\n",
    "        ax2.set_title('Proportional contribution of each feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "markdown_str1 = \"The table shows weighted kappa and correlation between predicted scores and the scores assigned by human raters. \"\n",
    "raw_or_scaled = \"scaled\" if use_scaled_predictions else \"raw\"\n",
    "markdown_str2 = \"All reported machine scores are {}.\".format(raw_or_scaled)\n",
    "HTML(markdown_str1 + markdown_str2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Trim` (`bound`) scores are truncated to [min-0.4998, max+.4998]. `Trim-round` scores correspond to e-rater `round` scores and are computed by first truncating and then rounding the predicted score. Scaled scores are computed by re-scaling the predicted scores using mean and standard deviation of human scores as observed on the training data and mean and standard deviation of machine scores as predicted for the training set. These results are computed on the evaluation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the computation of kappas scores are always rounded. The table below shows all metrics currently recommended for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv(join(output_dir, '{}_eval.csv'.format(experiment_id)), index_col=0)\n",
    "pd.options.display.width=10\n",
    "HTML('<span style=\"font-size:95%\">'+ df_eval.to_html(float_format=float_format_func) + '</span>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Markdown(\"Confusion matrix using {} trimmed rounded scores.\".format(raw_or_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_confmat = pd.read_csv(join(output_dir, '{}_confMatrix.csv'.format(experiment_id)), index_col=0)\n",
    "df_confmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of human and machine scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "markdown_str = \"The histograms and the table show the distibution of human scores and {} trimmed rounded machine scores (as % of all responses).\".format(raw_or_scaled)\n",
    "Markdown(markdown_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_scoredist = pd.read_csv(join(output_dir, '{}_score_dist.csv'.format(experiment_id)), index_col=0)\n",
    "df_scoredist_melted = pd.melt(df_scoredist, id_vars=['score'])\n",
    "df_scoredist_melted = df_scoredist_melted[df_scoredist_melted['variable'] != 'difference']\n",
    "with sns.axes_style('whitegrid'):\n",
    "    p = sns.factorplot(\"score\", \"value\", \"variable\", kind=\"bar\",\n",
    "                       palette=sns.color_palette(\"Greys\", 2), \n",
    "                       data=df_scoredist_melted, size=3, aspect=2, legend=False)\n",
    "    p.set_axis_labels('score', 'Of total N responses')\n",
    "    axis = p.axes[0][0]\n",
    "    axis.legend(labels=('Human', 'Machine'), title='', frameon=True, fancybox=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_html = df_scoredist.to_html(index=False)\n",
    "HTML('Distribution of human and machine scores (%) <br/>' + df_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on prompt level\n",
    "\n",
    "**Note: See `*_evalByPrompt.csv` for full results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "basic_metrics = {('wtkappa', 'trim_round'): [0.7],\n",
    "                 ('Corr', 'trim'): [0.7],\n",
    "                 ('SMD', 'trim_round'): [0.15, -0.15],\n",
    "                 ('SMD', 'trim'): [0.15, -0.15]}\n",
    "colprefix = 'scale' if use_scaled_predictions else 'raw'\n",
    "metrics = dict([('{}.{}_{}'.format(k[0], colprefix, k[1]), v) for k,v in basic_metrics.items()])\n",
    "df_eval_prompt = pd.read_csv(join(output_dir, '{}_evalByPrompt.csv'.format(experiment_id)), index_col=0)\n",
    "df_eval_prompt.index.name = 'prompt'\n",
    "df_eval_prompt.reset_index(inplace=True)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, len(metrics)*4)\n",
    "with sns.axes_style('white'):\n",
    "    for i, metric in enumerate(sorted(metrics.keys())):\n",
    "        df_plot = df_eval_prompt[['prompt', metric]]\n",
    "        ax = fig.add_subplot(len(metrics), 1, i + 1)\n",
    "        for lineval in metrics[metric]:\n",
    "            ax.axhline(y=float(lineval), linestyle='--', linewidth=0.5, color='black')\n",
    "        sns.barplot(df_plot.prompt, df_plot[metric], color='grey', ax=ax)\n",
    "        labels = sorted(df_plot.prompt.unique())\n",
    "        ax.set_xticklabels(labels, rotation=90) \n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_title(metric)\n",
    "plt.tight_layout(h_pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation on training set\n",
    "\n",
    "This section shows the evaluation of the model using 10-fold cross-validation on the training set (if requested in the config file). The folds were created so that the responses from the same speaker are always contained in one fold. Speakers were allocated to folds so that each fold had similar distribution of mean scores (see `*_folds.csv` for the allocation of responses between folds).\n",
    "\n",
    "If the experiment config specifies different train.lab and test.lab, these are also applied to crossvalidation, that is for each fold the model is trained on the train.lab and tested on the test.lab. However, at this stage z-normalization is applied to the training set as a whole, rather than to each fold.\n",
    "\n",
    "The figures show the distribution of the main evaluation metrics across folds.Red lines indicate the performance on the held-out evaluation set.\n",
    "\n",
    "Note that when the whole training set was used to select the best peforming features out of a large number of possible features, the final performance on the evaluation set is likely to be lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis\n",
    "\n",
    "PCA using scaled data and singular value decomposition. This is computed using processed features after the truncation of outliers and other transformations specified in feature config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pca = pd.read_csv(join(output_dir, '{}_pca.csv'.format(experiment_id)), index_col=0)\n",
    "df_pca.sort_index(inplace=True)\n",
    "HTML(df_pca.to_html(float_format=float_format_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pcavar = pd.read_csv(join(output_dir, '{}_pcavar.csv'.format(experiment_id)), index_col=0)\n",
    "df_pcavar.sort_index(inplace=True)\n",
    "HTML(df_pcavar.to_html(float_format=float_format_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVG(join(figure_dir, '{}_pca.svg'.format(experiment_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_str = \"\"\"## Factor Analysis\n",
    "\n",
    "**IMPORTANT: This analysis in its current form is only meaningful for e-rater features.**\n",
    "\n",
    "Factor analysis using maximum likelihood with three factors and oblique rotation. This is computed using processed features after the truncation of outliers and other transformations specified in feature config file (**Note**: for E-rater feature WORDS the only transformation used is the truncation of outliers).\n",
    "\"\"\"\n",
    "if exists(pcor_nologdta_file):\n",
    "    display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if exists(pcor_nologdta_file):\n",
    "    df_efa3 = pd.read_csv(join(output_dir, '{}_efa3.csv'.format(experiment_id)), index_col=0)\n",
    "    df_efa3.sort_index(inplace=True)\n",
    "    display(HTML(df_efa3.to_html(float_format=float_format_func)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if exists(pcor_nologdta_file):\n",
    "    df_efa3fv = pd.read_csv(join(output_dir, '{}_efa3fv.csv'.format(experiment_id)), index_col=0)\n",
    "    display(HTML(df_efa3fv.to_html(float_format=float_format_func)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if exists(pcor_nologdta_file):\n",
    "    df_efa3fcor = pd.read_csv(join(output_dir, '{}_efa3fcor.csv'.format(experiment_id)), index_col=0)\n",
    "    display(HTML(df_efa3fcor.to_html(float_format=float_format_func)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_str = \"\"\"If the data contains e-rater content features, the table below will show the results of factor analysis using maximum likelihood with four factors and oblique rotation. This is computed using processed features after the truncation of outliers and other transformations specified in feature config file (**Note**: for E-rater features WORDS, VAL_COS and PAT_COS the only transformation used is the truncation of outliers).\n",
    "\n",
    "If no content features are present, the table will duplicate factor analysis for 3 features.\n",
    "\"\"\"\n",
    "if exists(pcor_nologdta_file):\n",
    "    display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if exists(pcor_nologdta_file):\n",
    "    df_efa4 = pd.read_csv(join(output_dir, '{}_efa4.csv'.format(experiment_id)), index_col=0)\n",
    "    df_efa4.sort_index(inplace=True)\n",
    "    display(HTML(df_efa4.to_html(float_format=float_format_func)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if exists(pcor_nologdta_file):\n",
    "    df_efa4fv = pd.read_csv(join(output_dir, '{}_efa4fv.csv'.format(experiment_id)), index_col=0)\n",
    "    display(HTML(df_efa4fv.to_html(float_format=float_format_func)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if exists(pcor_nologdta_file):\n",
    "    df_efa4fcor = pd.read_csv(join(output_dir, '{}_efa4fcor.csv'.format(experiment_id)), index_col=0)\n",
    "    display(HTML(df_efa4fcor.to_html(float_format=float_format_func)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if model_name == 'empWt':\n",
    "    display(HTML('<h2>Model diagnostics</h2>'))\n",
    "    display(Markdown(\"These are standard plots for model diagnostics for the main model. All information is computed based on the training set.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if we used an R linear model, then we first just show a summary of that model\n",
    "if model_name == 'empWt':\n",
    "    %R -i experiment_id,output_dir library(etsmodels); modelfile <- paste0(output_dir, \"/\", experiment_id, \"_\", \"empWt.Rmodel\"); load(modelfile); par(mfrow=c(2, 2)); plot(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(etsmodels)\n",
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pip\n",
    "sorted([\"%s==%s\" % (i.key, i.version) for i in pip.get_installed_distributions()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "// Code to dynamically generate table of contents at the top of the HTML file\n",
    "var tocEntries = ['<ul>'];\n",
    "var anchors = $('a.anchor-link');\n",
    "var headingTypes = $(anchors).parent().map(function() { return $(this).prop('tagName')});\n",
    "var headingTexts = $(anchors).parent().map(function() { return $(this).text()});\n",
    "var subList = false;\n",
    "\n",
    "$.each(anchors, function(i, anch) {\n",
    "    var hType = headingTypes[i];\n",
    "    var hText = headingTexts[i];\n",
    "    hText = hText.substr(0, hText.length - 1);\n",
    "    if (hType == 'H2') {\n",
    "        if (subList) {\n",
    "            tocEntries.push('</ul>')\n",
    "            subList = false;\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "    else if (hType == 'H3') {\n",
    "        if (!subList) {\n",
    "            subList = true;\n",
    "            tocEntries.push('<ul>')\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "});\n",
    "tocEntries.push('</ul>')\n",
    "$('#toc').html(tocEntries.join(' '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
