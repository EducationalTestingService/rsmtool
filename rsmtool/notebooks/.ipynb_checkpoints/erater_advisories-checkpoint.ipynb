{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-Rater Advisories\n",
    "\n",
    "This section shows the distributions of the e-rater advisories for all responses in the original evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First we need to assemble all the original responses since df_test_other columns\n",
    "# includes only the responses used for the analysis. We do this rather than \n",
    "# using df_train_orig to make use of the renamed columns.\n",
    "\n",
    "# The first step is to combine the pred_proc and the test_other_columns frames \n",
    "# to get the data frame which contains both sc1 (pred_proc) and advisories (other_columns)\n",
    "df_used_data = pd.merge(df_pred_preproc, df_test_other_columns)\n",
    "\n",
    "# Then we add the metadata to get the prompt column\n",
    "df_used_data = df_used_data.merge(df_test_metadata)\n",
    "\n",
    "# we then check when the columns we need are available and raise an error if not\n",
    "missing_columns = set(['sc1', 'prompt', 'advisoryflag']).difference(df_used_data.columns)\n",
    "if missing_columns:\n",
    "   display(Markdown(\"Section skipped because the test data does not contain the following columns \"\n",
    "                         \"required for the analysis: {}\".format(' ,'.join(missing_columns))))\n",
    "else:\n",
    "    # we add all excluded responses\n",
    "    # do we have any responses excluded due to flags?\n",
    "    try:\n",
    "        df_used_and_flagged = pd.merge(df_used_data, df_test_responses_with_excluded_flags, how='outer')\n",
    "    except NameError:\n",
    "        df_used_and_flagged = df_used_data.copy()\n",
    "\n",
    "    # do we have any responses excluded due to non-numeric values?\n",
    "    try:\n",
    "        df_all_data = pd.merge(df_used_and_flagged, df_test_excluded, how='outer')\n",
    "    except NameError:\n",
    "        df_all_data = df_used_and_flagged.copy()\n",
    "\n",
    "    # check that the new data has the same N responses as the original data\n",
    "    # and select only the columns we need\n",
    "    assert (len(df_all_data) == len(df_test_orig))\n",
    "    df = df_all_data[['spkitemid', 'sc1', 'advisoryflag', 'prompt']].copy()\n",
    "\n",
    "    # finally we can start the advisory analysis\n",
    "    \n",
    "    # these are the possible e-rater individual advisory values\n",
    "    possible_advisory_values = [2**i for i in range(12)]\n",
    "\n",
    "    # convert the 'advisoryflag' column to an integer\n",
    "    # since that will be more convenient for later analysis\n",
    "    df['advisoryflag'] = df['advisoryflag'].astype(int)\n",
    "\n",
    "    # create a data frame that indicates for each response, whether a particular \n",
    "    # advisory flag has fired or not, and contains the sc1 and prompt values\n",
    "    advisory_flag_names = ['Flag {}'.format(av) for av in possible_advisory_values]\n",
    "    data = []\n",
    "    for _, _, s, c, p in df.itertuples():\n",
    "        row = [c & v for v in possible_advisory_values] + [s, p]\n",
    "        data.append(row)\n",
    "    df_with_sc1 = pd.DataFrame(data, columns=advisory_flag_names + ['sc1', 'prompt'])\n",
    "\n",
    "    # convert all the 'Flag *' columns to boolean\n",
    "    for avn in advisory_flag_names:\n",
    "        df_with_sc1[avn] = df_with_sc1[avn].astype(bool)\n",
    "\n",
    "    # create a column that indicates whether a response had no advisory flags\n",
    "    df_with_sc1['None'] = df_with_sc1[advisory_flag_names].apply(lambda advisories: not(any(advisories)), axis=1)\n",
    "    advisory_flag_names.append('None')\n",
    "\n",
    "    # compute the total number of responses per prompt\n",
    "    totals = df_with_sc1.drop('sc1', axis=1).groupby('prompt').sum().sum(axis=1)\n",
    "\n",
    "    # compute the total number of flagged responses by prompt and score\n",
    "    df_advisory_flags_by_prompt_and_score = df_with_sc1.groupby(['prompt', 'sc1']).sum()\n",
    "\n",
    "    # now compute the percentage columns\n",
    "    for avn in advisory_flag_names:\n",
    "        df_advisory_flags_by_prompt_and_score['{} (%)'.format(avn)] = df_advisory_flags_by_prompt_and_score[avn] * 100 / totals\n",
    "\n",
    "    # reorder the columns to make the table easier to read\n",
    "    ordered_columns = list(itertools.chain(*zip(advisory_flag_names, ['{} (%)'.format(avn) for avn in advisory_flag_names])))\n",
    "    df_advisory_flags_by_prompt_and_score = df_advisory_flags_by_prompt_and_score[ordered_columns]    \n",
    "\n",
    "    # now we simply exclude 'sc1' from the analysis\n",
    "    # and just compute the numbers and percentages by prompts\n",
    "    df_advisory_flags_by_prompt = df_advisory_flags_by_prompt_and_score.reset_index().groupby('prompt').sum().drop('sc1', axis=1)\n",
    "\n",
    "    # reset the indices for both data frames because otherwise the HTML tables \n",
    "    # will not be dynamically sortable\n",
    "    df_advisory_flags_by_prompt_and_score.reset_index(inplace=True)\n",
    "    df_advisory_flags_by_prompt.reset_index(inplace=True)\n",
    "\n",
    "    # write out the files to disk\n",
    "    outfile = join(output_dir, '{}_test_advisories_by_prompt.csv'.format(experiment_id))\n",
    "    df_advisory_flags_by_prompt.to_csv(outfile, index=False)\n",
    "    outfile = join(output_dir, '{}_test_advisories_by_prompt_and_score.csv'.format(experiment_id))\n",
    "    df_advisory_flags_by_prompt_and_score.to_csv(outfile, index=False)\n",
    "    \n",
    "    # show the heading and the first table\n",
    "    display(Markdown(\"### Distribution of advisories by prompts\"))\n",
    "    display(Markdown(\"This table shows the distribution of advisory flags by prompts.\"))\n",
    "    display(Markdown(\"Advisory flag percentages above 1% are highlighted in <span style='color: red'>red</span>.\"))\n",
    "    general_formatter = partial(float_format_func, prec=2)\n",
    "    \n",
    "    # get a list of all the percentage columns and apply the\n",
    "    # formatting to them that highlight in red anything \n",
    "    # higher than 1%. Also, use a precision of 2 in general.\n",
    "    perc_columns = [c for c in df_advisory_flags_by_prompt.columns if re.match(r'Flag [0-9]+ \\(%\\)', c)]\n",
    "    formatter = partial(color_highlighter, high=1, prec=2)\n",
    "    formatter_dict = {c: formatter for c in perc_columns}\n",
    "    display(HTML(df_advisory_flags_by_prompt.to_html(classes=['sortable'], \n",
    "                                                     index=False, \n",
    "                                                     escape=False, \n",
    "                                                     formatters=formatter_dict,\n",
    "                                                     float_format=general_formatter)))\n",
    "\n",
    "    # show the heading and the second table with the same formatting options\n",
    "    display(Markdown(\"### Distribution of advisories by prompts and human scores\"))\n",
    "    display(Markdown(\"This table shows the distribution of advisory flags by prompts and human scores.\"))\n",
    "    display(HTML(\"Advisory flag percentages above 1% are highlighted in <span style='color: red'>red</span>.\"))\n",
    "    display(HTML(df_advisory_flags_by_prompt_and_score.to_html(classes=['sortable'], \n",
    "                                                               index=False, \n",
    "                                                               escape=False,\n",
    "                                                               formatters=formatter_dict,\n",
    "                                                               float_format=general_formatter)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
