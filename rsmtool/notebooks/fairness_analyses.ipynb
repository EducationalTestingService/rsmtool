{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional fairness analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a note for ETS users\n",
    "from rsmtool import HAS_RSMEXTRA\n",
    "if HAS_RSMEXTRA:\n",
    "    from rsmextra.settings import fairness_note\n",
    "    display(Markdown(fairness_note))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section presents additional fairness analyses described in detail in [Loukina et al. (2019)](https://aclweb.org/anthology/papers/W/W19/W19-4401/).\n",
    "These analyses consider separately different definitions of fairness and can assist in further trouble-shooting \n",
    "the observed subgroup differences. The evaluations focuses on three dimensions:\n",
    "\n",
    "* **Outcome fairness** measures:  \n",
    "\n",
    "    - *Overall score accuracy*: whether the automated scores are equally accurate for each group. The metric shows how much of the variance in squared error $(S-H)^2$ is explained by subgroup membership.\n",
    "\n",
    "    -  *Overall score difference*: whether the automated scores are consistently different from human scores for members of a certain group. The metric shows how much of the variance in actual error $S-H$ is explained by subgroup membership. \n",
    "\n",
    "The differences in the outcome fairness measures might be due to different mean scores (different score distributions) across subgroups or due to differential treatment of different subgroups by the scoring engine or both. \n",
    "\n",
    "* **Process fairness** measures:\n",
    "\n",
    "    - *Conditional score difference*: whether the automated scoring model assigns different scores to members from different groups despite them having the same construct proficiency. The metric shows how much additional variance in actual error ($S-H$) is explained by subgroup membership after controlling for human score, which can be thought of as a reasonable proxy for proficiency. \n",
    "\n",
    "The differences in process fairness measures indicate differential treatment of different subgroups by the scoring model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and auxiliary function that we will need \n",
    "# for the plot later on\n",
    "def errplot(x, y, xerr, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    data = kwargs.pop(\"data\")\n",
    "    # let's remove color from kwargs\n",
    "    color = kwargs.pop('color')\n",
    "    data.plot(x=x, y=y, xerr=xerr,\n",
    "              kind=\"barh\", ax=ax,\n",
    "              color=colors,\n",
    "              **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check if we already created the merged file in another notebook\n",
    "\n",
    "try:\n",
    "    df_pred_preproc_merged\n",
    "except NameError:\n",
    "    df_pred_preproc_merged = pd.merge(df_pred_preproc, df_test_metadata, on = 'spkitemid')\n",
    "    \n",
    "# check which score we are using\n",
    "system_score_column = \"scale_trim\" if use_scaled_predictions else \"raw_trim\"\n",
    "\n",
    "for group in groups_eval:\n",
    "    \n",
    "    display(Markdown(\"### Additional fairness evaluation by {}\".format(group)))\n",
    "    \n",
    "    # run the actual analyses. We are currently doing this in the notebook\n",
    "    # so that it is easy to skip these if necessary. The notebook\n",
    "    # and analyses are set up in a way that will make it easy to move these\n",
    "    # in future to the main pipeline and read in the outputs here. \n",
    "    fit_dict, fairness_container = get_fairness_analyses(df_pred_preproc_merged,\n",
    "                                                         group,\n",
    "                                                         system_score_column)\n",
    "    \n",
    "    # write the results to disk so that we can consider them in tests\n",
    "    write_fairness_results(fit_dict,\n",
    "                           fairness_container,\n",
    "                           group,\n",
    "                           output_dir,\n",
    "                           experiment_id,\n",
    "                           file_format)\n",
    "    \n",
    "    # first show summary results\n",
    "    df_fairness = fairness_container['fairness_metrics_by_{}'.format(group)]\n",
    "\n",
    "    \n",
    "    display(Markdown(\"The summary table shows the overall score accuracy (OSA), overall score difference (OSD) \"\n",
    "                    \"and conditional score difference (CSD). The first row reports the percentage of variance, \"\n",
    "                    \" explained by group membership, the second row shows $p$ value. \"\n",
    "                     \"{} was used as a reference category. \"\n",
    "                    \"Larger values of R2 indicate larger differences between subgroups. \" \n",
    "                    \"Further detail about each model can be found in [intermediate \"\n",
    "                    \"output files](#Links-to-Intermediate-Files).\".format(df_fairness['base_category'].values[0])))\n",
    "    \n",
    "    display(HTML(df_fairness.loc[['R2', 'sig'],\n",
    "                                 ['Overall score accuracy',\n",
    "                                  'Overall score difference',\n",
    "                                  'Conditional score difference']].to_html(classes='sortable',\n",
    "                                                                     float_format=float_format_func)))\n",
    "\n",
    "    \n",
    "    Markdown_str = [(\"The plots show error estimates for different categories for each group \"\n",
    "                    \"(squared error for OSA, raw error for OSD, and conditional raw error for CSD) \"\n",
    "                    \"The estimates have been adjusted for the value of the group used as the Intercept. \"\n",
    "                    \"Black lines show 95% confidence intervals estimated by the model.\")]\n",
    "    \n",
    "    # if we only care about groups above threshold, identify those.\n",
    "    \n",
    "    category_counts = df_pred_preproc_merged[group].value_counts()        \n",
    "    \n",
    "    if group in min_n_per_group:\n",
    "        Markdown_str.append(\"While the models were fit to all data, the plots only show estimates for \"\n",
    "                            \"categories with more than {} members and the Intercept ({}).\".format(min_n_per_group[group],\n",
    "                                                                                                  df_fairness['base_category'].values[0]))\n",
    "        \n",
    "        groups_by_size = category_counts[category_counts >= min_n_per_group[group]].index\n",
    "        df_pred_preproc_selected = df_pred_preproc_merged[df_pred_preproc_merged[group].isin(groups_by_size)].copy()\n",
    "    else:\n",
    "        groups_by_size = category_counts.index\n",
    "        df_pred_preproc_selected = df_pred_preproc_merged.copy()\n",
    "    \n",
    "    display(Markdown('\\n'.join(Markdown_str)))\n",
    "   \n",
    "    \n",
    "    if len(groups_by_size) > 0:\n",
    "\n",
    "        # assemble all coefficients into a long data frame\n",
    "        all_coefs = []\n",
    "        for metrics in ['osa', 'csd', 'osd']:\n",
    "            df_metrics = fairness_container['estimates_{}_by_{}'.format(metrics, group)].copy()\n",
    "            # compute adjusted error estimates by adding the value of the Intercept\n",
    "            # to all non-Intercept values\n",
    "            non_index_cols = [r for r in df_metrics.index if not \"Intercept\" in r]\n",
    "            index_col = [r for r in df_metrics.index if \"Intercept\" in r]\n",
    "            df_metrics['error_estimate'] = df_metrics['estimate']\n",
    "            df_metrics.loc[non_index_cols,\n",
    "                          'error_estimate'] = df_metrics.loc[non_index_cols,\n",
    "                                                            'estimate'] + df_metrics.loc[index_col,\n",
    "                                                                                        'estimate'].values\n",
    "            # create a column for metrics\n",
    "            df_metrics['metrics'] = metrics\n",
    "            # only use groups with values above threshold and the intercept\n",
    "            df_metrics[group] = df_metrics.index\n",
    "            df_metrics_selected = df_metrics[df_metrics[group].isin(groups_by_size) |\n",
    "                                             (df_metrics[group] == index_col[0])]\n",
    "            all_coefs.append(df_metrics_selected)\n",
    "\n",
    "\n",
    "        # show coefficient plots\n",
    "        # define groups and color palette\n",
    "        colors = sns.color_palette(\"Greys_r\", len(groups_by_size))\n",
    "\n",
    "        df_coefs_all = pd.concat(all_coefs)\n",
    "\n",
    "        # compute the size of the confidence interval from the boundary\n",
    "        df_coefs_all['CI'] = np.abs(df_coefs_all['[0.025'] - df_coefs_all['estimate'])\n",
    "\n",
    "        # plot the coefficients\n",
    "        with sns.axes_style('whitegrid'), sns.plotting_context('notebook', font_scale=2):\n",
    "            g = sns.FacetGrid(df_coefs_all, col=\"metrics\",\n",
    "                              height=10, col_order = ['osa', 'osd', 'csd'])\n",
    "            g.map_dataframe(errplot, group, \"error_estimate\",  \"CI\").set_axis_labels(\"Error estimate\",\n",
    "                                                                                    group)\n",
    "\n",
    "            imgfile = join(figure_dir, '{}_fairness_estimates_{}.svg'.format(experiment_id, group))\n",
    "            plt.savefig(imgfile)\n",
    "            if use_thumbnails:\n",
    "                show_thumbnail(imgfile, next(id_generator))\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "        # Show the conditional score plot\n",
    "        markdown_str = [(\"The plot shows average {} system score for each \"\n",
    "                        \"group conditioned \"\n",
    "                         \"on human score.\".format(system_score_column))]\n",
    "        if group in min_n_per_group:\n",
    "            markdown_str.append(\"The plot only shows estimates for \"\n",
    "                               \"categories with more than {} members and the\"\n",
    "                                \"reference category ({}).\".format(min_n_per_group[group],\n",
    "                                                          df_fairness['base_category'].values[0]))\n",
    "        \n",
    "        display(Markdown('\\n'.join(markdown_str)))\n",
    "        \n",
    "\n",
    "\n",
    "        with sns.axes_style('whitegrid'), sns.plotting_context('notebook', font_scale=1.2):\n",
    "            p = sns.catplot(x='sc1', y=system_score_column,\n",
    "                            hue=group, \n",
    "                            hue_order = groups_by_size,\n",
    "                            palette=colors,\n",
    "                            legend_out=True,\n",
    "                            kind=\"point\",\n",
    "                            data=df_pred_preproc_selected)\n",
    "\n",
    "            #plt.tight_layout(h_pad=1.0)\n",
    "            imgfile = join(figure_dir, '{}_conditional_score_{}.svg'.format(experiment_id, group))\n",
    "            plt.savefig(imgfile)\n",
    "            if use_thumbnails:\n",
    "                show_thumbnail(imgfile, next(id_generator))\n",
    "            else:\n",
    "                plt.show()\n",
    "    else:\n",
    "        display(Markdown(\"None of the groups in {} had {} or more responses in the evaluation set.\".format(group,\n",
    "                                                                                     min_n_per_group[group])))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
