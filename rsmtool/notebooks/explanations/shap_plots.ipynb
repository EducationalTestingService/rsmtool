{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa07b05-dcda-4828-8a1c-6af8b1536a2d",
   "metadata": {},
   "source": [
    "### SHAP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0eb6dd-ca01-43fb-8289-8355ee1839ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "msg = (f\"These are general SHAP plots computed over the entire dataset. \"\n",
    "       f\"These plots display the **top {num_display_features} features** \"\n",
    "       \"according to the specific ranking metric for each plot. This number \"\n",
    "       \"can be adjusted by specifying a different integer value value for \"\n",
    "       \"the `num_features_to_display` option in the `rsmexplain` configuration file.\")\n",
    "display(Markdown(msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b141a5fc-5c41-4d94-8e04-9fb8ede46f91",
   "metadata": {},
   "source": [
    "#### Heatmap Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb56eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_heatmap = (\"This plot offers a condensed, high-level overview of the entire dataset. \"\n",
    "               \"It shows the instances on the x-axis, the model decisions on the y-axis, \"\n",
    "               \"and the SHAP values encoded using a color scale. The instances are clustered \"\n",
    "               \"by their explanation similarity. This means examples with similar model \"\n",
    "               \"outputs for similar reasons are grouped together. The output of the \"\n",
    "               \"model is shown above the heatmap matrix (centered around the \"\n",
    "               \"explanationâ€™s base value), and the mean absolute impact of each model \"\n",
    "               \"input shown as a bar plot on the right hand side of the plot. Features \"\n",
    "               \"are ranked by mean absolute impact, i.e., the topmost feature in this \"\n",
    "               \"plot has the highest average impact on model decisions for the given dataset.\")\n",
    "if not has_single_example:\n",
    "    display(Markdown(msg_heatmap))\n",
    "    shap.plots.heatmap(explanations, max_display=num_display_features, show=False)\n",
    "    imgfile = join(fig_path, f\"{experiment_id}_heatmap.svg\")\n",
    "    plt.savefig(imgfile, bbox_inches=\"tight\")\n",
    "else:\n",
    "    display(Markdown(\"Heatmap plots are unavailable when explaining a single example.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b1540-67a4-4166-a6f9-9d7873f013ce",
   "metadata": {},
   "source": [
    "#### Heatmap plot ordered by prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9927f2a-b0f2-4b59-af75-7bf0aba19e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_heatmap_ordered = (\"This heatmap plot has its x-axis sorted in descending order of the \"\n",
    "                       \"model prediction, starting at the highest prediction down to the \"\n",
    "                       \"lowest prediction value.\\n This plot can be useful to spot features \"\n",
    "                       \"that display counter-intuitive behaviors or clustering. We expect \"\n",
    "                       \"the feature colors (which represent the shap value) to be on a \"\n",
    "                       \"gradient if they correlate with the model predictions. If the \"\n",
    "                       \"colors instead display clusters, then the feature does not \"\n",
    "                       \"necessarily correlate with the prediction.\")\n",
    "if not has_single_example:\n",
    "    display(Markdown(msg_heatmap_ordered))\n",
    "    shap.plots.heatmap(explanations,instance_order=explanations.sum(1), max_display=num_display_features, show=False)\n",
    "    imgfile = join(fig_path, f\"{experiment_id}_heatmap_prediction_order.svg\")\n",
    "    plt.savefig(imgfile, bbox_inches='tight')\n",
    "else:\n",
    "    display(Markdown(\"Heatmap plots are unavailable when explaining a single example.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d377663e",
   "metadata": {},
   "source": [
    "#### Global Bar Plot\n",
    "\n",
    "This plot gives a quick overview over the SHAP values of the data passed. Features are ranked by mean absolute impact. The number to the right of the bar represents the mean absolute shap value of that feature. The higher the mean shap value of your feature is, the higher the average contribution of that feature to a model decision is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20025cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(explanations, max_display=num_display_features, show=False)\n",
    "imgfile = join(fig_path, f\"{experiment_id}_global_bar.svg\")\n",
    "plt.savefig(imgfile, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd7cfab",
   "metadata": {},
   "source": [
    "#### Beeswarm Plot\n",
    "\n",
    "The beeswarm plot gives an information-dense overview over of the SHAP values. Each example in the data is represented by a dot on the given feature row in the plot.  The x-axis position of the dot is determined by the Shap value of that feature in that given decision. The further away from 0 a dot is, the higher the impact of that feature was for that decision. This impact can be negative (to the left) or positive (to the right).\n",
    "\n",
    "The feature value (*not* the Shap value!) is denoted by plot colors. Red signifies a high feature value and blue signifies a low feature value. Features are ranked by the mean-absolute impact they have on a model decision. The top feature in this plot will have the highest mean absolute impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(explanations, max_display=num_display_features, show=False)\n",
    "imgfile = join(fig_path, f\"{experiment_id}_beeswarm.svg\")\n",
    "plt.savefig(imgfile, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd912d",
   "metadata": {},
   "source": [
    "#### Beeswarm ranked by maximum impact\n",
    "\n",
    "This beeswarm plot is ranked by the absolute maximum impact of the features. The highest ranked feature in this plot will have the highest impact on the model decision. This can be useful to discover features that do not have a high average impact have a high maximum impact. This could represent features with outlier values, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38012a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(explanations, order=explanations.abs.max(0), max_display=num_display_features, show=False)\n",
    "imgfile = join(fig_path, f\"{experiment_id}_beeswarm_max_impact.svg\")\n",
    "plt.savefig(imgfile, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ebe795",
   "metadata": {},
   "source": [
    "#### Absolute mean beeswarm\n",
    "\n",
    "This plot is equivalent to the original beeswarm plot, but has the values transformed for absolute impact. This is useful to see how much impact a feature has on average while also displaying where those impact values are clustered. This can be considered a richer version of the global bar lot.\n",
    "\n",
    "**Important**: The beeswarm plot is known to have ordering issues due to a rounding effect. If the feature order does not match the order in the bar plot, then assume that the order in the bar plot is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ff0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(explanations.abs, order=explanations.abs.mean(0), max_display=num_display_features, show=False)\n",
    "imgfile = join(fig_path, f\"{experiment_id}_beeswarm_abs_impact.svg\")\n",
    "plt.savefig(imgfile, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f3472-72b6-47bb-b581-be3136f0eb92",
   "metadata": {},
   "source": [
    "#### Decision Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = (\"SHAP decision plots show how models arrive at their predictions. \"\n",
    "       \"Each plotted line explains a single model prediction. \")\n",
    "\n",
    "if has_single_example:\n",
    "    display(Markdown(msg))\n",
    "    base_values = np.array([explanations.base_values[0]])\n",
    "    shap.decision_plot(base_values, explanations.values, feature_names=explanations.feature_names, show=False)\n",
    "    path = join(fig_path, f\"{experiment_id}_decision.svg\")\n",
    "    plt.savefig(path,dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "    display(Markdown(\"Decision plot is only available when explaining a single example.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a79d00-664a-4911-8b8f-623ee37eaef4",
   "metadata": {},
   "source": [
    "#### Waterfall Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = (\"The waterfall plot is designed to visually display how the SHAP values \"\n",
    "       \"(contributions) of each feature moves the model prediction from our prior \"\n",
    "       \"expectation under the background data distribution, to the final model \"\n",
    "       \"prediction for the chosen example given the contribution from all \"\n",
    "       \"the features. Each row shows how the positive (red) or negative \"\n",
    "       \"(blue) contribution of each feature moves the value from the expected \"\n",
    "       \"model prediction for the background dataset to the model prediction \"\n",
    "       \"for the chosen example.\")\n",
    "\n",
    "if has_single_example:\n",
    "    display(Markdown(msg))\n",
    "    wplot = shap.plots.waterfall(explanations[0], max_display=num_display_features, show=False)\n",
    "    path = join(fig_path, f\"{experiment_id}_waterfall.svg\")\n",
    "    plt.savefig(path,dpi=300, bbox_inches='tight')\n",
    "    display(wplot)\n",
    "else:\n",
    "    display(Markdown(\"Waterfall plot is only available when explaining a single example.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
