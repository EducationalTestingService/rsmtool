{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f403f94",
   "metadata": {},
   "source": [
    "## Description of the Data Passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22841c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"A total of {len(ids)} examples were passed to the explainer.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ids) < 100:\n",
    "    msg = f\"The following examples were selected for explanation: {list(ids.values())}\"\n",
    "else:\n",
    "     msg = (\"Too many examples (>100) were selected to be displayed here. \"\n",
    "            f\"Please refer to the `{experiment_id}_ids.pkl` file in the `output` directory.\" )\n",
    "\n",
    "display(Markdown(msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aeb967",
   "metadata": {},
   "source": [
    "### Background Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171e672",
   "metadata": {},
   "source": [
    "The background data is responsible for generating the *base value* of your SHAP explanations. Specifically, this is the average model prediction in the background data passed to `rsmexplain`.\n",
    "\n",
    "SHAP computes feature contributions by replacing feature values in the data you wish to explain with values sampled from the background set and measuring changes in the prediction. Therefore, the background data is crucial in helping us understand the impact of a feature moving from its 'baseline' or 'average' state to the current state. This movement is what the SHAP values shown in the report below quantify. This means that as long as **sufficiently large and diverse** background data is used, the SHAP values for individual examples in the explain data can be considered reliable.\n",
    "\n",
    "However, running `rsmexplain` on very large background datasets can be computationally expensive and time-consuming. One trick we can use is to use k-means clustering to \"summarize\" and reduce the size of the background dataset without losing too much information. To do this, we run a k-means clustering algorithm on the background dataset which yields `k` clusters of feature values, each represented by a \"centroid\" (the average of all data points in the cluster). These centroids are then used to compute the SHAP values, i.e., when we \"omit\" a feature and go to replace its value, we do so by sampling from one of the centroids rather than the original dataset. \n",
    "\n",
    "`rsmexplain` applies this trick by default. The number of clusters `k` can be specified via the \"background_kmeans_size\" option in its configuration file. The default value is 500 since that has been shown to represent a good compromise between accuracy and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef8cb85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if background_kmeans_size:\n",
    "    msg = (f\"For this run, a value of {background_kmeans_size} was used for `background_kmeans_size`. \"\n",
    "           \"A smaller value will be faster but even less accurate.\")\n",
    "else:\n",
    "    msg = (\"For this run, the default value of 500 was used for `background_kmeans_size`. \"\n",
    "           \"A smaller value will be faster but less accurate.\")\n",
    "display(Markdown(msg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
