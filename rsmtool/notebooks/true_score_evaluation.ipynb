{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_or_scaled = \"scaled\" if use_scaled_predictions else \"raw\"\n",
    "true_eval_file = join(output_dir, '{}_true_score_eval.{}'.format(experiment_id, file_format))\n",
    "if exists(true_eval_file): \n",
    "    df_true_eval = DataReader.read_from_file(true_eval_file, index_col=0)\n",
    "    df_true_eval.replace({np.nan: '-'}, inplace=True)\n",
    "    variance_columns = ['N','N_single','N_double','h1_var_single','h1_var_double', 'h2_var_double','true_var']\n",
    "    prmse_columns = ['N','N_single', 'N_double','sys_var_single','sys_var_double','mse_true','prmse_true']\n",
    "    df_human_variance = df_true_eval[variance_columns].iloc[0:1]\n",
    "    df_human_variance.index = ['human']\n",
    "    df_prmse = df_true_eval[prmse_columns]\n",
    "    \n",
    "    markdown_strs = ['### True score evaluations']\n",
    "    markdown_strs.append(\"The tables in this section show how well system scores can \"\n",
    "                        \"predict *true* scores. According to Test theory, a *true* score \"\n",
    "                        \"is a score that would have been obtained if there were no errors \"\n",
    "                        \"in measurement. While true scores cannot be observed, the variance \"\n",
    "                        \"of true scores and the prediction error can be estimated using observed \"\n",
    "                        \"human scores when multiple human ratings are available for a subset of \"\n",
    "                        \"responses. In this notebook these are estimated using human scores for \"\n",
    "                        \"responses in the evaluation set.\")\n",
    "    \n",
    "    if context == 'rsmtool':\n",
    "        label_column = \"test_label_column\"\n",
    "    else:\n",
    "        label_column = \"human_score_column\"\n",
    "\n",
    "    markdown_strs.append(\"\\n Note that the analyses in this section assume that the values \"\n",
    "                        \"in `{}` and `second_human_score_column` are independent scores \"\n",
    "                        \"from different raters or groups of raters. These analyses are \"\n",
    "                        \"not applicable to a situation where `{}` contains an average \"\n",
    "                        \"score from multiple raters\".format(label_column, label_column))\n",
    "    \n",
    "    markdown_strs.append(\"#### Variance of human scores\")\n",
    "    markdown_strs.append(\"The table below shows variance of both sets of human scores \"\n",
    "                        \"for the whole evaluation set and for the subset of responses \"\n",
    "                        \"that were double-scored. Large differences in variance between \"\n",
    "                        \"the two human scores require further investigation. The last column \"\n",
    "                        \"shows estimated true score variance. \")\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    pd.options.display.width=10\n",
    "    display(HTML('<span style=\"font-size:95%\">'+ df_human_variance.to_html(classes=['sortable'], \n",
    "                                                               escape=False,\n",
    "                                                               float_format=float_format_func) + '</span>'))\n",
    "    \n",
    "    markdown_strs = [\"#### Proportional reduction in mean squared error (PRMSE)\"]\n",
    "    markdown_strs.append(\"The table shows the variance of system scores for single-scored \"\n",
    "                        \"and double-scored responses, and mean squared error (MSE) and \"\n",
    "                        \"proportional reduction in mean squared error (PRMSE) for \"\n",
    "                        \"predicting a true score with system score. As for other evaluations, \"\n",
    "                        \"these results are computed on the evaluation set. `raw_trim` scores \"\n",
    "                        \"are truncated to [{}, {}]. `raw_trim_round` scores are computed \"\n",
    "                        \"by first truncating and then rounding the predicted score. Scaled scores \"\n",
    "                        \"are computed by re-scaling the predicted scores using mean and standard \"\n",
    "                        \"deviation of human scores as observed on the training data and mean and \"\n",
    "                        \"standard deviation of machine scores as predicted for the training set.\".format(min_score,\n",
    "                                                                                                         max_score))\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    pd.options.display.width=10\n",
    "    display(HTML('<span style=\"font-size:95%\">'+ df_prmse.to_html(classes=['sortable'], \n",
    "                                                               escape=False,\n",
    "                                                               float_format=float_format_func) + '</span>'))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
