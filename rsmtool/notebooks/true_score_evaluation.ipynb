{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_strs = ['### True score evaluations']\n",
    "\n",
    "raw_or_scaled = \"scaled\" if use_scaled_predictions else \"raw\"\n",
    "true_eval_file = join(output_dir, '{}_true_score_eval.{}'.format(experiment_id, file_format))\n",
    "if exists(true_eval_file): \n",
    "    df_true_eval = DataReader.read_from_file(true_eval_file, index_col=0)\n",
    "    df_true_eval.replace({np.nan: '-'}, inplace=True)\n",
    "    prmse_columns = ['N','N raters', 'N single', 'N multiple', \n",
    "                     'Variance of errors', 'True score var',\n",
    "                     'MSE true', 'PRMSE true']\n",
    "    df_prmse = df_true_eval[prmse_columns]\n",
    "\n",
    "    markdown_strs.append(\"The tables in this section show how well system scores can \"\n",
    "                         \"predict *true* scores. According to Test theory, a *true* score \"\n",
    "                         \"is a score that would have been obtained if there were no errors \"\n",
    "                         \"in measurement. While true scores cannot be observed, the variance \"\n",
    "                         \"of true scores and the prediction error can be estimated using observed \"\n",
    "                         \"human scores when multiple human ratings are available for a subset of \"\n",
    "                         \"responses.\")\n",
    "\n",
    "    if rater_error_variance is None: \n",
    "        \n",
    "        rater_variance_source = 'estimated'\n",
    "        # if we estimated rater error variance from the data,\n",
    "        # we display the variance of the two human raters\n",
    "        # so that the user can verify there is no bias\n",
    "        # We get that data from existing analyses\n",
    "        df_human_variance = df_consistency[['N', 'h1_sd', 'h2_sd']].copy()\n",
    "        df_human_variance['N_double'] = df_human_variance['N']\n",
    "        df_human_variance['h1_var (double)'] = df_human_variance['h1_sd']**2\n",
    "        df_human_variance['h2_var (double)'] = df_human_variance['h2_sd']**2\n",
    "        df_human_variance['N_total'] = df_eval.iloc[0]['N']\n",
    "        df_human_variance['h1_var (single)'] = df_eval.iloc[0]['h_sd']**2\n",
    "        df_human_variance.index = ['human']\n",
    "\n",
    "\n",
    "        if context == 'rsmtool':\n",
    "            label_column = \"test_label_column\"\n",
    "        else:\n",
    "            label_column = \"human_score_column\"\n",
    "\n",
    "        markdown_strs.append(\"In this notebook the variance of true scores is estimated using \"\n",
    "                             \"the human ratings available for \"\n",
    "                             \"responses in the evaluation set. Note that the analyses in this \"\n",
    "                             \"section assume that the values \"\n",
    "                            \"in `{}` and `second_human_score_column` are independent scores \"\n",
    "                            \"from different raters or groups of raters. These analyses are \"\n",
    "                            \"not applicable to a situation where `{}` contains an average \"\n",
    "                            \"score from multiple raters\".format(label_column, label_column))\n",
    "\n",
    "        markdown_strs.append(\"#### Variance of human scores\")\n",
    "        markdown_strs.append(\"The table below shows variance of both sets of human scores \"\n",
    "                            \"for the whole evaluation set and for the subset of responses \"\n",
    "                            \"that were double-scored. Large differences in variance between \"\n",
    "                            \"the two human scores require further investigation.\")\n",
    "        display(Markdown('\\n'.join(markdown_strs)))\n",
    "        pd.options.display.width=10\n",
    "        column_order = ['N_total', 'N_double', 'h1_var (single)', 'h1_var (double)', 'h2_var (double)']\n",
    "        display(HTML('<span style=\"font-size:95%\">'+ df_human_variance[column_order].to_html(classes=['sortable'], \n",
    "                                                                                            escape=False,\n",
    "                                                                                            float_format=float_format_func) + '</span>'))\n",
    "    else:\n",
    "        markdown_strs.append(\"In this notebook the variance of true scores was \"\n",
    "                            \"estimated using the value of rater error variance \"\n",
    "                            \"supplied by the user ({})\".format(rater_error_variance))\n",
    "        display(Markdown('\\n'.join(markdown_strs)))\n",
    "        rater_variance_source = 'supplied'\n",
    "    \n",
    "    \n",
    "    markdown_strs = [\"#### Proportional reduction in mean squared error (PRMSE)\"]\n",
    "    markdown_strs.append(\"The table shows {} variance of human rater errors, \"\n",
    "                         \"true score variance, mean squared error (MSE) and \"\n",
    "                         \"proportional reduction in mean squared error (PRMSE) for \"\n",
    "                         \"predicting a true score with system score. As for other evaluations, \"\n",
    "                         \"these results are computed on the evaluation set. `raw_trim` scores \"\n",
    "                         \"are truncated to [{}, {}]. `raw_trim_round` scores are computed \"\n",
    "                         \"by first truncating and then rounding the predicted score. Scaled scores \"\n",
    "                         \"are computed by re-scaling the predicted scores using mean and standard \"\n",
    "                         \"deviation of human scores as observed on the training data and mean and \"\n",
    "                         \"standard deviation of machine scores as predicted for the training set.\".format(rater_variance_source,\n",
    "                                                                                                          min_score,\n",
    "                                                                                                          max_score))\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    pd.options.display.width=10\n",
    "    display(HTML('<span style=\"font-size:95%\">'+ df_prmse.to_html(classes=['sortable'], \n",
    "                                                               escape=False,\n",
    "                                                               float_format=float_format_func) + '</span>'))\n",
    "else:\n",
    "    markdown_strs.append(\"The configuration file did not specify \"\n",
    "                         \"`second_human_score_column` or `rater_error_variance`. \"\n",
    "                         \"At least one of these must be specified to compute \"\n",
    "                         \"evaluations against true scores.\")\n",
    "    display(Markdown('\\n'.join(markdown_strs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
