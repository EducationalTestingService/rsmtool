{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall association statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables in this section show the standard association metrics between human scores and different types of machine scores. These results are computed on the evaluation set. The scores for each model have been truncated to values indicated in `truncation range`. When indicated, scaled scores are computed by re-scaling the predicted scores using mean and standard deviation of human scores as observed on the training data and mean and standard deviation of machine scores as predicted for the training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_evals(model_list, file_format_summarize):\n",
    "\n",
    "    has_missing_trims = False\n",
    "\n",
    "    evals = []\n",
    "    for (model_id, model_name, config, csvdir, file_format) in model_list:\n",
    "        csv_file = os.path.join(csvdir, '{}_eval_short.{}'.format(model_id, file_format))\n",
    "        if os.path.exists(csv_file):\n",
    "            df_eval = DataReader.read_from_file(csv_file, index_col=0)\n",
    "            df_eval.index = [model_name]\n",
    "            \n",
    "            # figure out whether the score was scaled\n",
    "            df_eval['system score type'] = 'scale' if config.get('use_scaled_predictions') == True or config.get('scale_with') is not None else 'raw'        \n",
    "\n",
    "            # we want to display the truncation range, but this is slightly complicated\n",
    "            # we first check to see if the post-processing params file exists; if it does,\n",
    "            # we grab the trim_min and trim_max values from that file (which still could be None!)            \n",
    "            trim_min, trim_max = None, None\n",
    "            postproc_file = os.path.join(csvdir, '{}_postprocessing_params.{}'.format(model_id, file_format))\n",
    "            if os.path.exists(postproc_file):\n",
    "                df_postproc = DataReader.read_from_file(postproc_file)\n",
    "                trim_min = df_postproc['trim_min'].values[0]\n",
    "                trim_max = df_postproc['trim_max'].values[0] \n",
    "    \n",
    "            # if the trim_min or trim_max is still None, we then grab whatever is in the config\n",
    "            trim_min = config.get('trim_min') if trim_min is None else trim_min\n",
    "            trim_max = config.get('trim_max') if trim_max is None else trim_max\n",
    "            \n",
    "            # finally, we calculate the max and min scores; if we couldn't get any trim values,\n",
    "            # then we default these to `?` and the set `has_missing_trims=True`\n",
    "            if trim_min is None:\n",
    "                min_score, has_missing_trims = '?', True\n",
    "            else:\n",
    "                min_score = float(trim_min) - config.get('trim_tolerance', 0.4998)\n",
    "            if trim_max is None:\n",
    "                max_score, has_missing_trims = '?', True\n",
    "            else:\n",
    "                max_score = float(trim_max) + config.get('trim_tolerance', 0.4998)        \n",
    "\n",
    "            df_eval['truncation range'] = \"[{}, {}]\".format(min_score, max_score)\n",
    "            \n",
    "            # rename the columns to remove reference to scale/raw scores\n",
    "            new_column_names = [col.split('.')[0] if not 'round' in col \n",
    "                                else '{} (rounded)'.format(col.split('.')[0])\n",
    "                                for col in df_eval.columns ]\n",
    "            df_eval.columns = new_column_names\n",
    "            evals.append(df_eval)\n",
    "\n",
    "    if len(evals) > 0:\n",
    "        df_evals = pd.concat(evals, sort=True)\n",
    "    else:\n",
    "        df_evals = pd.DataFrame()\n",
    "    return df_evals, has_missing_trims\n",
    "\n",
    "df_eval, has_missing_trims = read_evals(model_list, file_format_summarize)\n",
    "\n",
    "if has_missing_trims:\n",
    "    display(Markdown('**Note:** The minimum and/or maximum scores after truncation could not be '\n",
    "                     'be computed in some cases. This is because `trim_min` and/or `trim_max` '\n",
    "                     'could not be found in either the configuration file or the postprocessing '\n",
    "                     'parameters file. Scores that could not be computed are shown as `?`.'))\n",
    "if not df_eval.empty:\n",
    "    writer = DataWriter(summary_id)\n",
    "    writer.write_experiment_output(output_dir,\n",
    "                                   {'eval_short': df_eval},\n",
    "                                   index=True,\n",
    "                                   file_format=file_format_summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive holistic score statistics\n",
    "\n",
    "The table shows distributional properties of human and system scores. SMD values lower then -0.15 or higher than 0.15 are <span class=\"highlight_color\">highlighted</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.width=10\n",
    "formatter = partial(color_highlighter, low=-0.15, high=0.15)\n",
    "if not df_eval.empty:\n",
    "     display(HTML(df_eval[['N', 'system score type', \"truncation range\", 'h_mean', 'h_sd', \n",
    "                           'sys_mean', 'sys_sd',  'SMD']].to_html(index=True,\n",
    "                                                                  classes=['sortable'],\n",
    "                                                                  escape=False,\n",
    "                                                                  formatters={'SMD': formatter},\n",
    "                                                                  float_format=int_or_float_format_func)))\n",
    "else:\n",
    "     display(Markdown(\"No information available for any of the models\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Association statistics\n",
    "\n",
    "The table shows the standard association metrics between human scores and machine scores. Note that some evaluations (`*_trim_round`) are based on rounded scores computed by first truncating and then rounding the predicted score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_eval.empty:\n",
    "    wtkappa_col = 'wtkappa' if 'wtkappa' in df_eval else 'wtkappa (rounded)'\n",
    "    display(HTML(df_eval[['N',\n",
    "                          'system score type',\n",
    "                          'corr', 'R2', 'RMSE',\n",
    "                          wtkappa_col,\n",
    "                          'kappa (rounded)',\n",
    "                          'exact_agr (rounded)',\n",
    "                          'adj_agr (rounded)']].to_html(index=True,\n",
    "                                                        classes=['sortable'],\n",
    "                                                        escape=False,\n",
    "                                                        float_format=int_or_float_format_func)))\n",
    "else:\n",
    "    display(Markdown(\"No information available for any of the models\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
