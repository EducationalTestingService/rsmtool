{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall association statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables in this section show the standard association metrics between human scores and different types of machine scores. These results are computed on the evaluation set. The scores for each model have been truncated to [min-0.4998, max+.4998].When indicated, scaled scores are computed by re-scaling the predicted scores using mean and standard deviation of human scores as observed on the training data and mean and standard deviation of machine scores as predicted for the training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_evals(model_list, file_format_summarize):\n",
    "    evals = []\n",
    "    for (model_id, config, csvdir, file_format) in model_list:\n",
    "        csv_file = os.path.join(csvdir, '{}_eval_short.{}'.format(model_id, file_format))\n",
    "        if os.path.exists(csv_file):\n",
    "            df_eval = DataReader.read_from_file(csv_file, index_col=0)\n",
    "            df_eval.index = [model_id]\n",
    "            \n",
    "            # figure out whether the score was scaled\n",
    "            df_eval['system score type'] = 'scale' if config.get('use_scaled_predictions') == True or config.get('scale_with') is not None else 'raw'        \n",
    "            #rename the columns to remove reference to scale/raw scores\n",
    "            new_column_names = [col.split('.')[0] if not 'round' in col \n",
    "                                else '{} (rounded)'.format(col.split('.')[0])\n",
    "                                for col in df_eval.columns ]\n",
    "            df_eval.columns = new_column_names\n",
    "            evals.append(df_eval)          \n",
    "    if len(evals) > 0:\n",
    "        df_evals = pd.concat(evals, sort=True)\n",
    "    else:\n",
    "        df_evals = pd.DataFrame()\n",
    "    return(df_evals)\n",
    "\n",
    "df_eval = read_evals(model_list, file_format_summarize)\n",
    "if not df_eval.empty:\n",
    "    writer = DataWriter(summary_id)\n",
    "    writer.write_experiment_output(output_dir,\n",
    "                                   {'eval_short': df_eval},\n",
    "                                   index=True,\n",
    "                                   file_format=file_format_summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive holistic score statistics\n",
    "\n",
    "The table shows distributional properties of human and system scores. SMD values lower then -0.15 or higher than 0.15 are <span class=\"highlight_color\">highlighted</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.width=10\n",
    "formatter = partial(color_highlighter, low=-0.15, high=0.15)\n",
    "if not df_eval.empty:\n",
    "     display(HTML(df_eval[['N', 'system score type', 'h_mean', 'h_sd', \n",
    "                           'sys_mean', 'sys_sd',  'SMD']].to_html(index=True,\n",
    "                                                                  classes=['sortable'],\n",
    "                                                                  escape=False,\n",
    "                                                                  formatters={'SMD': formatter},\n",
    "                                                                  float_format=int_or_float_format_func)))\n",
    "else:\n",
    "     display(Markdown(\"No information available for any of the models\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Association statistics\n",
    "\n",
    "The table shows the standard association metrics between human scores and machine scores. Note that some evaluations are based on rounded (`Trim-round`) scores computed by first truncating and then rounding the predicted score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_eval.empty:\n",
    "     display(HTML(df_eval[['N',\n",
    "                           'system score type',\n",
    "                           'corr', 'R2', 'RMSE',\n",
    "                           'wtkappa (rounded)',\n",
    "                           'kappa (rounded)',\n",
    "                           'exact_agr (rounded)',\n",
    "                           'adj_agr (rounded)']].to_html(index=True,\n",
    "                                                         classes=['sortable'],\n",
    "                                                         escape=False,\n",
    "                                                         float_format=int_or_float_format_func)))\n",
    "else:\n",
    "     display(Markdown(\"No information available for any of the models\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
