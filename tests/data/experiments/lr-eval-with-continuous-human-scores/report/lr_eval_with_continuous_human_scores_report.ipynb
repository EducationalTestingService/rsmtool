{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting options for the plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats={'retina', 'svg'}\n",
    "%config InlineBackend.rc={'savefig.dpi': 150}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import platform\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from functools import partial\n",
    "from os.path import abspath, exists, join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython import sys_info\n",
    "from IPython.display import display, HTML, Image, Javascript, Markdown, SVG\n",
    "from rsmtool.reader import DataReader\n",
    "from rsmtool.writer import DataWriter\n",
    "from rsmtool.utils import (float_format_func,\n",
    "                           int_or_float_format_func,\n",
    "                           compute_subgroup_plot_params,\n",
    "                           bold_highlighter,\n",
    "                           color_highlighter,\n",
    "                           show_thumbnail)\n",
    "\n",
    "from rsmtool.version import VERSION as rsmtool_version\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "  div.prompt.output_prompt { \n",
    "    color: white; \n",
    "  }\n",
    "  \n",
    "  span.highlight_color {\n",
    "    color: red;\n",
    "  }\n",
    "  \n",
    "  span.highlight_bold {\n",
    "    font-weight: bold;  \n",
    "  }\n",
    "    \n",
    "  @media print {\n",
    "    @page {\n",
    "      size: landscape;\n",
    "      margin: 0cm 0cm 0cm 0cm;\n",
    "    }\n",
    "\n",
    "    * {\n",
    "      margin: 0px;\n",
    "      padding: 0px;\n",
    "    }\n",
    "\n",
    "    #toc {\n",
    "      display: none;\n",
    "    }\n",
    "\n",
    "    span.highlight_color, span.highlight_bold {\n",
    "        font-weight: bolder;\n",
    "        text-decoration: underline;\n",
    "    }\n",
    "\n",
    "    div.prompt.output_prompt {\n",
    "      display: none;\n",
    "    }\n",
    "    \n",
    "    h3#Python-packages, div#packages {\n",
    "      display: none;\n",
    "  }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: you will need to set the following manually\n",
    "# if you are using this notebook interactively.\n",
    "experiment_id = os.environ.get('EXPERIMENT_ID')\n",
    "description = os.environ.get('DESCRIPTION')\n",
    "context = os.environ.get('CONTEXT')\n",
    "train_file_location = os.environ.get('TRAIN_FILE_LOCATION')\n",
    "test_file_location = os.environ.get('TEST_FILE_LOCATION')\n",
    "output_dir = os.environ.get('OUTPUT_DIR')\n",
    "figure_dir = os.environ.get('FIGURE_DIR')\n",
    "model_name = os.environ.get('MODEL_NAME')\n",
    "model_type = os.environ.get('MODEL_TYPE')\n",
    "skll_objective = os.environ.get('SKLL_OBJECTIVE')\n",
    "file_format = os.environ.get('FILE_FORMAT')\n",
    "length_column = os.environ.get('LENGTH_COLUMN')\n",
    "second_human_score_column = os.environ.get('H2_COLUMN')\n",
    "scaled = os.environ.get('SCALED')\n",
    "standardize_features = os.environ.get('STANDARDIZE_FEATURES') == '1'\n",
    "use_scaled_predictions = scaled == '1'\n",
    "exclude_zero_scores = os.environ.get('EXCLUDE_ZEROS') == '1'\n",
    "feature_subset_file = os.environ.get('FEATURE_SUBSET_FILE')\n",
    "min_items = int(os.environ.get('MIN_ITEMS'))\n",
    "use_thumbnails = os.environ.get('USE_THUMBNAILS') == '1'\n",
    "predict_expected_scores = os.environ.get('PREDICT_EXPECTED_SCORES') == '1'\n",
    "\n",
    "\n",
    "# groups for analysis by prompt or subgroup.\n",
    "# set to 'prompt' for the standard analysis of 'prompt%%subgroup1%%subgroup2' for subgroup analysis.\n",
    "groups_desc_string = os.environ.get('GROUPS_FOR_DESCRIPTIVES') \n",
    "groups_desc = groups_desc_string.split('%%')\n",
    "groups_eval_string = os.environ.get('GROUPS_FOR_EVALUATIONS') \n",
    "groups_eval = groups_eval_string.split('%%')\n",
    "\n",
    "# javascript path\n",
    "javascript_path = os.environ.get(\"JAVASCRIPT_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize counter for thumbnail IDs\n",
    "id_generator = itertools.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(join(javascript_path, \"sort.js\"), \"r\", encoding=\"utf-8\") as sortf:\n",
    "    display(Javascript(data=sortf.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Markdown('''This report presents the analysis for **{}**: {}'''.format(experiment_id, description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = ''\n",
    "if use_thumbnails:\n",
    "    markdown_str += (\"\"\"\\n  - Images in this report have been converted to \"\"\"\n",
    "                     \"\"\"clickable thumbnails.\"\"\")\n",
    "if predict_expected_scores:\n",
    "    markdown_str += (\"\"\"\\n  - Predictions analyzed in this report are *expected scores*, \"\"\"\n",
    "                     \"\"\"i.e., probability-weighted averages over all score points.\"\"\")\n",
    "\n",
    "if markdown_str:\n",
    "    markdown_str = '**Notes**:' + markdown_str\n",
    "    display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML(time.strftime('%c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the training and testing features, both raw and pre-processed\n",
    "# Make sure that the `spkitemid` and `candidate` columns are read as strings \n",
    "# to preserve any leading zeros\n",
    "# We filter DtypeWarnings that pop up mostly in very large files\n",
    "\n",
    "string_columns = ['spkitemid', 'candidate']\n",
    "converter_dict = {column: str for column in string_columns}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore', category=pd.io.common.DtypeWarning)\n",
    "    if exists(train_file_location):\n",
    "        df_train_orig = DataReader.read_from_file(train_file_location)\n",
    "\n",
    "    train_file = join(output_dir, '{}_train_features.{}'.format(experiment_id,\n",
    "                                                                file_format))\n",
    "    if exists(train_file):\n",
    "        df_train = DataReader.read_from_file(train_file, converters=converter_dict)\n",
    "\n",
    "    train_metadata_file = join(output_dir, '{}_train_metadata.{}'.format(experiment_id,\n",
    "                                                                         file_format))    \n",
    "    if exists(train_metadata_file):\n",
    "        df_train_metadata = DataReader.read_from_file(train_metadata_file, converters=converter_dict)\n",
    "\n",
    "    train_other_columns_file = join(output_dir, '{}_train_other_columns.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(train_other_columns_file):\n",
    "        df_train_other_columns = DataReader.read_from_file(train_other_columns_file, converters=converter_dict)\n",
    "\n",
    "    train_length_file = join(output_dir, '{}_train_response_lengths.{}'.format(experiment_id,\n",
    "                                                                               file_format))\n",
    "    if exists(train_length_file):\n",
    "        df_train_length = DataReader.read_from_file(train_length_file, converters=converter_dict)\n",
    "\n",
    "    train_excluded_file = join(output_dir, '{}_train_excluded_responses.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(train_excluded_file):\n",
    "        df_train_excluded = DataReader.read_from_file(train_excluded_file, converters=converter_dict)\n",
    "\n",
    "    train_responses_with_excluded_flags_file = join(output_dir, '{}_train_responses_with_excluded_flags.{}'.format(experiment_id,\n",
    "                                                                                                                   file_format))\n",
    "    if exists(train_responses_with_excluded_flags_file):\n",
    "        df_train_responses_with_excluded_flags = DataReader.read_from_file(train_responses_with_excluded_flags_file,\n",
    "                                                                           converters=converter_dict)\n",
    "\n",
    "    train_preproc_file = join(output_dir, '{}_train_preprocessed_features.{}'.format(experiment_id,\n",
    "                                                                                     file_format))    \n",
    "    if exists(train_preproc_file):\n",
    "        df_train_preproc = DataReader.read_from_file(train_preproc_file, converters=converter_dict)\n",
    "\n",
    "    if exists(test_file_location):\n",
    "        df_test_orig = DataReader.read_from_file(test_file_location)\n",
    "\n",
    "    test_file = join(output_dir, '{}_test_features.{}'.format(experiment_id,\n",
    "                                                              file_format))\n",
    "    if exists(test_file):\n",
    "        df_test = DataReader.read_from_file(test_file, converters=converter_dict)\n",
    "\n",
    "    test_metadata_file = join(output_dir, '{}_test_metadata.{}'.format(experiment_id,\n",
    "                                                                       file_format))    \n",
    "    if exists(test_metadata_file):\n",
    "        df_test_metadata = DataReader.read_from_file(test_metadata_file, converters=converter_dict)\n",
    "\n",
    "    test_other_columns_file = join(output_dir, '{}_test_other_columns.{}'.format(experiment_id,\n",
    "                                                                                 file_format))\n",
    "    if exists(test_other_columns_file):\n",
    "        df_test_other_columns = DataReader.read_from_file(test_other_columns_file, converters=converter_dict)\n",
    "\n",
    "    test_human_scores_file = join(output_dir, '{}_test_human_scores.{}'.format(experiment_id,\n",
    "                                                                               file_format))\n",
    "    if exists(test_human_scores_file):\n",
    "        df_test_human_scores = DataReader.read_from_file(test_human_scores_file, converters=converter_dict)\n",
    "\n",
    "    test_excluded_file = join(output_dir, '{}_test_excluded_responses.{}'.format(experiment_id,\n",
    "                                                                                 file_format))\n",
    "    if exists(test_excluded_file):\n",
    "        df_test_excluded = DataReader.read_from_file(test_excluded_file, converters=converter_dict)\n",
    "\n",
    "    test_responses_with_excluded_flags_file = join(output_dir, '{}_test_responses_with_excluded_flags.{}'.format(experiment_id,\n",
    "                                                                                                                 file_format))\n",
    "    if exists(test_responses_with_excluded_flags_file):\n",
    "        df_test_responses_with_excluded_flags = DataReader.read_from_file(test_responses_with_excluded_flags_file,\n",
    "                                                                          converters=converter_dict)\n",
    "\n",
    "    test_preproc_file = join(output_dir, '{}_test_preprocessed_features.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(test_preproc_file):\n",
    "        df_test_preproc = DataReader.read_from_file(test_preproc_file, converters=converter_dict)\n",
    "\n",
    "    pred_preproc_file = join(output_dir, '{}_pred_processed.{}'.format(experiment_id,\n",
    "                                                                       file_format))\n",
    "    if exists(pred_preproc_file):\n",
    "        df_pred_preproc = DataReader.read_from_file(pred_preproc_file, converters=converter_dict)\n",
    "\n",
    "    feature_file = join(output_dir, '{}_feature.{}'.format(experiment_id,\n",
    "                                                           file_format))\n",
    "    if exists(feature_file):\n",
    "        df_features = DataReader.read_from_file(feature_file, converters=converter_dict)\n",
    "        features_used = [c for c in df_features.feature.values]\n",
    "\n",
    "    betas_file = join(output_dir, '{}_betas.{}'.format(experiment_id,\n",
    "                                                       file_format))\n",
    "    if exists(betas_file):\n",
    "        df_betas = DataReader.read_from_file(betas_file)\n",
    "\n",
    "    if exists(feature_subset_file):\n",
    "        df_feature_subset_specs = DataReader.read_from_file(feature_subset_file)\n",
    "    else:\n",
    "        df_feature_subset_specs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for continuous human scores in the evaluation set\n",
    "continuous_human_score = False\n",
    "\n",
    "if exists(pred_preproc_file):\n",
    "    if not df_pred_preproc['sc1'].equals(np.round(df_pred_preproc['sc1'])):\n",
    "        continuous_human_score = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    num_excluded_train = len(df_train_responses_with_excluded_flags)\n",
    "except NameError:\n",
    "    num_excluded_train = 0\n",
    "\n",
    "try:\n",
    "    num_excluded_test = len(df_test_responses_with_excluded_flags)\n",
    "except NameError:\n",
    "    num_excluded_test = 0\n",
    "\n",
    "if context == 'rsmtool':\n",
    "    pct_excluded_train = round(100*num_excluded_train/len(df_train_orig), 2)\n",
    "pct_excluded_test = round(100*num_excluded_test/len(df_test_orig), 2)\n",
    "\n",
    "if (num_excluded_train != 0 or num_excluded_test != 0):\n",
    "    display(Markdown(\"### Responses excluded due to flags\"))\n",
    "\n",
    "    display(Markdown(\"Total number of responses excluded due to flags:\"))\n",
    "    if context=='rsmtool':\n",
    "        display(Markdown(\"Training set: {} responses ({:.1f}% of the original {} responses)\".format(num_excluded_train, pct_excluded_train, len(df_train_orig))))\n",
    "    display(Markdown(\"Evaluation set: {} responses ({:.1f}% of the original {} responses)\".format(num_excluded_test, pct_excluded_test, len(df_test_orig))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses excluded due to non-numeric feature values or scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    num_missing_rows_train = len(df_train_excluded)\n",
    "except NameError:\n",
    "    num_missing_rows_train = 0\n",
    "\n",
    "if context == 'rsmtool':\n",
    "    pct_missing_rows_train = 100*num_missing_rows_train/len(df_train_orig)\n",
    "\n",
    "try:\n",
    "    num_missing_rows_test = len(df_test_excluded)\n",
    "except:\n",
    "    num_missing_rows_test = 0\n",
    "pct_missing_rows_test = 100*num_missing_rows_test/len(df_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excluded_candidates_note = Markdown(\"Note: if a candidate had less than {} responses left for analysis after applying all filters, \"\n",
    "                                    \"all responses from that \"\n",
    "                                    \"candidate were excluded from further analysis.\".format(min_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if context == 'rsmtool':\n",
    "    display(Markdown(\"**Training set**\"))\n",
    "    display(Markdown('Total number of excluded responses: {} ({:.1f}% of the original {})'.format(num_missing_rows_train, pct_missing_rows_train, len(df_train_orig))))\n",
    "    if num_missing_rows_train != 0:\n",
    "        train_excluded_analysis_file = join(output_dir, '{}_train_excluded_composition.{}'.format(experiment_id,\n",
    "                                                                                                  file_format))\n",
    "        df_train_excluded_analysis = DataReader.read_from_file(train_excluded_analysis_file)\n",
    "        display(HTML(df_train_excluded_analysis.to_html(classes=['sortable'], float_format=float_format_func, index=False))) \n",
    "        if min_items > 0:\n",
    "            display(excluded_candidates_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('**Evaluation set**'))\n",
    "display(Markdown('Total number of excluded responses: {} ({:.1f}% of the original {})'.format(num_missing_rows_test, pct_missing_rows_test, len(df_test_orig))))\n",
    "if num_missing_rows_test != 0:\n",
    "    test_excluded_analysis_file = join(output_dir, '{}_test_excluded_composition.{}'.format(experiment_id,\n",
    "                                                                                            file_format))\n",
    "    df_test_excluded_analysis = DataReader.read_from_file(test_excluded_analysis_file)\n",
    "    display(HTML(df_test_excluded_analysis.to_html(classes=['sortable'], float_format=float_format_func, index=False)))\n",
    "    if min_items > 0:\n",
    "        display(excluded_candidates_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of this report is based only on the responses that were not excluded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if context == 'rsmtool':\n",
    "    display(Markdown('### Composition of the training and evaluation sets'))\n",
    "elif context == 'rsmeval':\n",
    "    display(Markdown('### Composition of the evaluation set'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the table showing candidate (speaker), prompt \n",
    "# and responses stats for training and test\n",
    "\n",
    "# feature descriptives extra table\n",
    "data_composition_file = join(output_dir, '{}_data_composition.{}'.format(experiment_id, file_format))\n",
    "df_data_desc = DataReader.read_from_file(data_composition_file)\n",
    "display(HTML(df_data_desc.to_html(classes=['sortable'], float_format=float_format_func, index=False)))\n",
    "\n",
    "try:\n",
    "    num_double_scored_responses = len(df_test_human_scores[df_test_human_scores['sc2'].notnull()])\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    zeros_included_or_excluded = 'excluded' if exclude_zero_scores else 'included'\n",
    "    display(Markdown(\"Total number of double scored responses in the evaluation set\" \n",
    "                     \" used: {} (zeros {})\".format(num_double_scored_responses,\n",
    "                                                   zeros_included_or_excluded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consistency_file = join(output_dir, '{}_consistency.{}'.format(experiment_id, file_format))\n",
    "degradation_file = join(output_dir, '{}_degradation.{}'.format(experiment_id, file_format))\n",
    "disattenuation_file = join(output_dir, '{}_disattenuated_correlations.{}'.format(experiment_id, file_format))\n",
    "eval_file = join(output_dir, '{}_eval.{}'.format(experiment_id,\n",
    "                                                 file_format))\n",
    "\n",
    "if exists(consistency_file) and exists(degradation_file) and exists(disattenuation_file):\n",
    "    df_consistency = DataReader.read_from_file(consistency_file, index_col=0)\n",
    "    df_degradation = DataReader.read_from_file(degradation_file, index_col=0)\n",
    "    df_dis_corrs = DataReader.read_from_file(disattenuation_file, index_col=0)\n",
    "    df_eval = DataReader.read_from_file(eval_file, index_col=0)\n",
    "    markdown_strs = ['## Consistency']\n",
    "    markdown_strs.append('*Note: this section assumes that the score used for evaluating machine scores '\n",
    "                         'is the score assigned by the first rater.*')\n",
    "    markdown_strs.append('### Human-human agreement')\n",
    "    markdown_strs.append(\"This table shows the human-human agreement on the \"\n",
    "                         \"double-scored evaluation data.\")\n",
    "    if continuous_human_score:\n",
    "        markdown_strs.append('For the computation of `kappa` and `wtkappa` '\n",
    "                             'human scores have beeen rounded to the nearest integer.')\n",
    "        \n",
    "    markdown_strs.append(\"The following are <span class='highlight_color'>highlighted </span>: \")\n",
    "    markdown_strs.append(' - Exact agreement (`exact_agr`) < 50%')\n",
    "    markdown_strs.append(' - Adjacent agreement (`adj_agr`) < 95%')\n",
    "    markdown_strs.append(' - Quadratic weighted kappa (`wtkappa`) < 0.7')\n",
    "    markdown_strs.append(' - Pearson correlation (`corr`) < 0.7')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    \n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_exact_agr = partial(color_highlighter, low=50, high=100)\n",
    "    formatter_adj_agr = partial(color_highlighter, low=95, high=100)\n",
    "    formatter_wtkappa_corr = partial(color_highlighter, low=0.7)\n",
    "    formatter_dict = {'exact_agr': formatter_exact_agr, \n",
    "                      'adj_agr': formatter_adj_agr,\n",
    "                      'wtkappa': formatter_wtkappa_corr, \n",
    "                      'corr': formatter_wtkappa_corr}\n",
    "    display(HTML(df_consistency.to_html(index=False,\n",
    "                                        escape=False,\n",
    "                                        float_format=float_format_func,\n",
    "                                        formatters=formatter_dict)))\n",
    "    \n",
    "    markdown_strs = ['### Degradation']\n",
    "    markdown_strs.append('The next table shows the degradation in the evaluation metrics '\n",
    "                         '(`diff`) when comparing the machine (`H-M`) to a second human (`H-H`). '\n",
    "                         'A positive degradation value indicates better human-machine performance. '\n",
    "                         'Note that the human-machine agreement is computed on the full '\n",
    "                         'dataset (to get a reliable estimate) whereas the human-human '\n",
    "                         'agreement is computed on the subset of responses that were double-scored.')\n",
    "    markdown_strs.append(\"\\nThe following degradation values are \"\n",
    "                         \"<span class='highlight_color'>highlighted</span>\")\n",
    "    markdown_strs.append(' - `corr` < -0.1')\n",
    "    markdown_strs.append(' - `wtkappa` < -0.1')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    df_eval_for_degradation = df_eval[df_degradation.columns].copy()\n",
    "    df_consistency_for_degradation = pd.concat([df_consistency]*len(df_eval))\n",
    "    df_consistency_for_degradation = df_consistency_for_degradation[df_degradation.columns].copy()\n",
    "    df_consistency_for_degradation.index = df_eval_for_degradation.index\n",
    "\n",
    "    df_consistency_for_degradation['type'] = 'H-H'\n",
    "    df_eval_for_degradation['type'] = 'H-M'\n",
    "    df_degradation['type'] = 'diff'\n",
    "\n",
    "    df = pd.concat([df_consistency_for_degradation, df_eval_for_degradation, df_degradation])\n",
    "    df = df[['type','corr', 'kappa', 'wtkappa', 'exact_agr', 'adj_agr', 'SMD']]\n",
    "    df = df.reset_index()\n",
    "    df = df.set_index(['index', 'type']).sort_index(level='index')\n",
    "    df.index.names = [None, None]\n",
    "    \n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_corr = partial(color_highlighter, low=-0.1, high=100)\n",
    "    formatter_wtkappa = partial(color_highlighter, low=-0.1, high=100)\n",
    "    formatter_dict = {'corr': formatter_corr, 'wtkappa': formatter_wtkappa}\n",
    "    display(HTML(df.to_html(float_format=float_format_func, \n",
    "                            formatters=formatter_dict, escape=False)))\n",
    "    \n",
    "    \n",
    "    markdown_strs = ['### Disattenuated correlations']\n",
    "    markdown_strs.append('The next table shows the correlations between human and machine scores, '\n",
    "                         'the correlations between two human scores, '  \n",
    "                         'and disattenuated correlations between human and machine scores computed as '\n",
    "                         'human-machine correlations divided by the square root of human-human '\n",
    "                         'correlation. '\n",
    "                         'Note that the human-machine correlation is computed on the full '\n",
    "                         'dataset (to get a reliable estimate) whereas the human-human '\n",
    "                         'correlation is computed on the subset of responses that were double-scored.')\n",
    "    markdown_strs.append(\"\\nThe following values are \"\n",
    "                         \"<span class='highlight_color'>highlighted</span>\")\n",
    "    markdown_strs.append(' - `disattenuated_corr` < -0.9')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_dis_corr = partial(color_highlighter, low=0.9)\n",
    "    formatter_dict = {'corr_disattenuated': formatter_dis_corr}\n",
    "    display(HTML(df_dis_corrs.to_html(index=True,\n",
    "                                      escape=False,\n",
    "                                      classes=['sortable'],\n",
    "                                      float_format=float_format_func,\n",
    "                                      formatters=formatter_dict)))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall association statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables in this section show the standard association metrics between human scores and different types of machine scores. These results are computed on the evaluation set. `Trim` (`bound`) scores are truncated to [min-0.4998, max+.4998]. `Trim-round` scores are computed by first truncating and then rounding the predicted score. Scaled scores are computed by re-scaling the predicted scores using mean and standard deviation of human scores as observed on the training data and mean and standard deviation of machine scores as predicted for the training set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive holistic score statistics\n",
    "\n",
    "The table shows distributional properties of human and system scores. SMD values lower then -0.15 or higher than 0.15 are <span class=\"highlight_color\">highlighted</span>.\n",
    "\n",
    "*Please note that for raw scores, SMD values are likely to be affected by possible differences in scale.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_or_scaled = \"scaled\" if use_scaled_predictions else \"raw\"\n",
    "eval_file = join(output_dir, '{}_eval.{}'.format(experiment_id, file_format))\n",
    "df_eval = DataReader.read_from_file(eval_file, index_col=0)\n",
    "distribution_columns = ['N', 'h_mean', 'sys_mean', 'h_sd',  'sys_sd', 'h_min', 'sys_min', 'h_max', 'sys_max', 'SMD']\n",
    "association_columns = ['N'] + [column for column in df_eval.columns if not column in distribution_columns]\n",
    "df_distribution = df_eval[distribution_columns]\n",
    "df_association = df_eval[association_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.width=10\n",
    "formatter = partial(color_highlighter, low=-0.15, high=0.15)\n",
    "HTML('<span style=\"font-size:95%\">'+ df_distribution.to_html(classes=['sortable'], \n",
    "                                                             escape=False,\n",
    "                                                             formatters={'SMD': formatter},\n",
    "                                                             float_format=float_format_func) + '</span>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Association statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = ['The table shows the standard association metrics between human scores and machine scores.']\n",
    "if continuous_human_score:\n",
    "    markdown_str.append(\"Note that for computation of `kappa` and `wtkappa` both human and machine scores are rounded.\")\n",
    "else:\n",
    "    markdown_str_append(\"Note that for computation of `kappa` and `wtkappa` all machine scores are rounded.\")\n",
    "\n",
    "Markdown('\\n'.join(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.width=10\n",
    "HTML('<span style=\"font-size:95%\">'+ df_association.to_html(classes=['sortable'], \n",
    "                                                            escape=False,\n",
    "                                                            float_format=float_format_func) + '</span>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = [\"Confusion matrix using {} trimmed rounded scores and human scores (rows=system, columns=human).\".format(raw_or_scaled)]\n",
    "\n",
    "if continuous_human_score:\n",
    "    markdown_str.append(\"*Human scores have beeen rounded to the nearest integer.*\")\n",
    "            \n",
    "Markdown('\\n'.join(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confmat_file = join(output_dir, '{}_confMatrix.{}'.format(experiment_id, file_format))\n",
    "df_confmat = DataReader.read_from_file(confmat_file, index_col=0)\n",
    "df_confmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of human and machine scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_strs = [\"The histogram and the table below show the distibution of \"\n",
    "                 \"rounded human scores and {} trimmed rounded machine scores \"\n",
    "                 \"(as % of all responses).\".format(raw_or_scaled)]\n",
    "markdown_strs.append(\"Differences in the table between human and machine distributions \"\n",
    "                     \"larger than 5 percentage points are <span class='highlight_color'>highlighted</span>.\")\n",
    "if continuous_human_score:\n",
    "    markdown_str.append(\"*Human scores have beeen rounded to the nearest integer.*\")\n",
    "    \n",
    "display(Markdown('\\n'.join(markdown_strs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoredist_file = join(output_dir, '{}_score_dist.{}'.format(experiment_id, file_format))\n",
    "df_scoredist = DataReader.read_from_file(scoredist_file, index_col=0)\n",
    "df_scoredist_melted = pd.melt(df_scoredist, id_vars=['score'])\n",
    "df_scoredist_melted = df_scoredist_melted[df_scoredist_melted['variable'] != 'difference']\n",
    "\n",
    "# get the colors for the plot\n",
    "colors = sns.color_palette(\"Greys\", 2)\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "\n",
    "    # make a barplot without a legend since we will \n",
    "    # add one manually later\n",
    "    p = sns.factorplot(\"score\", \"value\", \"variable\", kind=\"bar\",\n",
    "                       palette=colors, data=df_scoredist_melted, \n",
    "                       size=3, aspect=2, legend=False)\n",
    "    p.set_axis_labels('score', '% of responses')\n",
    "    \n",
    "    # add a legend with the right colors\n",
    "    axis = p.axes[0][0]\n",
    "    legend = axis.legend(labels=('Human', 'Machine'), title='', frameon=True, fancybox=True)\n",
    "    legend.legendHandles[0].set_color(colors[0])\n",
    "    legend.legendHandles[1].set_color(colors[1])\n",
    "\n",
    "    imgfile = join(figure_dir, '{}_score_dist.svg'.format(experiment_id))\n",
    "    plt.savefig(imgfile)\n",
    "\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(imgfile, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatter = partial(color_highlighter, low=0, high=5, absolute=True)\n",
    "df_html = df_scoredist.to_html(classes=['sortable'], index=False, \n",
    "                               escape=False, formatters={'difference': formatter})\n",
    "display(HTML(df_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "system_name = platform.system()\n",
    "\n",
    "# People might not know what 'Darwin' is, so we should replace that with 'Mac OS X'\n",
    "if system_name == 'Darwin':\n",
    "    system_name = 'Mac OS X'\n",
    "    \n",
    "# get the architecture\n",
    "architecture = platform.architecture()[0]\n",
    "\n",
    "# get the rsmtool version\n",
    "rsmtool_version_str = '.'.join(map(str, rsmtool_version))\n",
    "\n",
    "display(Markdown('This report was generated using rsmtool v{} on a '\n",
    "                 '{} computer running {}.'.format(rsmtool_version_str, \n",
    "                                                  architecture, \n",
    "                                                  system_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pip\n",
    "package_names = '\\n'.join(sorted([\"%s==%s\" % (i.key, i.version) for i in pip.get_installed_distributions()]))\n",
    "display(HTML('<div id=\"packages\"><pre>{}</pre></div>'.format(package_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "// Code to dynamically generate table of contents at the top of the HTML file\n",
    "var tocEntries = ['<ul>'];\n",
    "var anchors = $('a.anchor-link');\n",
    "var headingTypes = $(anchors).parent().map(function() { return $(this).prop('tagName')});\n",
    "var headingTexts = $(anchors).parent().map(function() { return $(this).text()});\n",
    "var subList = false;\n",
    "\n",
    "$.each(anchors, function(i, anch) {\n",
    "    var hType = headingTypes[i];\n",
    "    var hText = headingTexts[i];\n",
    "    hText = hText.substr(0, hText.length - 1);\n",
    "    if (hType == 'H2') {\n",
    "        if (subList) {\n",
    "            tocEntries.push('</ul>')\n",
    "            subList = false;\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "    else if (hType == 'H3') {\n",
    "        if (!subList) {\n",
    "            subList = true;\n",
    "            tocEntries.push('<ul>')\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "});\n",
    "tocEntries.push('</ul>')\n",
    "$('#toc').html(tocEntries.join(' '))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}