{
 "nbformat": 4,
 "cells": [
  {
   "outputs": [],
   "source": [
    "# Setting options for the plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats={'retina', 'svg'}\n",
    "%config InlineBackend.rc={'savefig.dpi': 150}"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "# Experiment Report "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import platform\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from os.path import abspath, exists, join\n",
    "from string import Template\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython import sys_info\n",
    "from IPython.display import display, HTML, Image, Markdown, SVG\n",
    "\n",
    "from rsmtool.version import VERSION as rsmtool_version"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "/* sortttable v2 from http://www.kryogenix.org/code/browser/sorttable */\n",
    "function dean_addEvent(t,e,r){if(t.addEventListener)t.addEventListener(e,r,!1);else{r.$$guid||(r.$$guid=dean_addEvent.guid++),t.events||(t.events={});var o=t.events[e];o||(o=t.events[e]={},t[\"on\"+e]&&(o[0]=t[\"on\"+e])),o[r.$$guid]=r,t[\"on\"+e]=handleEvent}}function removeEvent(t,e,r){t.removeEventListener?t.removeEventListener(e,r,!1):t.events&&t.events[e]&&delete t.events[e][r.$$guid]}function handleEvent(t){var e=!0;t=t||fixEvent(((this.ownerDocument||this.document||this).parentWindow||window).event);var r=this.events[t.type];for(var o in r)this.$$handleEvent=r[o],this.$$handleEvent(t)===!1&&(e=!1);return e}function fixEvent(t){return t.preventDefault=fixEvent.preventDefault,t.stopPropagation=fixEvent.stopPropagation,t}var stIsIE=!1;if(sorttable={init:function(){arguments.callee.done||(arguments.callee.done=!0,_timer&&clearInterval(_timer),document.createElement&&document.getElementsByTagName&&(sorttable.DATE_RE=/^(\\d\\d?)[\\/\\.-](\\d\\d?)[\\/\\.-]((\\d\\d)?\\d\\d)$/,forEach(document.getElementsByTagName(\"table\"),function(t){-1!=t.className.search(/\\bsortable\\b/)&&sorttable.makeSortable(t)})))},makeSortable:function(t){if(0==t.getElementsByTagName(\"thead\").length&&(the=document.createElement(\"thead\"),the.appendChild(t.rows[0]),t.insertBefore(the,t.firstChild)),null==t.tHead&&(t.tHead=t.getElementsByTagName(\"thead\")[0]),1==t.tHead.rows.length){sortbottomrows=[];for(var e=0;e<t.rows.length;e++)-1!=t.rows[e].className.search(/\\bsortbottom\\b/)&&(sortbottomrows[sortbottomrows.length]=t.rows[e]);if(sortbottomrows){null==t.tFoot&&(tfo=document.createElement(\"tfoot\"),t.appendChild(tfo));for(var e=0;e<sortbottomrows.length;e++)tfo.appendChild(sortbottomrows[e]);delete sortbottomrows}headrow=t.tHead.rows[0].cells;for(var e=0;e<headrow.length;e++)headrow[e].className.match(/\\bsorttable_nosort\\b/)||(mtch=headrow[e].className.match(/\\bsorttable_([a-z0-9]+)\\b/),mtch&&(override=mtch[1]),headrow[e].sorttable_sortfunction=mtch&&\"function\"==typeof sorttable[\"sort_\"+override]?sorttable[\"sort_\"+override]:sorttable.guessType(t,e),headrow[e].sorttable_columnindex=e,headrow[e].sorttable_tbody=t.tBodies[0],dean_addEvent(headrow[e],\"click\",sorttable.innerSortFunction=function(){if(-1!=this.className.search(/\\bsorttable_sorted\\b/))return sorttable.reverse(this.sorttable_tbody),this.className=this.className.replace(\"sorttable_sorted\",\"sorttable_sorted_reverse\"),this.removeChild(document.getElementById(\"sorttable_sortfwdind\")),sortrevind=document.createElement(\"span\"),sortrevind.id=\"sorttable_sortrevind\",sortrevind.innerHTML=stIsIE?'&nbsp<font face=\"webdings\">5</font>':\"&nbsp;&#x25B4;\",void this.appendChild(sortrevind);if(-1!=this.className.search(/\\bsorttable_sorted_reverse\\b/))return sorttable.reverse(this.sorttable_tbody),this.className=this.className.replace(\"sorttable_sorted_reverse\",\"sorttable_sorted\"),this.removeChild(document.getElementById(\"sorttable_sortrevind\")),sortfwdind=document.createElement(\"span\"),sortfwdind.id=\"sorttable_sortfwdind\",sortfwdind.innerHTML=stIsIE?'&nbsp<font face=\"webdings\">6</font>':\"&nbsp;&#x25BE;\",void this.appendChild(sortfwdind);theadrow=this.parentNode,forEach(theadrow.childNodes,function(t){1==t.nodeType&&(t.className=t.className.replace(\"sorttable_sorted_reverse\",\"\"),t.className=t.className.replace(\"sorttable_sorted\",\"\"))}),sortfwdind=document.getElementById(\"sorttable_sortfwdind\"),sortfwdind&&sortfwdind.parentNode.removeChild(sortfwdind),sortrevind=document.getElementById(\"sorttable_sortrevind\"),sortrevind&&sortrevind.parentNode.removeChild(sortrevind),this.className+=\" sorttable_sorted\",sortfwdind=document.createElement(\"span\"),sortfwdind.id=\"sorttable_sortfwdind\",sortfwdind.innerHTML=stIsIE?'&nbsp<font face=\"webdings\">6</font>':\"&nbsp;&#x25BE;\",this.appendChild(sortfwdind),row_array=[],col=this.sorttable_columnindex,rows=this.sorttable_tbody.rows;for(var t=0;t<rows.length;t++)row_array[row_array.length]=[sorttable.getInnerText(rows[t].cells[col]),rows[t]];row_array.sort(this.sorttable_sortfunction),tb=this.sorttable_tbody;for(var t=0;t<row_array.length;t++)tb.appendChild(row_array[t][1]);delete row_array}))}},guessType:function(t,e){sortfn=sorttable.sort_alpha;for(var r=0;r<t.tBodies[0].rows.length;r++)if(text=sorttable.getInnerText(t.tBodies[0].rows[r].cells[e]),\"\"!=text){if(text.match(/^-?[\u00a3$\u00a4]?[\\d,.]+%?$/))return sorttable.sort_numeric;if(possdate=text.match(sorttable.DATE_RE)){if(first=parseInt(possdate[1]),second=parseInt(possdate[2]),first>12)return sorttable.sort_ddmm;if(second>12)return sorttable.sort_mmdd;sortfn=sorttable.sort_ddmm}}return sortfn},getInnerText:function(t){if(!t)return\"\";if(hasInputs=\"function\"==typeof t.getElementsByTagName&&t.getElementsByTagName(\"input\").length,null!=t.getAttribute(\"sorttable_customkey\"))return t.getAttribute(\"sorttable_customkey\");if(\"undefined\"!=typeof t.textContent&&!hasInputs)return t.textContent.replace(/^\\s+|\\s+$/g,\"\");if(\"undefined\"!=typeof t.innerText&&!hasInputs)return t.innerText.replace(/^\\s+|\\s+$/g,\"\");if(\"undefined\"!=typeof t.text&&!hasInputs)return t.text.replace(/^\\s+|\\s+$/g,\"\");switch(t.nodeType){case 3:if(\"input\"==t.nodeName.toLowerCase())return t.value.replace(/^\\s+|\\s+$/g,\"\");case 4:return t.nodeValue.replace(/^\\s+|\\s+$/g,\"\");case 1:case 11:for(var e=\"\",r=0;r<t.childNodes.length;r++)e+=sorttable.getInnerText(t.childNodes[r]);return e.replace(/^\\s+|\\s+$/g,\"\");default:return\"\"}},reverse:function(t){newrows=[];for(var e=0;e<t.rows.length;e++)newrows[newrows.length]=t.rows[e];for(var e=newrows.length-1;e>=0;e--)t.appendChild(newrows[e]);delete newrows},sort_numeric:function(t,e){return aa=parseFloat(t[0].replace(/[^0-9.-]/g,\"\")),isNaN(aa)&&(aa=0),bb=parseFloat(e[0].replace(/[^0-9.-]/g,\"\")),isNaN(bb)&&(bb=0),aa-bb},sort_alpha:function(t,e){return t[0]==e[0]?0:t[0]<e[0]?-1:1},sort_ddmm:function(t,e){return mtch=t[0].match(sorttable.DATE_RE),y=mtch[3],m=mtch[2],d=mtch[1],1==m.length&&(m=\"0\"+m),1==d.length&&(d=\"0\"+d),dt1=y+m+d,mtch=e[0].match(sorttable.DATE_RE),y=mtch[3],m=mtch[2],d=mtch[1],1==m.length&&(m=\"0\"+m),1==d.length&&(d=\"0\"+d),dt2=y+m+d,dt1==dt2?0:dt2>dt1?-1:1},sort_mmdd:function(t,e){return mtch=t[0].match(sorttable.DATE_RE),y=mtch[3],d=mtch[2],m=mtch[1],1==m.length&&(m=\"0\"+m),1==d.length&&(d=\"0\"+d),dt1=y+m+d,mtch=e[0].match(sorttable.DATE_RE),y=mtch[3],d=mtch[2],m=mtch[1],1==m.length&&(m=\"0\"+m),1==d.length&&(d=\"0\"+d),dt2=y+m+d,dt1==dt2?0:dt2>dt1?-1:1},shaker_sort:function(t,e){for(var r=0,o=t.length-1,n=!0;n;){n=!1;for(var s=r;o>s;++s)if(e(t[s],t[s+1])>0){var a=t[s];t[s]=t[s+1],t[s+1]=a,n=!0}if(o--,!n)break;for(var s=o;s>r;--s)if(e(t[s],t[s-1])<0){var a=t[s];t[s]=t[s-1],t[s-1]=a,n=!0}r++}}},document.addEventListener&&document.addEventListener(\"DOMContentLoaded\",sorttable.init,!1),/WebKit/i.test(navigator.userAgent))var _timer=setInterval(function(){/loaded|complete/.test(document.readyState)&&sorttable.init()},10);window.onload=sorttable.init,dean_addEvent.guid=1,fixEvent.preventDefault=function(){this.returnValue=!1},fixEvent.stopPropagation=function(){this.cancelBubble=!0},Array.forEach||(Array.forEach=function(t,e,r){for(var o=0;o<t.length;o++)e.call(r,t[o],o,t)}),Function.prototype.forEach=function(t,e,r){for(var o in t)\"undefined\"==typeof this.prototype[o]&&e.call(r,t[o],o,t)},String.forEach=function(t,e,r){Array.forEach(t.split(\"\"),function(o,n){e.call(r,o,n,t)})};var forEach=function(t,e,r){if(t){var o=Object;if(t instanceof Function)o=Function;else{if(t.forEach instanceof Function)return void t.forEach(e,r);\"string\"==typeof t?o=String:\"number\"==typeof t.length&&(o=Array)}o.forEach(t,e,r)}};"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "<style>\n",
    "    div.prompt.output_prompt { color: white; }\n",
    "</style>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "# NOTE: you will need to set the following manually\n",
    "# if you are using this notebook interactively.\n",
    "experiment_id = os.environ.get('EXPERIMENT_ID')\n",
    "description = os.environ.get('DESCRIPTION')\n",
    "context = os.environ.get('CONTEXT')\n",
    "train_file_location = os.environ.get('TRAIN_FILE_LOCATION')\n",
    "test_file_location = os.environ.get('TEST_FILE_LOCATION')\n",
    "output_dir = os.environ.get('OUTPUT_DIR')\n",
    "figure_dir = os.environ.get('FIGURE_DIR')\n",
    "model_name = os.environ.get('MODEL_NAME')\n",
    "model_type = os.environ.get('MODEL_TYPE')\n",
    "length_column = os.environ.get('LENGTH_COLUMN')\n",
    "second_human_score_column = os.environ.get('H2_COLUMN')\n",
    "scaled = os.environ.get('SCALED')\n",
    "use_scaled_predictions = scaled == '1'\n",
    "exclude_zero_scores = os.environ.get('EXCLUDE_ZEROS') == '1'\n",
    "feature_subset_file = os.environ.get('FEATURE_SUBSET_FILE')\n",
    "\n",
    "# groups for analysis by prompt or subgroup.\n",
    "# set to 'prompt' for the standard analysis of 'prompt%%subgroup1%%subgroup2' for subgroup analysis.\n",
    "groups_desc_string = os.environ.get('GROUPS_FOR_DESCRIPTIVES') \n",
    "groups_desc = groups_desc_string.split('%%')\n",
    "groups_eval_string = os.environ.get('GROUPS_FOR_EVALUATIONS') \n",
    "groups_eval = groups_eval_string.split('%%')"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "outputs": [],
   "source": [
    "Markdown('''This report presents the analysis for **{}**: {}'''.format(experiment_id, description))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "HTML(time.strftime('%c'))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"toc\"></div>"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "# Read in the training and testing features, both raw and pre-processed\n",
    "# Make sure that the `spkitemid` column is read as a string\n",
    "\n",
    "if exists(train_file_location):\n",
    "    df_train_orig = pd.read_csv(train_file_location)\n",
    "\n",
    "train_file = join(output_dir, '{}_train_features.csv'.format(experiment_id))\n",
    "if exists(train_file):\n",
    "    df_train = pd.read_csv(train_file, converters={'spkitemid': str})\n",
    "    \n",
    "train_metadata_file = join(output_dir, '{}_train_metadata.csv'.format(experiment_id))    \n",
    "if exists(train_metadata_file):\n",
    "    df_train_metadata = pd.read_csv(train_metadata_file, converters={'spkitemid': str})\n",
    "\n",
    "train_other_columns_file = join(output_dir, '{}_train_other_columns.csv'.format(experiment_id))\n",
    "if exists(train_other_columns_file):\n",
    "    df_train_other_columns = pd.read_csv(train_other_columns_file, converters={'spkitemid': str})\n",
    "\n",
    "train_length_file = join(output_dir, '{}_train_response_lengths.csv'.format(experiment_id))\n",
    "if exists(train_length_file):\n",
    "    df_train_length = pd.read_csv(train_length_file, converters={'spkitemid': str})\n",
    "    \n",
    "train_excluded_file = join(output_dir, '{}_train_excluded_responses.csv'.format(experiment_id))\n",
    "if exists(train_excluded_file):\n",
    "    df_train_excluded = pd.read_csv(train_excluded_file, converters={'spkitemid': str})\n",
    "    \n",
    "train_responses_with_excluded_flags_file = join(output_dir, '{}_train_responses_with_excluded_flags.csv'.format(experiment_id))\n",
    "if exists(train_responses_with_excluded_flags_file):\n",
    "    df_train_responses_with_excluded_flags = pd.read_csv(train_responses_with_excluded_flags_file, converters={'spkitemid': str})\n",
    "    \n",
    "train_preproc_file = join(output_dir, '{}_train_preprocessed_features.csv'.format(experiment_id))    \n",
    "if exists(train_preproc_file):\n",
    "    df_train_preproc = pd.read_csv(train_preproc_file, converters={'spkitemid': str})\n",
    "    \n",
    "if exists(test_file_location):\n",
    "    df_test_orig = pd.read_csv(test_file_location)\n",
    "\n",
    "test_file = join(output_dir, '{}_test_features.csv'.format(experiment_id))\n",
    "if exists(test_file):\n",
    "    df_test = pd.read_csv(test_file, converters={'spkitemid': str})\n",
    "\n",
    "test_metadata_file = join(output_dir, '{}_test_metadata.csv'.format(experiment_id))    \n",
    "if exists(test_metadata_file):\n",
    "    df_test_metadata = pd.read_csv(test_metadata_file, converters={'spkitemid': str})\n",
    "    \n",
    "test_other_columns_file = join(output_dir, '{}_test_other_columns.csv'.format(experiment_id))\n",
    "if exists(test_other_columns_file):\n",
    "    df_test_other_columns = pd.read_csv(test_other_columns_file, converters={'spkitemid': str})\n",
    "\n",
    "test_human_scores_file = join(output_dir, '{}_test_human_scores.csv'.format(experiment_id))\n",
    "if exists(test_human_scores_file):\n",
    "    df_test_human_scores = pd.read_csv(test_human_scores_file, converters={'spkitemid': str})\n",
    "        \n",
    "test_excluded_file = join(output_dir, '{}_test_excluded_responses.csv'.format(experiment_id))\n",
    "if exists(test_excluded_file):\n",
    "    df_test_excluded = pd.read_csv(test_excluded_file, converters={'spkitemid': str})\n",
    "    \n",
    "test_responses_with_excluded_flags_file = join(output_dir, '{}_test_responses_with_excluded_flags.csv'.format(experiment_id))\n",
    "if exists(test_responses_with_excluded_flags_file):\n",
    "    df_test_responses_with_excluded_flags = pd.read_csv(test_responses_with_excluded_flags_file, converters={'spkitemid': str})\n",
    "\n",
    "test_preproc_file = join(output_dir, '{}_test_preprocessed_features.csv'.format(experiment_id))\n",
    "if exists(test_preproc_file):\n",
    "    df_test_preproc = pd.read_csv(test_preproc_file, converters={'spkitemid': str})\n",
    "\n",
    "pred_preproc_file = join(output_dir, '{}_pred_processed.csv'.format(experiment_id))\n",
    "if exists(pred_preproc_file):\n",
    "    df_pred_preproc = pd.read_csv(pred_preproc_file, converters={'spkitemid': str})\n",
    "\n",
    "feature_file = join(output_dir, '{}_feature.csv'.format(experiment_id))\n",
    "if exists(feature_file):\n",
    "    df_features = pd.read_csv(feature_file, converters={'spkitemid': str})\n",
    "    features_used = [c for c in df_features.feature.values]\n",
    "    \n",
    "betas_file = join(output_dir, '{}_betas.csv'.format(experiment_id))\n",
    "if exists(betas_file):\n",
    "    df_betas = pd.read_csv(betas_file)\n",
    "    \n",
    "if exists(feature_subset_file):\n",
    "    df_feature_subset_specs = pd.read_csv(feature_subset_file)\n",
    "else:\n",
    "    df_feature_subset_specs = None\n",
    "\n",
    "# define float formatting functions\n",
    "def float_format_func(x, prec=3):\n",
    "    formatter_string = Template('{:.${prec}f}').substitute(prec=prec)\n",
    "    return formatter_string.format(x)\n",
    "\n",
    "def int_or_float_format_func(x, prec=3):\n",
    "    if float.is_integer(x):\n",
    "        ans = '{}'.format(int(x))\n",
    "    else:\n",
    "        ans = float_format_func(x, prec=prec)\n",
    "    return ans\n",
    "\n",
    "def bold_highlighter(x, low=0, high=1, prec=3, absolute=False):\n",
    "    abs_x = abs(x) if absolute else x\n",
    "    val = float_format_func(x, prec=prec)\n",
    "    ans = '<span style=\"font-weight: bold;\">{}</span>'.format(val) if abs_x < low or abs_x > high else val\n",
    "    return ans\n",
    "\n",
    "def color_highlighter(x, low=0, high=1, prec=3, color='red', absolute=False):\n",
    "    abs_x = abs(x) if absolute else x\n",
    "    val = float_format_func(x, prec=prec)\n",
    "    ans = '<span style=\"color: {}\">{}</span>'.format(color, val) if abs_x < low or abs_x > high else val\n",
    "    return ans"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "## Description of the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "try:\n",
    "    num_excluded_train = len(df_train_responses_with_excluded_flags)\n",
    "except NameError:\n",
    "    num_excluded_train = 0\n",
    "\n",
    "try:\n",
    "    num_excluded_test = len(df_test_responses_with_excluded_flags)\n",
    "except NameError:\n",
    "    num_excluded_test = 0\n",
    "\n",
    "pct_excluded_train = round(100*num_excluded_train/len(df_train_orig), 2)\n",
    "pct_excluded_test = round(100*num_excluded_test/len(df_test_orig), 2)\n",
    "\n",
    "if (num_excluded_train != 0 or num_excluded_test != 0):\n",
    "    display(Markdown(\"### Responses excluded due to flags\"))\n",
    "\n",
    "    display(Markdown(\"Total number of responses excluded due to flags:\"))\n",
    "    display(Markdown(\"Training set: {} responses ({:.1f}% of the original {} responses)\".format(num_excluded_train, pct_excluded_train, len(df_train_orig))))\n",
    "    display(Markdown(\"Evaluation set: {} responses ({:.1f}% of the original {} responses)\".format(num_excluded_test, pct_excluded_test, len(df_test_orig))))\n"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "source": [
    "### Responses excluded due to non-numeric feature values or scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "try:\n",
    "    num_missing_rows_train = len(df_train_excluded)\n",
    "except NameError:\n",
    "    num_missing_rows_train = 0\n",
    "pct_missing_rows_train = 100*num_missing_rows_train/len(df_train_orig)\n",
    "\n",
    "try:\n",
    "    num_missing_rows_test = len(df_test_excluded)\n",
    "except:\n",
    "    num_missing_rows_test = 0\n",
    "pct_missing_rows_test = 100*num_missing_rows_test/len(df_test_orig)"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "#### Training set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "display(Markdown('Total number of excluded responses: {} ({:.1f}% of the original {})'.format(num_missing_rows_train, pct_missing_rows_train, len(df_train_orig))))\n",
    "if num_missing_rows_train != 0:\n",
    "    df_train_excluded_analysis = pd.read_csv(join(output_dir, '{}_train_excluded_composition.csv'.format(experiment_id)))\n",
    "    display(HTML(df_train_excluded_analysis.to_html(classes=['sortable'], float_format=float_format_func, index=False)))       "
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "#### Evaluation set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "display(Markdown('Total number of excluded responses: {} ({:.1f}% of the original {})'.format(num_missing_rows_test, pct_missing_rows_test, len(df_test_orig))))\n",
    "if num_missing_rows_test != 0:\n",
    "    df_test_excluded_analysis = pd.read_csv(join(output_dir, '{}_test_excluded_composition.csv'.format(experiment_id)))\n",
    "    display(HTML(df_test_excluded_analysis.to_html(classes=['sortable'], float_format=float_format_func, index=False)))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "The rest of this report is based only on the responses used to build and train the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Composition of the training and evaluation sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "# show the table showing candidate (speaker), prompt \n",
    "# and responses stats for training and test\n",
    "\n",
    "# feature descriptives extra table\n",
    "df_data_desc = pd.read_csv(join(output_dir, '{}_data_composition.csv'.format(experiment_id)))\n",
    "display(HTML(df_data_desc.to_html(classes=['sortable'], float_format=float_format_func, index=False)))\n",
    "\n",
    "try:\n",
    "    num_double_scored_responses = len(df_test_human_scores[df_test_human_scores['sc2'].notnull()])\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    zeros_included_or_excluded = 'excluded' if exclude_zero_scores else 'included'\n",
    "    display(Markdown(\"Total number of double scored responses\" \n",
    "                     \" used: {} (zeros {})\".format(num_double_scored_responses,\n",
    "                                                   zeros_included_or_excluded)))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "## Overall descriptive feature statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "These values are reported before transformations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "# feature descriptives table\n",
    "df_desc = pd.read_csv(join(output_dir, '{}_feature_descriptives.csv'.format(experiment_id)), index_col=0)\n",
    "HTML(df_desc.to_html(classes=['sortable'], float_format=float_format_func))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "The following table shows additional statistics for the data. Quantiles are computed using type=3 method used in SAS. The mild outliers are defined as data points between [1.5, 3) \\* IQR away from the nearest quartile. Extreme outliers are the data points >= 3 * IQR away from the nearest quartile."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Prevalence of recoded cases\n",
    "\n",
    "This sections shows the number and percentage of cases truncated to mean +/- 4 SD for each feature."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "df_outliers = pd.read_csv(join(output_dir, '{}_feature_outliers.csv'.format(experiment_id)), index_col=0)\n",
    "df_outliers.index.name = 'feature'\n",
    "df_outliers = df_outliers.reset_index()\n",
    "df_outliers = pd.melt(df_outliers, id_vars=['feature'])\n",
    "df_outliers = df_outliers[df_outliers.variable.str.contains(r'[ulb].*?perc')]\n",
    "\n",
    "# we need a higher aspect if we have more than 40 features\n",
    "aspect = 3 if len(features_used) > 40 else 2\n",
    "\n",
    "# colors for the plot\n",
    "colors = sns.color_palette(\"Greys\", 3)\n",
    "\n",
    "# what's the largest value in the data frame\n",
    "maxperc = df_outliers['value'].max()\n",
    "\n",
    "# compute the limits for the graph\n",
    "limits = (0, max(2.5, maxperc))\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "    # create a barplot without a legend since we will manually\n",
    "    # add one later\n",
    "    p = sns.factorplot(\"feature\", \"value\", \"variable\", kind=\"bar\", \n",
    "                       palette=colors, data=df_outliers, size=3, \n",
    "                       aspect=aspect, legend=False)\n",
    "    p.set_axis_labels('', '% cases truncated to mean +/- 4*sd')\n",
    "    p.set_xticklabels(rotation=90)\n",
    "    p.set(ylim=limits)\n",
    "    \n",
    "    # add a line at 2%\n",
    "    axis = p.axes[0][0]\n",
    "    axis.axhline(y=2.0, linestyle='--', linewidth=1.5, color='black')\n",
    "    \n",
    "    # add a legend with the right colors\n",
    "    legend=axis.legend(('both', 'lower', 'upper'), title='', frameon=True, fancybox=True, ncol=3)\n",
    "    legend.legendHandles[0].set_color(colors[0])\n",
    "    legend.legendHandles[1].set_color(colors[1])\n",
    "    plt.savefig(join(figure_dir, '{}_outliers.svg'.format(experiment_id)))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "### Feature value distribution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "# feature descriptives extra table\n",
    "df_desce = pd.read_csv(join(output_dir, '{}_feature_descriptivesExtra.csv'.format(experiment_id)), index_col=0)\n",
    "HTML(df_desce.to_html(classes=['sortable'], float_format=float_format_func))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "##  Feature Distributions and Inter-feature Correlations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Training set distributions\n",
    "\n",
    "The following plot shows the distributions of the feature values in \n",
    "the training set, after transformation (if applicable), truncation \n",
    "and standardization. The line shows the kernel density estimate. The \n",
    "human score (`sc1`) is also included. \n",
    "\n",
    "Response length (`length`) is included if you specified `length_column` in the config file, unless\n",
    "the column had missing values or a standard deviation <= 0."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "selected_columns = features_used + ['sc1', 'spkitemid']\n",
    "df_train_preproc_selected_features = df_train_preproc[selected_columns]\n",
    "try:\n",
    "    df_train_preproc_selected_features = df_train_preproc_selected_features.merge(df_train_length, on='spkitemid')\n",
    "except NameError:\n",
    "    column_order = sorted(features_used) + ['sc1']\n",
    "else:\n",
    "    column_order = sorted(features_used) + ['sc1', 'length']\n",
    "\n",
    "df_train_preproc_melted = pd.melt(df_train_preproc_selected_features, id_vars=['spkitemid'])\n",
    "df_train_preproc_melted = df_train_preproc_melted[['variable', 'value']]\n",
    "with sns.axes_style('white'):\n",
    "    g = sns.FacetGrid(col='variable', data=df_train_preproc_melted, col_wrap=3, \n",
    "                      col_order=column_order, sharex=False, sharey=False, size=2, \n",
    "                      aspect=1)\n",
    "    g.map(sns.distplot, \"value\", color=\"grey\")\n",
    "    for ax, cname in zip(g.axes, g.col_names):\n",
    "        labels = ax.get_xticks()\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_xticklabels(labels,rotation=90)\n",
    "        ax.set_title(cname)\n",
    "    plt.tight_layout(h_pad=1.0)\n",
    "    plt.savefig(join(figure_dir, '{}_distrib.svg'.format(experiment_id)))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "source": [
    "### Inter-feature correlations\n",
    "\n",
    "The following table shows the Pearson correlations between all the training features\n",
    "after transformation (if applicable), truncation and standardization. The human score \n",
    "(`sc1`) is also included. \n",
    "\n",
    "Response length (`length`) is included if \n",
    "you specified `length_column` in the config file, unless the column had missing \n",
    "values or a standard deviation <= 0. \n",
    "\n",
    "The following values are highlighted in <span style=\"color: red\">red</span>:\n",
    "- inter-feature correlations above 0.7, and\n",
    "- `sc1`-feature correlations lower than 0.1 or higher than 0.7"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "df_cors = pd.read_csv(join(output_dir, '{}_cors_processed.csv'.format(experiment_id)), index_col=0)\n",
    "if 'length' in df_cors.columns:\n",
    "    feature_columns = sorted([c for c in df_cors.columns if c not in ['sc1', 'length']])\n",
    "    order = ['sc1', 'length'] + feature_columns\n",
    "else:\n",
    "    feature_columns = sorted([c for c in df_cors.columns if c != 'sc1'])\n",
    "    order = ['sc1'] + feature_columns\n",
    "df_cors = df_cors.reindex(index=order, columns=order)\n",
    "\n",
    "# apply two different formatting to the columns according\n",
    "# to two different thresholds. The first one highlights all\n",
    "# inter-feature correlations > 0.7 (so, not including sc1)\n",
    "# and the second highlights all sc1-X correlations lower\n",
    "# than 0.1 and higher than 0.7. We will use red for the\n",
    "# first formatting and blue for the second one. \n",
    "formatter1 = partial(color_highlighter, low=-1, high=0.7)\n",
    "formatter2 = partial(color_highlighter, low=0.1, high=0.7)\n",
    "\n",
    "formatter_dict = {c: formatter1 for c in feature_columns+['length']}\n",
    "formatter_dict.update({'sc1': formatter2})\n",
    "\n",
    "HTML(df_cors.to_html(classes=['sortable'], formatters=formatter_dict, escape=False))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "### Marginal and partial correlations\n",
    "\n",
    "The plot below shows correlations between truncated and standardized values of each feature against human score. The first bar (`Marginal`) in each case shows Pearson's correlation. The second bar (`Partial - all`) shows partial correlations after controlling for all other variables. If you specified `length_column` in the config file, a third bar (`Partial - length`) will show partial correlations of each feature against the human score after controlling for length."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "# read in and merge the score correlations \n",
    "df_margcor = pd.read_csv(join(output_dir, '{}_margcor_score_all_data.csv'.format(experiment_id)), index_col=0)\n",
    "df_pcor = pd.read_csv(join(output_dir, '{}_pcor_score_all_data.csv'.format(experiment_id)), index_col=0)\n",
    "\n",
    "# check if we have length partial correlations\n",
    "pcor_no_length_file = join(output_dir, '{}_pcor_score_no_length_all_data.csv'.format(experiment_id))\n",
    "with_length = exists(pcor_no_length_file)\n",
    "if with_length:\n",
    "    df_pcor_no_length = pd.read_csv(pcor_no_length_file, index_col=0)\n",
    "    df_mpcor = pd.DataFrame([df_margcor.loc['All data'], \n",
    "                             df_pcor.loc['All data'], \n",
    "                             df_pcor_no_length.loc['All data']]).transpose()\n",
    "    df_mpcor.columns = ['marginal', 'partial_all', 'partial_length']\n",
    "    num_entries = 3\n",
    "    labels = ('Marginal', 'Partial - all', 'Partial - length')\n",
    "\n",
    "else:\n",
    "    df_mpcor = pd.DataFrame([df_margcor.loc['All data'], \n",
    "                             df_pcor.loc['All data']]).transpose()\n",
    "    df_mpcor.columns = ['marginal', 'partial_all']\n",
    "    num_entries = 2\n",
    "    labels = ('Marginal', 'Partial (all)')\n",
    "\n",
    "df_mpcor.index.name = 'feature'\n",
    "df_mpcor = df_mpcor.reset_index()\n",
    "df_mpcor = pd.melt(df_mpcor, id_vars=['feature'])\n",
    "\n",
    "# we need a higher aspect if we have more than 40 features\n",
    "aspect = 3 if len(features_used) > 40 else 2\n",
    "\n",
    "# get the colors for the plot\n",
    "colors = sns.color_palette(\"Greys\", num_entries)\n",
    "\n",
    "# check for any negative correlations\n",
    "limits = (0, 1)\n",
    "if len(df_mpcor[df_mpcor.value < 0]):\n",
    "    limits = (-1, 1)\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "\n",
    "    # generate a bar plot but without the legend since we will\n",
    "    # manually add one later\n",
    "    p = sns.factorplot(\"feature\", \"value\", \"variable\", kind=\"bar\",\n",
    "                       palette=colors, data=df_mpcor, size=3, \n",
    "                       aspect=aspect, legend=False)\n",
    "    p.set_axis_labels('', 'Correlation with score')\n",
    "    p.set_xticklabels(rotation=90)\n",
    "    p.set(ylim=limits)\n",
    "    \n",
    "    # add a line at 0.1 and 0.7\n",
    "    axis = p.axes[0][0]\n",
    "    axis.axhline(y=0.1, linestyle='--', linewidth=0.5, color='black')\n",
    "    axis.axhline(y=0.7, linestyle='--', linewidth=0.5, color='black')\n",
    "\n",
    "    # create the legend manually with the right colors\n",
    "    legend = axis.legend(labels=labels, title='', frameon=True, \n",
    "                         fancybox=True, ncol=num_entries)\n",
    "    for i in range(num_entries):\n",
    "        legend.legendHandles[i].set_color(colors[i])\n",
    "    plt.savefig(join(figure_dir, '{}_cors_score.svg'.format(experiment_id)))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "len_margcor_file = join(output_dir, '{}_margcor_length_all_data.csv'.format(experiment_id))\n",
    "len_pcor_file = join(output_dir, '{}_pcor_length_all_data.csv'.format(experiment_id))\n",
    "if exists(len_margcor_file) and exists(len_pcor_file):\n",
    "    display(Markdown(\"The plot below shows the same correlations between truncated and standardized values of each feature against length.\")) \n",
    "\n",
    "    df_margcor = pd.read_csv(len_margcor_file, index_col=0)\n",
    "    df_pcor = pd.read_csv(len_pcor_file, index_col=0)\n",
    "    df_mpcor = pd.DataFrame([df_margcor.loc['All data'], df_pcor.loc['All data']]).transpose()\n",
    "    df_mpcor.index.name = 'feature'\n",
    "    df_mpcor.columns = ['marginal', 'partial']\n",
    "    df_mpcor = df_mpcor.reset_index()\n",
    "    df_mpcor = pd.melt(df_mpcor, id_vars=['feature'])\n",
    "\n",
    "    # we need a higher aspect if we have more than 40 features\n",
    "    aspect = 3 if len(features_used) > 40 else 2\n",
    "\n",
    "    # check for any negative correlations\n",
    "    limits = (0, 1)\n",
    "    if len(df_mpcor[df_mpcor.value < 0]):\n",
    "        limits = (-1, 1)\n",
    "\n",
    "    # get the colors for the plot\n",
    "    colors = sns.color_palette(\"Greys\", 2)\n",
    "        \n",
    "    with sns.axes_style('whitegrid'):\n",
    "        \n",
    "        # create a barplot but without the legend since\n",
    "        # we will manually add one later\n",
    "        p = sns.factorplot(\"feature\", \"value\", \"variable\", kind=\"bar\",\n",
    "                           palette=colors, data=df_mpcor, size=3, \n",
    "                           aspect=aspect, legend=False)\n",
    "        p.set_axis_labels('', 'Correlation with length')\n",
    "        p.set_xticklabels(rotation=90)\n",
    "        p.set(ylim=limits)\n",
    "\n",
    "        # create the legend manually with the right colors\n",
    "        axis = p.axes[0][0]\n",
    "        legend = axis.legend(labels=('Marginal', 'Partial  - all'), title='', \n",
    "                             frameon=True, fancybox=True, ncol=2)\n",
    "        legend.legendHandles[0].set_color(colors[0])\n",
    "        legend.legendHandles[1].set_color(colors[1])\n",
    "        plt.savefig(join(figure_dir, '{}_cors_length.svg'.format(experiment_id))) "
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "consistency_file = join(output_dir, '{}_consistency.csv'.format(experiment_id))\n",
    "degradation_file = join(output_dir, '{}_degradation.csv'.format(experiment_id))\n",
    "if exists(consistency_file) and exists(degradation_file):\n",
    "    df_consistency = pd.read_csv(consistency_file, index_col=0)\n",
    "    df_degradation = pd.read_csv(degradation_file, index_col=0)\n",
    "    df_eval = pd.read_csv(join(output_dir, '{}_eval.csv'.format(experiment_id)), index_col=0)\n",
    "    markdown_strs = ['## Consistency']\n",
    "    markdown_strs.append('### Human-human agreement')\n",
    "    markdown_strs.append(\"This table shows the human-human agreement on the \"\n",
    "                         \"double-scored evaluation data. The following are \"\n",
    "                         \"highlighted in <span style='color: red'>red</span>: \")\n",
    "    markdown_strs.append(' - Exact agreement (`exact_agr`) < 50%')\n",
    "    markdown_strs.append(' - Adjacent agreement (`adj_agr`) < 95%')\n",
    "    markdown_strs.append(' - Quadratic weighted kappa (`wtkappa`) < 0.7')\n",
    "    markdown_strs.append(' - Pearson correlation (`corr`) < 0.7')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    \n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_exact_agr = partial(color_highlighter, low=50, high=100)\n",
    "    formatter_adj_agr = partial(color_highlighter, low=95, high=100)\n",
    "    formatter_wtkappa_corr = partial(color_highlighter, low=0.7)\n",
    "    formatter_dict = {'exact_agr': formatter_exact_agr, \n",
    "                      'adj_agr': formatter_adj_agr,\n",
    "                      'wtkappa': formatter_wtkappa_corr, \n",
    "                      'corr': formatter_wtkappa_corr}\n",
    "    display(HTML(df_consistency.to_html(index=False,\n",
    "                                        escape=False,\n",
    "                                        float_format=float_format_func,\n",
    "                                        formatters=formatter_dict)))\n",
    "    \n",
    "    markdown_strs = ['### Degradation']\n",
    "    markdown_strs.append('The next table shows the degradation in the evaluation metrics '\n",
    "                         '(`diff`) when comparing the machine (`H-M`) to a second human (`H-H`). '\n",
    "                         'A positive degradation value indicates better human-machine performance. '\n",
    "                         'Note that the human-machine agreement is computed on the full '\n",
    "                         'dataset (to get a reliable estimate) whereas the human-human '\n",
    "                         'agreement is computed on the subset of responses that were double-scored.')\n",
    "    markdown_strs.append(\"\\nThe following degradation values are highlighted in \"\n",
    "                         \"<span style='color: red'>red</span>: \")\n",
    "    markdown_strs.append(' - `corr` < -0.1')\n",
    "    markdown_strs.append(' - `wtkappa` < -0.1')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    df_eval_for_degradation = df_eval[df_degradation.columns].copy()\n",
    "    df_consistency_for_degradation = pd.concat([df_consistency]*len(df_eval))\n",
    "    df_consistency_for_degradation = df_consistency_for_degradation[df_degradation.columns].copy()\n",
    "    df_consistency_for_degradation.index = df_eval_for_degradation.index\n",
    "\n",
    "    df_consistency_for_degradation['type'] = 'H-H'\n",
    "    df_eval_for_degradation['type'] = 'H-M'\n",
    "    df_degradation['type'] = 'diff'\n",
    "\n",
    "    df = pd.concat([df_consistency_for_degradation, df_eval_for_degradation, df_degradation])\n",
    "    df = df[['type','corr', 'kappa', 'wtkappa', 'exact_agr', 'adj_agr', 'SMD']]\n",
    "    df = df.reset_index()\n",
    "    df = df.set_index(['index', 'type']).sortlevel('index')\n",
    "    df.index.names = [None, None]\n",
    "    \n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_corr = partial(color_highlighter, low=-0.1, high=100)\n",
    "    formatter_wtkappa = partial(color_highlighter, low=-0.1, high=100)\n",
    "    formatter_dict = {'corr': formatter_corr, 'wtkappa': formatter_wtkappa}\n",
    "    display(HTML(df.to_html(float_format=float_format_func, \n",
    "                            formatters=formatter_dict, escape=False)))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "Markdown('Model used: **{}**'.format(model_name))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "Markdown('Number of features in model: **{}**'.format(len(features_used)))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "builtin_ols_models = ['LinearRegression',\n",
    "                      'EqualWeightsLR',\n",
    "                      'RebalancedLR',\n",
    "                      'NNLR',\n",
    "                      'LassoFixedLambdaThenNNLR',\n",
    "                      'LassoFixedLambdaThenLR',\n",
    "                      'PositiveLassoCVThenLR']\n",
    "\n",
    "builtin_lasso_models = ['LassoFixedLambda',\n",
    "                        'PositiveLassoCV']"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "# we first just show a summary of the OLS model\n",
    "if model_name in builtin_ols_models:\n",
    "    display(Markdown('### Model summary'))\n",
    "    summary_file = join(output_dir, '{}_ols_summary.txt'.format(experiment_id))\n",
    "    with open(summary_file, 'r') as summf:\n",
    "        model_summary = summf.read()\n",
    "        print(model_summary)"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "### Standardized and Relative Regression Coefficients (Betas)\n",
    "\n",
    "The relative coefficients are intended to show relative contribution of different feature and their primary purpose is to indentify whether one of the features has an unproportionate effect over the final score. They are computed as standardized/(sum of absolute values of standardized coefficients). \n",
    "\n",
    "Negative standardized coefficients are highlighted in <span style=\"color: red\">red</span>.\n",
    "\n",
    "**Note**: if the model contains negative coefficients, relative values will not sum up to one and their interpretation is generally questionable. "
   ],
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "markdown_str = \"\"\"\n",
    "**Note**: The coefficients were estimated using LASSO regression. Unlike OLS (standard) linear regression, lasso estimation is based on an optimization routine and therefore the exact estimates may differ across different systems. \"\"\"\n",
    "\n",
    "if model_name in builtin_lasso_models:\n",
    "    display(Markdown(markdown_str))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "df_betas.sort_values(by='feature', inplace=True)\n",
    "display(HTML(df_betas.to_html(classes=['sortable'], \n",
    "                              index=False, \n",
    "                              escape=False,\n",
    "                              float_format=float_format_func,\n",
    "                              formatters={'standardized': color_highlighter})))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "Here are the same values, shown graphically."
   ],
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "df_betas_sorted = df_betas.sort_values(by='standardized', ascending=False)\n",
    "df_betas_sorted.reset_index(drop=True, inplace=True)\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(8, 3)\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "grey_colors = sns.color_palette('Greys', len(features_used))[::-1]\n",
    "with sns.axes_style('whitegrid'):\n",
    "    ax1=fig.add_subplot(121)\n",
    "    sns.barplot(\"feature\",\"standardized\", data=df_betas_sorted, \n",
    "                order=df_betas_sorted['feature'].values,\n",
    "                palette=sns.color_palette(\"Greys\", 1), ax=ax1)\n",
    "    ax1.set_xticklabels(df_betas_sorted['feature'].values, rotation=90)\n",
    "    ax1.set_title('Values of standardized coefficients')\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_ylabel('')\n",
    "    # no pie chart if we have more than 15 features\n",
    "    if len(features_used) <= 15:\n",
    "        ax2=fig.add_subplot(133, aspect=True)\n",
    "        ax2.pie(abs(df_betas_sorted['relative'].values), colors=grey_colors, \n",
    "            labels=df_betas_sorted['feature'].values)\n",
    "        ax2.set_title('Proportional contribution of each feature')\n",
    "    else:\n",
    "        fig.set_size_inches(0.35*len(features_used), 3)\n",
    "plt.savefig(join(figure_dir, '{}_betas.svg'.format(experiment_id)))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "if model_name in builtin_ols_models:\n",
    "    display(Markdown('### Model diagnostics'))\n",
    "    display(Markdown(\"These are standard plots for model diagnostics for the main model. All information is computed based on the training set.\"))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "# read in the OLS model file and create the diagnostics plots\n",
    "if model_name in builtin_ols_models:\n",
    "    ols_file = join(output_dir, '{}.ols'.format(experiment_id))\n",
    "    model = pickle.load(open(ols_file, 'rb'))\n",
    "    model_predictions = model.predict()\n",
    "\n",
    "    with sns.axes_style('white'):\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        f.set_size_inches((10, 4))\n",
    "        \n",
    "        ###\n",
    "        # for now, we do not show the influence plot since it can be slow to generate\n",
    "        ###\n",
    "        # sm.graphics.influence_plot(model.sm_ols, criterion=\"cooks\", size=10, ax=ax1)\n",
    "        # ax1.set_title('Residuals vs. Leverage', fontsize=16)\n",
    "        # ax1.set_xlabel('Leverage', fontsize=16)\n",
    "        # ax1.set_ylabel('Standardized Residuals', fontsize=16)\n",
    "\n",
    "        sm.qqplot(model.resid, stats.norm, fit=True, line='q', ax=ax1)\n",
    "        ax1.set_title('Normal Q-Q Plot', fontsize=16)\n",
    "        ax1.set_xlabel('Theoretical Quantiles', fontsize=16)\n",
    "        ax1.set_ylabel('Sample Quantiles', fontsize=16)\n",
    "\n",
    "        ax2.scatter(model_predictions, model.resid)\n",
    "        ax2.set_xlabel('Fitted values', fontsize=16)\n",
    "        ax2.set_ylabel('Residuals', fontsize=16)\n",
    "        ax2.set_title('Residuals vs. Fitted', fontsize=16)\n",
    "\n",
    "        imgfile = join(figure_dir, '{}_ols_diagnostic_plots.png'.format(experiment_id))\n",
    "        plt.savefig(imgfile)\n",
    "        display(Image(imgfile))\n",
    "        plt.close();    "
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "## Evaluation results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Association statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The table shows the standard association metrics between human scores and different types of machine scores. These results are computed on the evaluation set. `Trim` (`bound`) scores are truncated to [min-0.4998, max+.4998]. `Trim-round` scores are computed by first truncating and then rounding the predicted score. Scaled scores are computed by re-scaling the predicted scores using mean and standard deviation of human scores as observed on the training data and mean and standard deviation of machine scores as predicted for the training set. Note that for the computation of kappas scores are always rounded.\n",
    "\n",
    "SMD values lower then -0.15 or higher than 0.15 are highlighted in <span style=\"color: red\">red</span>."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "raw_or_scaled = \"scaled\" if use_scaled_predictions else \"raw\"\n",
    "df_eval = pd.read_csv(join(output_dir, '{}_eval.csv'.format(experiment_id)), index_col=0)\n",
    "pd.options.display.width=10\n",
    "formatter = partial(color_highlighter, low=-0.15, high=0.15)\n",
    "HTML('<span style=\"font-size:95%\">'+ df_eval.to_html(classes=['sortable'], \n",
    "                                                     escape=False,\n",
    "                                                     formatters={'SMD': formatter},\n",
    "                                                     float_format=float_format_func) + '</span>')"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "### Confusion Matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "Markdown(\"Confusion matrix using {} trimmed rounded scores (rows=system, columns=human).\".format(raw_or_scaled))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "outputs": [],
   "source": [
    "df_confmat = pd.read_csv(join(output_dir, '{}_confMatrix.csv'.format(experiment_id)), index_col=0)\n",
    "df_confmat"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "### Distribution of human and machine scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "markdown_strs = [\"The histogram and the table below show the distibution of \"\n",
    "                 \"human scores and {} trimmed rounded machine scores \"\n",
    "                 \"(as % of all responses).\".format(raw_or_scaled)]\n",
    "markdown_strs.append(\"Differences in the table between human and machine distributions \"\n",
    "                     \"larger than 5 percentage points are highlighted in <span style='color:red'>red</span>.\")\n",
    "display(Markdown('\\n'.join(markdown_strs)))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "df_scoredist = pd.read_csv(join(output_dir, '{}_score_dist.csv'.format(experiment_id)), index_col=0)\n",
    "df_scoredist_melted = pd.melt(df_scoredist, id_vars=['score'])\n",
    "df_scoredist_melted = df_scoredist_melted[df_scoredist_melted['variable'] != 'difference']\n",
    "\n",
    "# get the colors for the plot\n",
    "colors = sns.color_palette(\"Greys\", 2)\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "\n",
    "    # make a barplot without a legend since we will \n",
    "    # add one manually later\n",
    "    p = sns.factorplot(\"score\", \"value\", \"variable\", kind=\"bar\",\n",
    "                       palette=colors, data=df_scoredist_melted, \n",
    "                       size=3, aspect=2, legend=False)\n",
    "    p.set_axis_labels('score', '% of responses')\n",
    "    \n",
    "    # add a legend with the right colors\n",
    "    axis = p.axes[0][0]\n",
    "    legend = axis.legend(labels=('Human', 'Machine'), title='', frameon=True, fancybox=True)\n",
    "    legend.legendHandles[0].set_color(colors[0])\n",
    "    legend.legendHandles[1].set_color(colors[1])\n",
    "    \n",
    "    plt.savefig(join(figure_dir, '{}_score_dist.svg'.format(experiment_id)))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "formatter = partial(color_highlighter, low=0, high=5, absolute=True)\n",
    "df_html = df_scoredist.to_html(classes=['sortable'], index=False, \n",
    "                               escape=False, formatters={'difference': formatter})\n",
    "display(HTML(df_html))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "## Principal component analysis\n",
    "\n",
    "PCA using scaled data and singular value decomposition. This is computed using processed features after the truncation of outliers and other transformations specified in feature config file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "df_pca = pd.read_csv(join(output_dir, '{}_pca.csv'.format(experiment_id)), index_col=0)\n",
    "df_pca.sort_index(inplace=True)\n",
    "HTML(df_pca.to_html(classes=['sortable'], float_format=float_format_func))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "df_pcavar = pd.read_csv(join(output_dir, '{}_pcavar.csv'.format(experiment_id)), index_col=0)\n",
    "df_pcavar.sort_index(inplace=True)\n",
    "HTML(df_pcavar.to_html(classes=['sortable'], float_format=float_format_func))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "# generate the Scree plot\n",
    "with sns.axes_style('white'):\n",
    "    num_components = len(df_pcavar.columns)\n",
    "    labels = list(df_pcavar.columns)\n",
    "    ax = df_pcavar.transpose().plot(y='Eigenvalues', kind='line', \n",
    "                                    color='black', linestyle='dashed', marker='o', legend=False,\n",
    "                                    linewidth=1, use_index=True, xticks=range(num_components),\n",
    "                                    figsize=(11, 5), title='Scree Plot: Principal Component Analysis')\n",
    "    ax.set_ylabel('Variances')\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "    plt.savefig(join(figure_dir, '{}_pca.svg'.format(experiment_id)))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "## System information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "system_name = platform.system()\n",
    "\n",
    "# People might not know what 'Darwin' is, so we should replace that with 'Mac OS X'\n",
    "if system_name == 'Darwin':\n",
    "    system_name = 'Mac OS X'\n",
    "    \n",
    "# get the architecture\n",
    "architecture = platform.architecture()[0]\n",
    "\n",
    "# get the rsmtool version\n",
    "rsmtool_version_str = '.'.join(map(str, rsmtool_version))\n",
    "\n",
    "print('This report was generated using rsmtool v{} on a {} computer running {}.'.format(rsmtool_version_str, \n",
    "                                                                                        architecture, \n",
    "                                                                                        system_name))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "### Python packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "outputs": [],
   "source": [
    "import pip\n",
    "sorted([\"%s==%s\" % (i.key, i.version) for i in pip.get_installed_distributions()])"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "// Code to dynamically generate table of contents at the top of the HTML file\n",
    "var tocEntries = ['<ul>'];\n",
    "var anchors = $('a.anchor-link');\n",
    "var headingTypes = $(anchors).parent().map(function() { return $(this).prop('tagName')});\n",
    "var headingTexts = $(anchors).parent().map(function() { return $(this).text()});\n",
    "var subList = false;\n",
    "\n",
    "$.each(anchors, function(i, anch) {\n",
    "    var hType = headingTypes[i];\n",
    "    var hText = headingTexts[i];\n",
    "    hText = hText.substr(0, hText.length - 1);\n",
    "    if (hType == 'H2') {\n",
    "        if (subList) {\n",
    "            tocEntries.push('</ul>')\n",
    "            subList = false;\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "    else if (hType == 'H3') {\n",
    "        if (!subList) {\n",
    "            subList = true;\n",
    "            tocEntries.push('<ul>')\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "});\n",
    "tocEntries.push('</ul>')\n",
    "$('#toc').html(tocEntries.join(' '))"
   ],
   "execution_count": null,
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   }
  }
 ],
 "nbformat_minor": 0,
 "metadata": {
  "language_info": {
   "version": "3.4.4",
   "pygments_lexer": "ipython3",
   "file_extension": ".py",
   "codemirror_mode": {
    "version": 3,
    "name": "ipython"
   },
   "nbconvert_exporter": "python",
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 }
}