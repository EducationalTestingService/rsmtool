{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting options for the plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats={'retina', 'svg'}\n",
    "%config InlineBackend.rc={'savefig.dpi': 150}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import platform\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from functools import partial\n",
    "from os.path import abspath, relpath, exists, join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "from textwrap import wrap\n",
    "\n",
    "# allow older versions of pandas to work\n",
    "try:\n",
    "    from pandas.io.common import DtypeWarning\n",
    "except ImportError:\n",
    "    from pandas.errors import DtypeWarning\n",
    "\n",
    "from IPython import sys_info\n",
    "from IPython.display import display, HTML, Image, Javascript, Markdown, SVG\n",
    "from rsmtool.reader import DataReader\n",
    "from rsmtool.writer import DataWriter\n",
    "from rsmtool.utils.files import parse_json_with_comments\n",
    "from rsmtool.utils.notebook import (float_format_func,\n",
    "                                    int_or_float_format_func,\n",
    "                                    compute_subgroup_plot_params,\n",
    "                                    bold_highlighter,\n",
    "                                    color_highlighter,\n",
    "                                    show_thumbnail)\n",
    "\n",
    "from rsmtool.fairness_utils import (get_fairness_analyses,\n",
    "                                    write_fairness_results)\n",
    "\n",
    "from rsmtool.version import VERSION as rsmtool_version\n",
    "\n",
    "# turn off interactive plotting\n",
    "plt.ioff()\n",
    "\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_report_dir = os.environ.get('RSM_REPORT_DIR', None)\n",
    "if rsm_report_dir is None:\n",
    "    rsm_report_dir = os.getcwd()\n",
    "\n",
    "rsm_environ_config = join(rsm_report_dir, '.environ.json')\n",
    "if not exists(rsm_environ_config):\n",
    "    raise FileNotFoundError('The file {} cannot be located. '\n",
    "                            'Please make sure that either (1) '\n",
    "                            'you have set the correct directory with the `RSM_REPORT_DIR` '\n",
    "                            'environment variable, or (2) that your `.environ.json` '\n",
    "                            'file is in the same directory as your notebook.'.format(rsm_environ_config))\n",
    "    \n",
    "environ_config = parse_json_with_comments(rsm_environ_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "  div.prompt.output_prompt { \n",
    "    color: white; \n",
    "  }\n",
    "  \n",
    "  span.highlight_color {\n",
    "    color: red;\n",
    "  }\n",
    "  \n",
    "  span.highlight_bold {\n",
    "    font-weight: bold;  \n",
    "  }\n",
    "    \n",
    "  @media print {\n",
    "    @page {\n",
    "      size: landscape;\n",
    "      margin: 0cm 0cm 0cm 0cm;\n",
    "    }\n",
    "\n",
    "    * {\n",
    "      margin: 0px;\n",
    "      padding: 0px;\n",
    "    }\n",
    "\n",
    "    #toc {\n",
    "      display: none;\n",
    "    }\n",
    "\n",
    "    span.highlight_color, span.highlight_bold {\n",
    "        font-weight: bolder;\n",
    "        text-decoration: underline;\n",
    "    }\n",
    "\n",
    "    div.prompt.output_prompt {\n",
    "      display: none;\n",
    "    }\n",
    "    \n",
    "    h3#Python-packages, div#packages {\n",
    "      display: none;\n",
    "  }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you will need to set the following manually\n",
    "# if you are using this notebook interactively.\n",
    "experiment_id = environ_config.get('EXPERIMENT_ID')\n",
    "description = environ_config.get('DESCRIPTION')\n",
    "context = environ_config.get('CONTEXT')\n",
    "train_file_location = environ_config.get('TRAIN_FILE_LOCATION')\n",
    "test_file_location = environ_config.get('TEST_FILE_LOCATION')\n",
    "output_dir = environ_config.get('OUTPUT_DIR')\n",
    "figure_dir = environ_config.get('FIGURE_DIR')\n",
    "model_name = environ_config.get('MODEL_NAME')\n",
    "model_type = environ_config.get('MODEL_TYPE')\n",
    "skll_fixed_parameters = environ_config.get('SKLL_FIXED_PARAMETERS')\n",
    "skll_objective = environ_config.get('SKLL_OBJECTIVE')\n",
    "file_format = environ_config.get('FILE_FORMAT')\n",
    "length_column = environ_config.get('LENGTH_COLUMN')\n",
    "second_human_score_column = environ_config.get('H2_COLUMN')\n",
    "use_scaled_predictions = environ_config.get('SCALED')\n",
    "min_score = environ_config.get(\"MIN_SCORE\")\n",
    "max_score = environ_config.get(\"MAX_SCORE\")\n",
    "standardize_features = environ_config.get('STANDARDIZE_FEATURES')\n",
    "exclude_zero_scores = environ_config.get('EXCLUDE_ZEROS')\n",
    "feature_subset_file = environ_config.get('FEATURE_SUBSET_FILE', ' ')\n",
    "min_items = environ_config.get('MIN_ITEMS')\n",
    "use_thumbnails = environ_config.get('USE_THUMBNAILS')\n",
    "predict_expected_scores = environ_config.get('PREDICT_EXPECTED_SCORES')\n",
    "rater_error_variance = environ_config.get(\"RATER_ERROR_VARIANCE\")\n",
    "\n",
    "# groups for analysis by prompt or subgroup.\n",
    "groups_desc = environ_config.get('GROUPS_FOR_DESCRIPTIVES') \n",
    "groups_eval = environ_config.get('GROUPS_FOR_EVALUATIONS') \n",
    "\n",
    "# min number of n for group to be displayed in the report\n",
    "min_n_per_group = environ_config.get('MIN_N_PER_GROUP')\n",
    "\n",
    "if min_n_per_group is None:\n",
    "    min_n_per_group = {}\n",
    "\n",
    "# javascript path\n",
    "javascript_path = environ_config.get(\"JAVASCRIPT_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize counter for thumbnail IDs\n",
    "id_generator = itertools.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(javascript_path, \"sort.js\"), \"r\", encoding=\"utf-8\") as sortf:\n",
    "    display(Javascript(data=sortf.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown('''This report presents the analysis for **{}**: {}'''.format(experiment_id, description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = ''\n",
    "if use_thumbnails:\n",
    "    markdown_str += (\"\"\"\\n  - Images in this report have been converted to \"\"\"\n",
    "                     \"\"\"clickable thumbnails.\"\"\")\n",
    "if predict_expected_scores:\n",
    "    markdown_str += (\"\"\"\\n  - Predictions analyzed in this report are *expected scores*, \"\"\"\n",
    "                     \"\"\"i.e., probability-weighted averages over all score points.\"\"\")\n",
    "\n",
    "if markdown_str:\n",
    "    markdown_str = '**Notes**:' + markdown_str\n",
    "    display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(time.strftime('%c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training and testing features, both raw and pre-processed\n",
    "# Make sure that the `spkitemid` and `candidate` columns are read as strings \n",
    "# to preserve any leading zeros\n",
    "# We filter DtypeWarnings that pop up mostly in very large files\n",
    "\n",
    "string_columns = ['spkitemid', 'candidate']\n",
    "converter_dict = {column: str for column in string_columns}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore', category=DtypeWarning)\n",
    "    if exists(train_file_location):\n",
    "        df_train_orig = DataReader.read_from_file(train_file_location)\n",
    "\n",
    "    train_file = join(output_dir, '{}_train_features.{}'.format(experiment_id,\n",
    "                                                                file_format))\n",
    "    if exists(train_file):\n",
    "        df_train = DataReader.read_from_file(train_file, converters=converter_dict)\n",
    "\n",
    "    train_metadata_file = join(output_dir, '{}_train_metadata.{}'.format(experiment_id,\n",
    "                                                                         file_format))    \n",
    "    if exists(train_metadata_file):\n",
    "        df_train_metadata = DataReader.read_from_file(train_metadata_file, converters=converter_dict)\n",
    "\n",
    "    train_other_columns_file = join(output_dir, '{}_train_other_columns.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(train_other_columns_file):\n",
    "        df_train_other_columns = DataReader.read_from_file(train_other_columns_file, converters=converter_dict)\n",
    "\n",
    "    train_length_file = join(output_dir, '{}_train_response_lengths.{}'.format(experiment_id,\n",
    "                                                                               file_format))\n",
    "    if exists(train_length_file):\n",
    "        df_train_length = DataReader.read_from_file(train_length_file, converters=converter_dict)\n",
    "\n",
    "    train_excluded_file = join(output_dir, '{}_train_excluded_responses.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(train_excluded_file):\n",
    "        df_train_excluded = DataReader.read_from_file(train_excluded_file, converters=converter_dict)\n",
    "\n",
    "    train_responses_with_excluded_flags_file = join(output_dir, '{}_train_responses_with_excluded_flags.{}'.format(experiment_id,\n",
    "                                                                                                                   file_format))\n",
    "    if exists(train_responses_with_excluded_flags_file):\n",
    "        df_train_responses_with_excluded_flags = DataReader.read_from_file(train_responses_with_excluded_flags_file,\n",
    "                                                                           converters=converter_dict)\n",
    "\n",
    "    train_preproc_file = join(output_dir, '{}_train_preprocessed_features.{}'.format(experiment_id,\n",
    "                                                                                     file_format))    \n",
    "    if exists(train_preproc_file):\n",
    "        df_train_preproc = DataReader.read_from_file(train_preproc_file, converters=converter_dict)\n",
    "\n",
    "    if exists(test_file_location):\n",
    "        df_test_orig = DataReader.read_from_file(test_file_location)\n",
    "\n",
    "    test_file = join(output_dir, '{}_test_features.{}'.format(experiment_id,\n",
    "                                                              file_format))\n",
    "    if exists(test_file):\n",
    "        df_test = DataReader.read_from_file(test_file, converters=converter_dict)\n",
    "\n",
    "    test_metadata_file = join(output_dir, '{}_test_metadata.{}'.format(experiment_id,\n",
    "                                                                       file_format))    \n",
    "    if exists(test_metadata_file):\n",
    "        df_test_metadata = DataReader.read_from_file(test_metadata_file, converters=converter_dict)\n",
    "\n",
    "    test_other_columns_file = join(output_dir, '{}_test_other_columns.{}'.format(experiment_id,\n",
    "                                                                                 file_format))\n",
    "    if exists(test_other_columns_file):\n",
    "        df_test_other_columns = DataReader.read_from_file(test_other_columns_file, converters=converter_dict)\n",
    "\n",
    "    test_human_scores_file = join(output_dir, '{}_test_human_scores.{}'.format(experiment_id,\n",
    "                                                                               file_format))\n",
    "    if exists(test_human_scores_file):\n",
    "        df_test_human_scores = DataReader.read_from_file(test_human_scores_file, converters=converter_dict)\n",
    "\n",
    "    test_excluded_file = join(output_dir, '{}_test_excluded_responses.{}'.format(experiment_id,\n",
    "                                                                                 file_format))\n",
    "    if exists(test_excluded_file):\n",
    "        df_test_excluded = DataReader.read_from_file(test_excluded_file, converters=converter_dict)\n",
    "\n",
    "    test_responses_with_excluded_flags_file = join(output_dir, '{}_test_responses_with_excluded_flags.{}'.format(experiment_id,\n",
    "                                                                                                                 file_format))\n",
    "    if exists(test_responses_with_excluded_flags_file):\n",
    "        df_test_responses_with_excluded_flags = DataReader.read_from_file(test_responses_with_excluded_flags_file,\n",
    "                                                                          converters=converter_dict)\n",
    "\n",
    "    test_preproc_file = join(output_dir, '{}_test_preprocessed_features.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(test_preproc_file):\n",
    "        df_test_preproc = DataReader.read_from_file(test_preproc_file, converters=converter_dict)\n",
    "\n",
    "    pred_preproc_file = join(output_dir, '{}_pred_processed.{}'.format(experiment_id,\n",
    "                                                                       file_format))\n",
    "    if exists(pred_preproc_file):\n",
    "        df_pred_preproc = DataReader.read_from_file(pred_preproc_file, converters=converter_dict)\n",
    "\n",
    "    feature_file = join(output_dir, '{}_feature.{}'.format(experiment_id,\n",
    "                                                           file_format))\n",
    "    if exists(feature_file):\n",
    "        df_features = DataReader.read_from_file(feature_file, converters=converter_dict)\n",
    "        features_used = [c for c in df_features.feature.values]\n",
    "        # compute the longest feature name: we'll need if for the plots\n",
    "        longest_feature_name = max(map(len, features_used))\n",
    "\n",
    "    betas_file = join(output_dir, '{}_betas.{}'.format(experiment_id,\n",
    "                                                       file_format))\n",
    "    if exists(betas_file):\n",
    "        df_betas = DataReader.read_from_file(betas_file)\n",
    "\n",
    "    if exists(feature_subset_file):\n",
    "        df_feature_subset_specs = DataReader.read_from_file(feature_subset_file)\n",
    "    else:\n",
    "        df_feature_subset_specs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for continuous human scores in the evaluation set\n",
    "continuous_human_score = False\n",
    "\n",
    "if exists(pred_preproc_file):\n",
    "    if not df_pred_preproc['sc1'].equals(np.round(df_pred_preproc['sc1'])):\n",
    "        continuous_human_score = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    num_excluded_train = len(df_train_responses_with_excluded_flags)\n",
    "except NameError:\n",
    "    num_excluded_train = 0\n",
    "\n",
    "try:\n",
    "    num_excluded_test = len(df_test_responses_with_excluded_flags)\n",
    "except NameError:\n",
    "    num_excluded_test = 0\n",
    "\n",
    "if context == 'rsmtool':\n",
    "    pct_excluded_train = round(100*num_excluded_train/len(df_train_orig), 2)\n",
    "pct_excluded_test = round(100*num_excluded_test/len(df_test_orig), 2)\n",
    "\n",
    "if (num_excluded_train != 0 or num_excluded_test != 0):\n",
    "    display(Markdown(\"### Responses excluded due to flags\"))\n",
    "\n",
    "    display(Markdown(\"Total number of responses excluded due to flags:\"))\n",
    "    if context=='rsmtool':\n",
    "        display(Markdown(\"Training set: {} responses ({:.1f}% of the original {} responses)\".format(num_excluded_train, pct_excluded_train, len(df_train_orig))))\n",
    "    display(Markdown(\"Evaluation set: {} responses ({:.1f}% of the original {} responses)\".format(num_excluded_test, pct_excluded_test, len(df_test_orig))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses excluded due to non-numeric feature values or scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    num_missing_rows_train = len(df_train_excluded)\n",
    "except NameError:\n",
    "    num_missing_rows_train = 0\n",
    "\n",
    "if context == 'rsmtool':\n",
    "    pct_missing_rows_train = 100*num_missing_rows_train/len(df_train_orig)\n",
    "\n",
    "try:\n",
    "    num_missing_rows_test = len(df_test_excluded)\n",
    "except:\n",
    "    num_missing_rows_test = 0\n",
    "pct_missing_rows_test = 100*num_missing_rows_test/len(df_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excluded_candidates_note = Markdown(\"Note: if a candidate had less than {} responses left for analysis after applying all filters, \"\n",
    "                                    \"all responses from that \"\n",
    "                                    \"candidate were excluded from further analysis.\".format(min_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if context == 'rsmtool':\n",
    "    display(Markdown(\"**Training set**\"))\n",
    "    display(Markdown('Total number of excluded responses: {} ({:.1f}% of the original {})'.format(num_missing_rows_train, pct_missing_rows_train, len(df_train_orig))))\n",
    "    if num_missing_rows_train != 0:\n",
    "        train_excluded_analysis_file = join(output_dir, '{}_train_excluded_composition.{}'.format(experiment_id,\n",
    "                                                                                                  file_format))\n",
    "        df_train_excluded_analysis = DataReader.read_from_file(train_excluded_analysis_file)\n",
    "        display(HTML(df_train_excluded_analysis.to_html(classes=['sortable'], float_format=float_format_func, index=False))) \n",
    "        if min_items > 0:\n",
    "            display(excluded_candidates_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('**Evaluation set**'))\n",
    "display(Markdown('Total number of excluded responses: {} ({:.1f}% of the original {})'.format(num_missing_rows_test, pct_missing_rows_test, len(df_test_orig))))\n",
    "if num_missing_rows_test != 0:\n",
    "    test_excluded_analysis_file = join(output_dir, '{}_test_excluded_composition.{}'.format(experiment_id,\n",
    "                                                                                            file_format))\n",
    "    df_test_excluded_analysis = DataReader.read_from_file(test_excluded_analysis_file)\n",
    "    display(HTML(df_test_excluded_analysis.to_html(classes=['sortable'], float_format=float_format_func, index=False)))\n",
    "    if min_items > 0:\n",
    "        display(excluded_candidates_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of this report is based only on the responses that were not excluded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if context == 'rsmtool':\n",
    "    display(Markdown('### Composition of the training and evaluation sets'))\n",
    "elif context == 'rsmeval':\n",
    "    display(Markdown('### Composition of the evaluation set'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the table showing candidate (speaker), prompt \n",
    "# and responses stats for training and test\n",
    "\n",
    "# feature descriptives extra table\n",
    "data_composition_file = join(output_dir, '{}_data_composition.{}'.format(experiment_id, file_format))\n",
    "df_data_desc = DataReader.read_from_file(data_composition_file)\n",
    "display(HTML(df_data_desc.to_html(classes=['sortable'], float_format=float_format_func, index=False)))\n",
    "\n",
    "try:\n",
    "    num_double_scored_responses = len(df_test_human_scores[df_test_human_scores['sc2'].notnull()])\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    zeros_included_or_excluded = 'excluded' if exclude_zero_scores else 'included'\n",
    "    display(Markdown(\"Total number of double scored responses in the evaluation set\" \n",
    "                     \" used: {} (zeros {})\".format(num_double_scored_responses,\n",
    "                                                   zeros_included_or_excluded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This notebook displays data composition tables for all groups specified in groups_desc variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in groups_desc:\n",
    "    display(Markdown('### Number of responses by {}'.format(group)))\n",
    "    data_composition_by_file = join(output_dir, '{}_data_composition_by_{}.{}'.format(experiment_id,\n",
    "                                                                                      group,\n",
    "                                                                                      file_format))\n",
    "    df_data_composition_by_group = DataReader.read_from_file(data_composition_by_file)\n",
    "    display(HTML(df_data_composition_by_group.to_html(classes=['sortable'], float_format=float_format_func, index=False)))\n",
    "    if group in min_n_per_group:\n",
    "        display(Markdown(\"Note: the rest of this report will only display the results for groups with \"\n",
    "                         \"at least {} responses in the partition used for each set of analyses.\".format(min_n_per_group[group])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall descriptive feature statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are reported before transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature descriptives table\n",
    "desc_file = join(output_dir, '{}_feature_descriptives.{}'.format(experiment_id, file_format))\n",
    "\n",
    "df_desc = DataReader.read_from_file(desc_file, index_col=0)\n",
    "HTML(df_desc.to_html(classes=['sortable'], float_format=float_format_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prevalence of recoded cases\n",
    "\n",
    "This sections shows the number and percentage of cases truncated to mean +/- 4 SD for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outliers_file = join(output_dir, '{}_feature_outliers.{}'.format(experiment_id, file_format))\n",
    "df_outliers = DataReader.read_from_file(outliers_file, index_col=0)\n",
    "df_outliers.index.name = 'feature'\n",
    "df_outliers = df_outliers.reset_index()\n",
    "df_outliers = pd.melt(df_outliers, id_vars=['feature'])\n",
    "df_outliers = df_outliers[df_outliers.variable.str.contains(r'[ulb].*?perc')]\n",
    "\n",
    "\n",
    "# we need to increase the plot height if feature names are long\n",
    "if longest_feature_name > 10:\n",
    "    height = 3 + math.ceil((longest_feature_name - 10)/10)\n",
    "else:\n",
    "    height = 3\n",
    "    \n",
    "# we also need a higher aspect if we have more than 40 features\n",
    "# The aspect defines the final width of the plot (width=aspect*height).\n",
    "# We keep the width constant (9 for plots with many features or 6\n",
    "# for plots with few features) by dividing the expected width\n",
    "# by the height. \n",
    "aspect = 9/height if len(features_used) > 40 else 6/height\n",
    "\n",
    "\n",
    "# colors for the plot\n",
    "colors = sns.color_palette(\"Greys\", 3)\n",
    "\n",
    "# what's the largest value in the data frame\n",
    "maxperc = df_outliers['value'].max()\n",
    "\n",
    "# compute the limits for the graph\n",
    "limits = (0, max(2.5, maxperc))\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "    # create a barplot without a legend since we will manually\n",
    "    # add one later\n",
    "    p = sns.catplot(x=\"feature\", y=\"value\", hue=\"variable\", kind=\"bar\", \n",
    "                    palette=colors, data=df_outliers, height=height, \n",
    "                    aspect=aspect, legend=False)\n",
    "    p.set_axis_labels('', '% cases truncated\\nto mean +/- 4*sd')\n",
    "    p.set_xticklabels(rotation=90)\n",
    "    p.set(ylim=limits)\n",
    "\n",
    "    # add a line at 2%\n",
    "    axis = p.axes[0][0]\n",
    "    axis.axhline(y=2.0, linestyle='--', linewidth=1.5, color='black')\n",
    "\n",
    "    # add a legend with the right colors\n",
    "    legend=axis.legend(('both', 'lower', 'upper'), title='', frameon=True, fancybox=True, ncol=3)\n",
    "    legend.legendHandles[0].set_color(colors[0])\n",
    "    legend.legendHandles[1].set_color(colors[1])\n",
    "\n",
    "    # we want to try to force `tight_layout()`, but if this \n",
    "    # raises a warning, we don't want the entire notebook to fail\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "    imgfile = join(figure_dir, '{}_outliers.svg'.format(experiment_id))\n",
    "    plt.savefig(imgfile)\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(imgfile, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature value distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows additional statistics for the data. Quantiles are computed using type=3 method used in SAS. The mild outliers are defined as data points between [1.5, 3) \\* IQR away from the nearest quartile. Extreme outliers are the data points >= 3 * IQR away from the nearest quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature descriptives extra table\n",
    "desce_file = join(output_dir, '{}_feature_descriptivesExtra.{}'.format(experiment_id,\n",
    "                                                                       file_format))\n",
    "df_desce = DataReader.read_from_file(desce_file, index_col=0)\n",
    "HTML(df_desce.to_html(classes=['sortable'], float_format=float_format_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook generates boxplot with feature values for all groups specified in groups_desc variable. \n",
    "# Note that this analysis uses the raw feature values after filtering out the missing features and scores.\n",
    "\n",
    "# merge metadata and feature values\n",
    "df_train_merged = pd.merge(df_train, df_train_metadata, on = 'spkitemid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(features_used)\n",
    "\n",
    "if (num_features > 150):\n",
    "    display(Markdown('### Feature values by subgroup(s)'))\n",
    "    display(Markdown('Since the data has {} (> 150) features, boxplots with feature values for all groups '\n",
    "                     'will be skipped.'.format(num_features)))\n",
    "elif (30 < num_features <= 150 and not use_thumbnails):\n",
    "    display(Markdown('### Feature values by subgroup(s)'))\n",
    "    display(Markdown('Since the data has {} (> 30 but <= 150) features, you need to set `\"use_thumbnails\"` to `true` in your '\n",
    "                     'configuration file to generate boxplots with feature values for all groups.'.format(num_features)))\n",
    "else:\n",
    "    for group in groups_desc:\n",
    "        display(Markdown('### Feature values by {}'.format(group)))\n",
    "        display(Markdown('In all plots in this subsection the values are reported before '\n",
    "                         'transformations/truncation. The lines indicate the threshold for '\n",
    "                         'truncation (mean +/- 4*SD).'))\n",
    "\n",
    "        df_train_feats = df_train_merged[features_used + [group]]\n",
    "        \n",
    "        # if we have threshold set for this group, filter the data now\n",
    "        if group in min_n_per_group:\n",
    "            display(Markdown(\"The report only shows the results for groups with \"\n",
    "                             \"at least {} responses in the training set.\".format(min_n_per_group[group])))\n",
    "\n",
    "            category_counts = df_train_merged[group].value_counts()\n",
    "            selected_categories = category_counts[category_counts >= min_n_per_group[group]].index\n",
    "\n",
    "            df_train_feats_all = df_train_merged[df_train_merged[group].isin(selected_categories)].copy()\n",
    "        else:\n",
    "        \n",
    "            df_train_feats_all = df_train_merged.copy()\n",
    "        \n",
    "        if len(df_train_feats_all) > 0:\n",
    "        \n",
    "            df_train_feats_all[group] = 'All data'\n",
    "\n",
    "            df_train_combined = pd.concat([df_train_feats, df_train_feats_all], sort=True)\n",
    "            df_train_combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Define the order of the boxes: put 'All data' first and 'No info' last.\n",
    "            group_levels = sorted(list(df_train_feats[group].unique()))\n",
    "            if 'No info' in group_levels:\n",
    "                box_names = ['All data'] + [level for level in group_levels if level != 'No info'] + ['No info']\n",
    "            else:\n",
    "                box_names = ['All data'] + group_levels\n",
    "\n",
    "            # create the faceted boxplots\n",
    "            fig = plt.figure()\n",
    "            (figure_width, \n",
    "             figure_height, \n",
    "             num_rows, \n",
    "             num_columns, \n",
    "             wrapped_box_names) = compute_subgroup_plot_params(box_names, num_features)\n",
    "\n",
    "            fig.set_size_inches(figure_width, figure_height)\n",
    "            with sns.axes_style('white'), sns.plotting_context('notebook', font_scale=1.2):\n",
    "                for i, varname in enumerate(sorted(features_used)):\n",
    "                    df_plot = df_train_combined[[group, varname]]\n",
    "                    min_value = df_plot[varname].mean() - 4 * df_plot[varname].std()\n",
    "                    max_value = df_plot[varname].mean() + 4 * df_plot[varname].std()\n",
    "                    ax = fig.add_subplot(num_rows, num_columns, i + 1)\n",
    "                    ax.axhline(y=float(min_value), linestyle='--', linewidth=0.5, color='r')\n",
    "                    ax.axhline(y=float(max_value), linestyle='--', linewidth=0.5, color='r')\n",
    "                    sns.boxplot(x=df_plot[group], y=df_plot[varname], color='#b3b3b3', ax=ax, order=box_names)\n",
    "                    ax.set_xticklabels(wrapped_box_names, rotation=90) \n",
    "                    ax.set_xlabel('')\n",
    "                    ax.set_ylabel('')\n",
    "                    plot_title = '{} by {}'.format('\\n'.join(wrap(varname, 30)), group)\n",
    "                    ax.set_title(plot_title)\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "\n",
    "            # save the figure as an SVG file.\n",
    "            imgfile = join(figure_dir, '{}_feature_boxplot_by_{}.svg'.format(experiment_id, group))\n",
    "            plt.savefig(imgfile)\n",
    "            if use_thumbnails:\n",
    "                show_thumbnail(imgfile, next(id_generator))\n",
    "            else:\n",
    "                # needed so that the figures are shown after the heading and not at the end of the cell\n",
    "                plt.show()\n",
    "        else:\n",
    "            display(Markdown(\"None of the groups in {} had {} or more responses.\".format(group,\n",
    "                                                                                         min_n_per_group[group])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature distributions and inter-feature correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set distributions\n",
    "\n",
    "The following plot shows the distributions of the feature values in \n",
    "the training set, after transformation (if applicable), truncation \n",
    "and standardization (if applicable). The line shows the kernel density estimate. The \n",
    "human score (`sc1`) is also included. \n",
    "\n",
    "Response length (`length`) is included if you specified `length_column` in the config file, unless\n",
    "the column had missing values or a standard deviation <= 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = features_used + ['sc1', 'spkitemid']\n",
    "df_train_preproc_selected_features = df_train_preproc[selected_columns]\n",
    "try:\n",
    "    df_train_preproc_selected_features = df_train_preproc_selected_features.merge(df_train_length, on='spkitemid')\n",
    "except NameError:\n",
    "    column_order = sorted(features_used) + ['sc1']\n",
    "else:\n",
    "    column_order = sorted(features_used) + ['sc1', 'length']\n",
    "\n",
    "df_train_preproc_melted = pd.melt(df_train_preproc_selected_features, id_vars=['spkitemid'])\n",
    "df_train_preproc_melted = df_train_preproc_melted[['variable', 'value']]\n",
    "\n",
    "# we need to reduce col_wrap and increase width if the feature names are too long\n",
    "if longest_feature_name > 10:\n",
    "    col_wrap = 2\n",
    "    # adjust height to allow for wrapping really long names. We allow 0.25 in per line\n",
    "    height = 2+(math.ceil(longest_feature_name/30)*0.25)\n",
    "    aspect = 4/height\n",
    "else:\n",
    "    col_wrap = 3\n",
    "    aspect = 1\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    with sns.axes_style('white'):\n",
    "        g = sns.FacetGrid(col='variable', data=df_train_preproc_melted, col_wrap=col_wrap, \n",
    "                          col_order=column_order, sharex=False, sharey=False, height=height, \n",
    "                          aspect=aspect)\n",
    "        g.map(sns.distplot, \"value\", color=\"grey\", kde=False)\n",
    "        for ax, cname in zip(g.axes, g.col_names):\n",
    "            labels = ax.get_xticks()\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_xticklabels(labels, rotation=90)\n",
    "            plot_title = '\\n'.join(wrap(str(cname), 30))\n",
    "            ax.set_title(plot_title)\n",
    "\n",
    "        plt.tight_layout(h_pad=1.0)\n",
    "        imgfile = join(figure_dir, '{}_distrib.svg'.format(experiment_id))\n",
    "        plt.savefig(imgfile)\n",
    "        if use_thumbnails:\n",
    "            show_thumbnail(imgfile, next(id_generator))\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-feature correlations\n",
    "\n",
    "The following table shows the Pearson correlations between all the training features\n",
    "after transformation (if applicable), truncation and standardization (if applicable). The human score \n",
    "(`sc1`) is also included. \n",
    "\n",
    "Response length (`length`) is included if \n",
    "you specified `length_column` in the config file, unless the column had missing \n",
    "values or a standard deviation <= 0. \n",
    "\n",
    "The following values are <span class=\"highlight_color\">highlighted</span>:\n",
    "- inter-feature correlations above 0.7, and\n",
    "- `sc1`-feature correlations lower than 0.1 or higher than 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cors_file = join(output_dir, '{}_cors_processed.{}'.format(experiment_id,\n",
    "                                                           file_format))\n",
    "df_cors = DataReader.read_from_file(cors_file, index_col=0)\n",
    "if 'length' in df_cors.columns:\n",
    "    feature_columns = sorted([c for c in df_cors.columns if c not in ['sc1', 'length']])\n",
    "    order = ['sc1', 'length'] + feature_columns\n",
    "else:\n",
    "    feature_columns = sorted([c for c in df_cors.columns if c != 'sc1'])\n",
    "    order = ['sc1'] + feature_columns\n",
    "    \n",
    "df_cors = df_cors.reindex(index=order, columns=order)\n",
    "\n",
    "# wrap the column names if the feature names are very long\n",
    "if longest_feature_name > 10:\n",
    "    column_names = ['\\n'.join(wrap(c, 10)) for c in order]\n",
    "else:\n",
    "    column_names = order\n",
    "    \n",
    "df_cors.columns = column_names\n",
    "\n",
    "# apply two different formatting to the columns according\n",
    "# to two different thresholds. The first one highlights all\n",
    "# inter-feature correlations > 0.7 (so, not including sc1)\n",
    "# and the second highlights all sc1-X correlations lower\n",
    "# than 0.1 and higher than 0.7. We will use red for the\n",
    "# first formatting and blue for the second one. \n",
    "formatter1 = partial(color_highlighter, low=-1, high=0.7)\n",
    "formatter2 = partial(color_highlighter, low=0.1, high=0.7)\n",
    "\n",
    "formatter_dict = {c: formatter1 for c in column_names if not c == 'sc1'}\n",
    "formatter_dict.update({'sc1': formatter2})\n",
    "\n",
    "HTML(df_cors.to_html(classes=['sortable'], formatters=formatter_dict, escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal and partial correlations\n",
    "\n",
    "The plot below shows correlations between truncated and standardized (if applicable) values of each feature against human score. The first bar (`Marginal`) in each case shows Pearson's correlation. The second bar (`Partial - all`) shows partial correlations after controlling for all other variables. If you specified `length_column` in the config file, a third bar (`Partial - length`) will show partial correlations of each feature against the human score after controlling for length. The dotted lines correspond to *r* = 0.1 and *r* = 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in and merge the score correlations \n",
    "margcor_file = join(output_dir, '{}_margcor_score_all_data.{}'.format(experiment_id, file_format))\n",
    "pcor_file = join(output_dir, '{}_pcor_score_all_data.{}'.format(experiment_id, file_format))\n",
    "\n",
    "df_margcor = DataReader.read_from_file(margcor_file, index_col=0)\n",
    "df_pcor = DataReader.read_from_file(pcor_file, index_col=0)\n",
    "\n",
    "# check if we have length partial correlations\n",
    "pcor_no_length_file = join(output_dir, '{}_pcor_score_no_length_all_data.{}'.format(experiment_id,\n",
    "                                                                                    file_format))\n",
    "with_length = exists(pcor_no_length_file)\n",
    "if with_length:\n",
    "    df_pcor_no_length = DataReader.read_from_file(pcor_no_length_file, index_col=0)\n",
    "    df_mpcor = pd.DataFrame([df_margcor.loc['All data'], \n",
    "                             df_pcor.loc['All data'], \n",
    "                             df_pcor_no_length.loc['All data']]).transpose()\n",
    "    df_mpcor.columns = ['marginal', 'partial_all', 'partial_length']\n",
    "    num_entries = 3\n",
    "    labels = ('Marginal', 'Partial - all', 'Partial - length')\n",
    "\n",
    "else:\n",
    "    df_mpcor = pd.DataFrame([df_margcor.loc['All data'], \n",
    "                             df_pcor.loc['All data']]).transpose()\n",
    "    df_mpcor.columns = ['marginal', 'partial_all']\n",
    "    num_entries = 2\n",
    "    labels = ('Marginal', 'Partial (all)')\n",
    "\n",
    "df_mpcor.index.name = 'feature'\n",
    "df_mpcor = df_mpcor.reset_index()\n",
    "df_mpcor = pd.melt(df_mpcor, id_vars=['feature'])\n",
    "\n",
    "# we need to change the plot height if the feature names are long\n",
    "if longest_feature_name > 10:\n",
    "    height = 3 + math.ceil((longest_feature_name - 10)/10)\n",
    "else:\n",
    "    height = 3\n",
    "        \n",
    "# we need a higher aspect if we have more than 40 features\n",
    "aspect = 9/height if len(features_used) > 40 else 6/height\n",
    "\n",
    "\n",
    "# get the colors for the plot\n",
    "colors = sns.color_palette(\"Greys\", num_entries)\n",
    "\n",
    "# check for any negative correlations\n",
    "limits = (0, 1)\n",
    "if len(df_mpcor[df_mpcor.value < 0]):\n",
    "    limits = (-1, 1)\n",
    "\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "\n",
    "    # generate a bar plot but without the legend since we will\n",
    "    # manually add one later\n",
    "    p = sns.catplot(x=\"feature\", y=\"value\", hue=\"variable\", kind=\"bar\",\n",
    "                    palette=colors, data=df_mpcor, height=height, \n",
    "                    aspect=aspect, legend=False)\n",
    "    p.set_axis_labels('', 'Correlation with score')\n",
    "    p.set_xticklabels(rotation=90)\n",
    "    p.set(ylim=limits)\n",
    "    \n",
    "    # add a line at 0.1 and 0.7\n",
    "    axis = p.axes[0][0]\n",
    "    axis.axhline(y=0.1, linestyle='--', linewidth=0.5, color='black');\n",
    "    axis.axhline(y=0.7, linestyle='--', linewidth=0.5, color='black');\n",
    "\n",
    "    # create the legend manually with the right colors\n",
    "    legend = axis.legend(labels=labels, title='', frameon=True, \n",
    "                         fancybox=True, ncol=num_entries)\n",
    "    for i in range(num_entries):\n",
    "        legend.legendHandles[i].set_color(colors[i]);\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "    imgfile = join(figure_dir, '{}_cors_score.svg'.format(experiment_id))\n",
    "    plt.savefig(imgfile)\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(imgfile, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_margcor_file = join(output_dir, '{}_margcor_length_all_data.{}'.format(experiment_id,\n",
    "                                                                           file_format))\n",
    "len_pcor_file = join(output_dir, '{}_pcor_length_all_data.{}'.format(experiment_id,\n",
    "                                                                     file_format))\n",
    "if exists(len_margcor_file) and exists(len_pcor_file):\n",
    "\n",
    "    if standardize_features:\n",
    "        display(Markdown(\"The plot below shows the same correlations between truncated and \"\n",
    "                         \"standardized values of each feature against length.\"))\n",
    "    else:\n",
    "        display(Markdown(\"The plot below shows the same correlations between truncated and \"\n",
    "                         \"un-standardized values of each feature against length.\"))\n",
    "\n",
    "    df_margcor = DataReader.read_from_file(len_margcor_file, index_col=0)\n",
    "    df_pcor = DataReader.read_from_file(len_pcor_file, index_col=0)\n",
    "    df_mpcor = pd.DataFrame([df_margcor.loc['All data'], df_pcor.loc['All data']]).transpose()\n",
    "    df_mpcor.index.name = 'feature'\n",
    "    df_mpcor.columns = ['marginal', 'partial']\n",
    "    df_mpcor = df_mpcor.reset_index()\n",
    "    df_mpcor = pd.melt(df_mpcor, id_vars=['feature'])\n",
    "\n",
    "    # we need to change the plot height if the feature names are long\n",
    "    if longest_feature_name > 10:\n",
    "        height = 3 + math.ceil((longest_feature_name - 10)/10)\n",
    "    else:\n",
    "        height = 3\n",
    "        \n",
    "    # we need a higher aspect if we have more than 40 features\n",
    "    aspect = 9/height if len(features_used) > 40 else 6/height\n",
    "\n",
    "\n",
    "    # check for any negative correlations\n",
    "    limits = (0, 1)\n",
    "    if len(df_mpcor[df_mpcor.value < 0]):\n",
    "        limits = (-1, 1)\n",
    "\n",
    "    # get the colors for the plot\n",
    "    colors = sns.color_palette(\"Greys\", 2)\n",
    "        \n",
    "    with sns.axes_style('whitegrid'):\n",
    "        \n",
    "        # create a barplot but without the legend since\n",
    "        # we will manually add one later\n",
    "        p = sns.catplot(x=\"feature\", y=\"value\", hue=\"variable\", kind=\"bar\",\n",
    "                        palette=colors, data=df_mpcor, height=height, \n",
    "                        aspect=aspect, legend=False)\n",
    "        p.set_axis_labels('', 'Correlation with length')\n",
    "        p.set_xticklabels(rotation=90)\n",
    "        p.set(ylim=limits)\n",
    "\n",
    "        # create the legend manually with the right colors\n",
    "        axis = p.axes[0][0]\n",
    "        legend = axis.legend(labels=('Marginal', 'Partial  - all'), title='', \n",
    "                             frameon=True, fancybox=True, ncol=2)\n",
    "        legend.legendHandles[0].set_color(colors[0]);\n",
    "        legend.legendHandles[1].set_color(colors[1]);\n",
    "        imgfile = join(figure_dir, '{}_cors_length.svg'.format(experiment_id))\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "        plt.savefig(imgfile) \n",
    "        if use_thumbnails:\n",
    "            show_thumbnail(imgfile, next(id_generator))\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(groups_desc) > 0:\n",
    "    markdown_str = [\"## Differential feature functioning\"]\n",
    "    markdown_str.append(\"This section shows differential feature functioning (DFF) plots \"\n",
    "                        \"for all features and subgroups. The features are shown after applying \"\n",
    "                        \"transformations (if applicable) and truncation of outliers.\")\n",
    "    display(Markdown('\\n'.join(markdown_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check if we already created the merged file in another notebook\n",
    "\n",
    "try:\n",
    "    df_train_preproc_merged\n",
    "except NameError:\n",
    "    df_train_preproc_merged = pd.merge(df_train_preproc, df_train_metadata, on = 'spkitemid')\n",
    "\n",
    "for group in groups_desc:\n",
    "    display(Markdown(\"### DFF by {}\".format(group)))\n",
    "    \n",
    "    if group in min_n_per_group:\n",
    "        display(Markdown(\"The report only shows the results for groups with \"\n",
    "                         \"at least {} responses in the training set.\".format(min_n_per_group[group])))\n",
    "        \n",
    "        category_counts = df_train_preproc_merged[group].value_counts()\n",
    "        selected_categories = category_counts[category_counts >= min_n_per_group[group]].index\n",
    "        \n",
    "        df_train_preproc_selected = df_train_preproc_merged[df_train_preproc_merged[group].isin(selected_categories)].copy()\n",
    "    else:\n",
    "        df_train_preproc_selected = df_train_preproc_merged.copy()\n",
    "    \n",
    "    \n",
    "    if len(df_train_preproc_selected) > 0:\n",
    "        \n",
    "        # we need to reduce col_wrap and increase width if the feature names are too long\n",
    "        if longest_feature_name > 10:\n",
    "            col_wrap = 2\n",
    "            # adjust height to allow for wrapping really long names. We allow 0.25 in per line\n",
    "            height = 2+(math.ceil(longest_feature_name/30)*0.25)\n",
    "            aspect = 5/height\n",
    "            # show legend near the second plot in the grid\n",
    "            plot_with_legend = 1\n",
    "        else:\n",
    "            height=3\n",
    "            col_wrap = 3\n",
    "            aspect = 1\n",
    "            # show the legend near the third plot in the grid\n",
    "            plot_with_legend = 2\n",
    "        \n",
    "        selected_columns = ['spkitemid', 'sc1'] + features_used + [group]\n",
    "        df_melted = pd.melt(df_train_preproc_selected[selected_columns], id_vars=['spkitemid', 'sc1', group], var_name='feature')\n",
    "        group_values = sorted(df_melted[group].unique())\n",
    "        colors = sns.color_palette(\"Greys\", len(group_values))\n",
    "        with sns.axes_style('whitegrid'), sns.plotting_context('notebook', font_scale=1.2):\n",
    "            p = sns.catplot(x='sc1', y='value', hue=group, hue_order = group_values,\n",
    "                            col='feature', col_wrap=col_wrap, height=height, aspect=aspect,\n",
    "                            scale=0.6,\n",
    "                            palette=colors,\n",
    "                            sharey=False, sharex=False, legend=False, kind=\"point\",\n",
    "                            data=df_melted)\n",
    "\n",
    "            for i, axis in enumerate(p.axes):\n",
    "                axis.set_xlabel('score')\n",
    "                if i == plot_with_legend:\n",
    "                    legend = axis.legend(group_values, title=group, \n",
    "                                         frameon=True, fancybox=True, \n",
    "                                         ncol=1, fontsize=10,\n",
    "                                         loc='upper right', bbox_to_anchor=(1.75, 1))\n",
    "                    for j in range(len(group_values)):\n",
    "                        legend.legendHandles[j].set_color(colors[j])\n",
    "                    plt.setp(legend.get_title(), fontsize='x-small')\n",
    "            \n",
    "            for ax, cname in zip(p.axes, p.col_names):\n",
    "                ax.set_title('\\n'.join(wrap(str(cname), 30)))\n",
    "\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "            imgfile = join(figure_dir, '{}_dff_{}.svg'.format(experiment_id, group))\n",
    "            plt.savefig(imgfile)\n",
    "            if use_thumbnails:\n",
    "                show_thumbnail(imgfile, next(id_generator))\n",
    "            else:\n",
    "                plt.show()\n",
    "    else:\n",
    "        display(Markdown(\"None of the groups in {} had {} or more responses.\".format(group,\n",
    "                                                                                    min_n_per_group[group])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_strs = ['## Consistency']\n",
    "\n",
    "consistency_file = join(output_dir, '{}_consistency.{}'.format(experiment_id, file_format))\n",
    "degradation_file = join(output_dir, '{}_degradation.{}'.format(experiment_id, file_format))\n",
    "disattenuation_file = join(output_dir, '{}_disattenuated_correlations.{}'.format(experiment_id, file_format))\n",
    "eval_file = join(output_dir, '{}_eval.{}'.format(experiment_id,\n",
    "                                                 file_format))\n",
    "\n",
    "if exists(consistency_file) and exists(degradation_file) and exists(disattenuation_file):\n",
    "    df_consistency = DataReader.read_from_file(consistency_file, index_col=0)\n",
    "    df_degradation = DataReader.read_from_file(degradation_file, index_col=0)\n",
    "    df_dis_corrs = DataReader.read_from_file(disattenuation_file, index_col=0)\n",
    "    df_eval = DataReader.read_from_file(eval_file, index_col=0)\n",
    "\n",
    "    markdown_strs.append('*Note: this section assumes that the score used for evaluating machine scores '\n",
    "                         'is the score assigned by the first rater.*')\n",
    "    markdown_strs.append('### Human-human agreement')\n",
    "    markdown_strs.append(\"This table shows the human-human agreement on the \"\n",
    "                         \"double-scored evaluation data.\")\n",
    "    if continuous_human_score:\n",
    "        markdown_strs.append('For the computation of `kappa` and `wtkappa` '\n",
    "                             'human scores have beeen rounded to the nearest integer.')\n",
    "        \n",
    "    markdown_strs.append(\"The following are <span class='highlight_color'>highlighted </span>: \")\n",
    "    markdown_strs.append(' - Exact agreement (`exact_agr`) < 50%')\n",
    "    markdown_strs.append(' - Adjacent agreement (`adj_agr`) < 95%')\n",
    "    markdown_strs.append(' - Quadratic weighted kappa (`wtkappa`) < 0.7')\n",
    "    markdown_strs.append(' - Pearson correlation (`corr`) < 0.7')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    \n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_exact_agr = partial(color_highlighter, low=50, high=100)\n",
    "    formatter_adj_agr = partial(color_highlighter, low=95, high=100)\n",
    "    formatter_wtkappa_corr = partial(color_highlighter, low=0.7)\n",
    "    formatter_dict = {'exact_agr': formatter_exact_agr, \n",
    "                      'adj_agr': formatter_adj_agr,\n",
    "                      'wtkappa': formatter_wtkappa_corr, \n",
    "                      'corr': formatter_wtkappa_corr}\n",
    "    display(HTML(df_consistency.to_html(index=False,\n",
    "                                        escape=False,\n",
    "                                        float_format=float_format_func,\n",
    "                                        formatters=formatter_dict)))\n",
    "    \n",
    "    markdown_strs = ['### Degradation']\n",
    "    markdown_strs.append('The next table shows the degradation in the evaluation metrics '\n",
    "                         '(`diff`) when comparing the machine (`H-M`) to a second human (`H-H`). '\n",
    "                         'A positive degradation value indicates better human-machine performance. '\n",
    "                         'Note that the human-machine agreement is computed on the full '\n",
    "                         'dataset (to get a reliable estimate) whereas the human-human '\n",
    "                         'agreement is computed on the subset of responses that were double-scored.')\n",
    "    markdown_strs.append(\"\\nThe following degradation values are \"\n",
    "                         \"<span class='highlight_color'>highlighted</span>\")\n",
    "    markdown_strs.append(' - `corr` < -0.1')\n",
    "    markdown_strs.append(' - `wtkappa` < -0.1')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    df_eval_for_degradation = df_eval[df_degradation.columns].copy()\n",
    "    df_consistency_for_degradation = pd.concat([df_consistency]*len(df_eval), sort=True)\n",
    "    df_consistency_for_degradation = df_consistency_for_degradation[df_degradation.columns].copy()\n",
    "    df_consistency_for_degradation.index = df_eval_for_degradation.index\n",
    "\n",
    "    df_consistency_for_degradation['type'] = 'H-H'\n",
    "    df_eval_for_degradation['type'] = 'H-M'\n",
    "    df_degradation['type'] = 'diff'\n",
    "\n",
    "    df = pd.concat([df_consistency_for_degradation,\n",
    "                    df_eval_for_degradation,\n",
    "                    df_degradation], sort=True)\n",
    "    df = df[['type','corr', 'kappa', 'wtkappa', 'exact_agr', 'adj_agr', 'SMD']]\n",
    "    df = df.reset_index()\n",
    "    df = df.set_index(['index', 'type']).sort_index(level='index')\n",
    "    df.index.names = [None, None]\n",
    "    \n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_corr = partial(color_highlighter, low=-0.1, high=100)\n",
    "    formatter_wtkappa = partial(color_highlighter, low=-0.1, high=100)\n",
    "    formatter_dict = {'corr': formatter_corr, 'wtkappa': formatter_wtkappa}\n",
    "    display(HTML(df.to_html(float_format=float_format_func, \n",
    "                            formatters=formatter_dict, escape=False)))\n",
    "    \n",
    "    \n",
    "    markdown_strs = ['### Disattenuated correlations']\n",
    "    markdown_strs.append('The next table shows the correlations between human and machine scores, '\n",
    "                         'the correlations between two human scores, '  \n",
    "                         'and disattenuated correlations between human and machine scores computed as '\n",
    "                         'human-machine correlations divided by the square root of human-human '\n",
    "                         'correlation. '\n",
    "                         'Note that the human-machine correlation is computed on the full '\n",
    "                         'dataset (to get a reliable estimate) whereas the human-human '\n",
    "                         'correlation is computed on the subset of responses that were double-scored.')\n",
    "    markdown_strs.append(\"\\nThe following values are \"\n",
    "                         \"<span class='highlight_color'>highlighted</span>\")\n",
    "    markdown_strs.append(' - `disattenuated_corr` < -0.9')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_dis_corr = partial(color_highlighter, low=0.9)\n",
    "    formatter_dict = {'corr_disattenuated': formatter_dis_corr}\n",
    "    display(HTML(df_dis_corrs.to_html(index=True,\n",
    "                                      escape=False,\n",
    "                                      classes=['sortable'],\n",
    "                                      float_format=float_format_func,\n",
    "                                      formatters=formatter_dict)))\n",
    "else:  \n",
    "    markdown_strs.append(\"The configuration file did not specify \"\n",
    "                         \"`second_human_score_column` which is necessary to compute \"\n",
    "                         \"consistency metrics.\")\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Markdown('Model used: **{}**'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Markdown('Number of features in model: **{}**'.format(len(features_used)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "builtin_ols_models = ['LinearRegression',\n",
    "                      'EqualWeightsLR',\n",
    "                      'RebalancedLR',\n",
    "                      'NNLR',\n",
    "                      'LassoFixedLambdaThenNNLR',\n",
    "                      'LassoFixedLambdaThenLR',\n",
    "                      'PositiveLassoCVThenLR',\n",
    "                      'WeightedLeastSquares']\n",
    "\n",
    "builtin_lasso_models = ['LassoFixedLambda',\n",
    "                        'PositiveLassoCV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we first just show a summary of the OLS model and the main model parameters\n",
    "if model_name in builtin_ols_models:\n",
    "    display(Markdown('### Model summary'))\n",
    "    summary_file = join(output_dir, '{}_ols_summary.txt'.format(experiment_id))\n",
    "    with open(summary_file, 'r') as summf:\n",
    "        model_summary = summf.read()\n",
    "        print(model_summary)\n",
    "     \n",
    "    display(Markdown('### Model fit'))\n",
    "    df_fit = DataReader.read_from_file(join(output_dir, '{}_model_fit.{}'.format(experiment_id,\n",
    "                                                                                 file_format)))\n",
    "    display(HTML(df_fit.to_html(index=False,\n",
    "                                float_format=float_format_func)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Standardized and relative regression coefficients (betas)\n",
    "\n",
    "The relative coefficients are intended to show relative contribution of different feature and their primary purpose is to indentify whether one of the features has an unproportionate effect over the final score. They are computed as standardized/(sum of absolute values of standardized coefficients). \n",
    "\n",
    "Negative standardized coefficients are <span class=\"highlight_color\">highlighted</span>.\n",
    "\n",
    "**Note**: if the model contains negative coefficients, relative values will not sum up to one and their interpretation is generally questionable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_str = \"\"\"\n",
    "**Note**: The coefficients were estimated using LASSO regression. Unlike OLS (standard) linear regression, lasso estimation is based on an optimization routine and therefore the exact estimates may differ across different systems. \"\"\"\n",
    "\n",
    "if model_name in builtin_lasso_models:\n",
    "    display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_betas.sort_values(by='feature', inplace=True)\n",
    "display(HTML(df_betas.to_html(classes=['sortable'], \n",
    "                              index=False, \n",
    "                              escape=False,\n",
    "                              float_format=float_format_func,\n",
    "                              formatters={'standardized': color_highlighter})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here are the same values, shown graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_betas_sorted = df_betas.sort_values(by='standardized', ascending=False)\n",
    "df_betas_sorted.reset_index(drop=True, inplace=True)\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(8, 3)\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "grey_colors = sns.color_palette('Greys', len(features_used))[::-1]\n",
    "with sns.axes_style('whitegrid'):\n",
    "    ax1=fig.add_subplot(121)\n",
    "    sns.barplot(x=\"feature\", y=\"standardized\", data=df_betas_sorted, \n",
    "                order=df_betas_sorted['feature'].values,\n",
    "                palette=sns.color_palette(\"Greys\", 1), ax=ax1)\n",
    "    ax1.set_xticklabels(df_betas_sorted['feature'].values, rotation=90)\n",
    "    ax1.set_title('Values of standardized coefficients')\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_ylabel('')\n",
    "    # no pie chart if we have more than 15 features,\n",
    "    # if the feature names are long (pie chart looks ugly)\n",
    "    # or if there are any negative coefficients.\n",
    "    if len(features_used) <= 15 and longest_feature_name <= 10 and (df_betas_sorted['relative']>=0).all():\n",
    "        ax2=fig.add_subplot(133, aspect=True)\n",
    "        ax2.pie(abs(df_betas_sorted['relative'].values), colors=grey_colors, \n",
    "            labels=df_betas_sorted['feature'].values, normalize=True)\n",
    "        ax2.set_title('Proportional contribution of each feature')\n",
    "    else:\n",
    "        fig.set_size_inches(len(features_used), 3)\n",
    "    betas_file = join(figure_dir, '{}_betas.svg'.format(experiment_id))\n",
    "    plt.savefig(betas_file)\n",
    "\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(betas_file, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if model_name in builtin_ols_models:\n",
    "    display(Markdown('### Model diagnostics'))\n",
    "    display(Markdown(\"These are standard plots for model diagnostics for the main model. All information is computed based on the training set.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the OLS model file and create the diagnostics plots\n",
    "if model_name in builtin_ols_models:\n",
    "    ols_file = join(output_dir, '{}.ols'.format(experiment_id))\n",
    "    model = pickle.load(open(ols_file, 'rb'))\n",
    "    model_predictions = model.predict()\n",
    "\n",
    "    with sns.axes_style('white'):\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        f.set_size_inches((10, 4))\n",
    "        \n",
    "        ###\n",
    "        # for now, we do not show the influence plot since it can be slow to generate\n",
    "        ###\n",
    "        # sm.graphics.influence_plot(model.sm_ols, criterion=\"cooks\", size=10, ax=ax1)\n",
    "        # ax1.set_title('Residuals vs. Leverage', fontsize=16)\n",
    "        # ax1.set_xlabel('Leverage', fontsize=16)\n",
    "        # ax1.set_ylabel('Standardized Residuals', fontsize=16)\n",
    "\n",
    "        sm.qqplot(model.resid, stats.norm, fit=True, line='q', ax=ax1)\n",
    "        ax1.set_title('Normal Q-Q Plot', fontsize=16)\n",
    "        ax1.set_xlabel('Theoretical Quantiles', fontsize=16)\n",
    "        ax1.set_ylabel('Sample Quantiles', fontsize=16)\n",
    "\n",
    "        ax2.scatter(model_predictions, model.resid)\n",
    "        ax2.set_xlabel('Fitted values', fontsize=16)\n",
    "        ax2.set_ylabel('Residuals', fontsize=16)\n",
    "        ax2.set_title('Residuals vs. Fitted', fontsize=16)\n",
    "\n",
    "        imgfile = join(figure_dir, '{}_ols_diagnostic_plots.png'.format(experiment_id))\n",
    "        plt.savefig(imgfile)\n",
    "\n",
    "        if use_thumbnails:\n",
    "            show_thumbnail(imgfile, next(id_generator))\n",
    "        else:\n",
    "            display(Image(imgfile))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall association statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = (\"The tables in this section show the standard association metrics between \"\n",
    "                \"*observed* human scores and different types of machine scores. \"\n",
    "                \"These results are computed on the evaluation set. `raw_trim` scores \"\n",
    "                \"are truncated to [{}, {}]. `raw_trim_round` scores are computed by first truncating \"\n",
    "                \"and then rounding the predicted score. Scaled scores are computed by re-scaling \"\n",
    "                \"the predicted scores using mean and standard deviation of human scores as observed \"\n",
    "                \"on the training data and mean and standard deviation of machine scores as predicted \"\n",
    "                \"for the training set.\".format(min_score, max_score))\n",
    "display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive holistic score statistics\n",
    "\n",
    "The table shows distributional properties of human and system scores. SMD values lower then -0.15 or higher than 0.15 are <span class=\"highlight_color\">highlighted</span>.\n",
    "\n",
    "*Please note that for raw scores, SMD values are likely to be affected by possible differences in scale.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_or_scaled = \"scaled\" if use_scaled_predictions else \"raw\"\n",
    "eval_file = join(output_dir, '{}_eval.{}'.format(experiment_id, file_format))\n",
    "df_eval = DataReader.read_from_file(eval_file, index_col=0)\n",
    "distribution_columns = ['N', 'h_mean', 'sys_mean', 'h_sd',  'sys_sd', 'h_min', 'sys_min', 'h_max', 'sys_max', 'SMD']\n",
    "association_columns = ['N'] + [column for column in df_eval.columns if not column in distribution_columns]\n",
    "df_distribution = df_eval[distribution_columns]\n",
    "df_association = df_eval[association_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.width=10\n",
    "formatter = partial(color_highlighter, low=-0.15, high=0.15)\n",
    "HTML('<span style=\"font-size:95%\">'+ df_distribution.to_html(classes=['sortable'], \n",
    "                                                             escape=False,\n",
    "                                                             formatters={'SMD': formatter},\n",
    "                                                             float_format=float_format_func) + '</span>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Association statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = ['The table shows the standard association metrics between human scores and machine scores.']\n",
    "if continuous_human_score:\n",
    "    markdown_str.append(\"Note that for computation of `kappa` both human and machine scores are rounded.\")\n",
    "else:\n",
    "    markdown_str.append(\"Note that for computation of `kappa` all machine scores are rounded.\")\n",
    "\n",
    "Markdown('\\n'.join(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.width=10\n",
    "HTML('<span style=\"font-size:95%\">'+ df_association.to_html(classes=['sortable'], \n",
    "                                                            escape=False,\n",
    "                                                            float_format=float_format_func) + '</span>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = [\"Confusion matrix using {}, trimmed, and rounded scores and human scores (rows=system, columns=human).\".format(raw_or_scaled)]\n",
    "\n",
    "if continuous_human_score:\n",
    "    markdown_str.append(\"Note: Human scores have beeen rounded to the nearest integer.\")\n",
    "            \n",
    "Markdown('\\n'.join(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confmat_file = join(output_dir, '{}_confMatrix.{}'.format(experiment_id, file_format))\n",
    "df_confmat = DataReader.read_from_file(confmat_file, index_col=0)\n",
    "df_confmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of human and machine scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_strs = [\"The histogram and the table below show the distibution of \"\n",
    "                 \"human scores and {}, trimmed, and rounded machine scores \"\n",
    "                 \"(as % of all responses).\".format(raw_or_scaled)]\n",
    "markdown_strs.append(\"Differences in the table between human and machine distributions \"\n",
    "                     \"larger than 5 percentage points are <span class='highlight_color'>highlighted</span>.\")\n",
    "if continuous_human_score:\n",
    "    markdown_strs.append(\"Note: Human scores have beeen rounded to the nearest integer.\")\n",
    "    \n",
    "display(Markdown('\\n'.join(markdown_strs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoredist_file = join(output_dir, '{}_score_dist.{}'.format(experiment_id, file_format))\n",
    "df_scoredist = DataReader.read_from_file(scoredist_file, index_col=0)\n",
    "df_scoredist_melted = pd.melt(df_scoredist, id_vars=['score'])\n",
    "df_scoredist_melted = df_scoredist_melted[df_scoredist_melted['variable'] != 'difference']\n",
    "\n",
    "# get the colors for the plot\n",
    "colors = sns.color_palette(\"Greys\", 2)\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "\n",
    "    # make a barplot without a legend since we will \n",
    "    # add one manually later\n",
    "    p = sns.catplot(x=\"score\", y=\"value\", hue=\"variable\", kind=\"bar\",\n",
    "                    palette=colors, data=df_scoredist_melted, \n",
    "                    height=3, aspect=2, legend=False)\n",
    "    p.set_axis_labels('score', '% of responses')\n",
    "    \n",
    "    # add a legend with the right colors\n",
    "    axis = p.axes[0][0]\n",
    "    legend = axis.legend(labels=('Human', 'Machine'), title='', frameon=True, fancybox=True)\n",
    "    legend.legendHandles[0].set_color(colors[0])\n",
    "    legend.legendHandles[1].set_color(colors[1])\n",
    "\n",
    "    imgfile = join(figure_dir, '{}_score_dist.svg'.format(experiment_id))\n",
    "    plt.savefig(imgfile)\n",
    "\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(imgfile, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatter = partial(color_highlighter, low=0, high=5, absolute=True)\n",
    "df_html = df_scoredist.to_html(classes=['sortable'], index=False, \n",
    "                               escape=False, formatters={'difference': formatter})\n",
    "display(HTML(df_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_strs = ['### True score evaluations']\n",
    "\n",
    "raw_or_scaled = \"scaled\" if use_scaled_predictions else \"raw\"\n",
    "true_eval_file = join(output_dir, '{}_true_score_eval.{}'.format(experiment_id, file_format))\n",
    "if exists(true_eval_file): \n",
    "    df_true_eval = DataReader.read_from_file(true_eval_file, index_col=0)\n",
    "    df_true_eval.replace({np.nan: '-'}, inplace=True)\n",
    "    prmse_columns = ['N','N raters', 'N single', 'N multiple', \n",
    "                     'Variance of errors', 'True score var',\n",
    "                     'MSE true', 'PRMSE true']\n",
    "    df_prmse = df_true_eval[prmse_columns]\n",
    "\n",
    "    markdown_strs.append(\"The tables in this section show how well system scores can \"\n",
    "                         \"predict *true* scores. According to Test theory, a *true* score \"\n",
    "                         \"is a score that would have been obtained if there were no errors \"\n",
    "                         \"in measurement. While true scores cannot be observed, the variance \"\n",
    "                         \"of true scores and the prediction error can be estimated using observed \"\n",
    "                         \"human scores when multiple human ratings are available for a subset of \"\n",
    "                         \"responses.\")\n",
    "\n",
    "    if rater_error_variance is None: \n",
    "        \n",
    "        rater_variance_source = 'estimated'\n",
    "        # if we estimated rater error variance from the data,\n",
    "        # we display the variance of the two human raters\n",
    "        # so that the user can verify there is no bias\n",
    "        # We get that data from existing analyses\n",
    "        df_human_variance = df_consistency[['N', 'h1_sd', 'h2_sd']].copy()\n",
    "        df_human_variance['N_double'] = df_human_variance['N']\n",
    "        df_human_variance['h1_var (double)'] = df_human_variance['h1_sd']**2\n",
    "        df_human_variance['h2_var (double)'] = df_human_variance['h2_sd']**2\n",
    "        df_human_variance['N_total'] = df_eval.iloc[0]['N']\n",
    "        df_human_variance['h1_var (single)'] = df_eval.iloc[0]['h_sd']**2\n",
    "        df_human_variance.index = ['human']\n",
    "\n",
    "\n",
    "        if context == 'rsmtool':\n",
    "            label_column = \"test_label_column\"\n",
    "        else:\n",
    "            label_column = \"human_score_column\"\n",
    "\n",
    "        markdown_strs.append(\"In this notebook the variance of true scores is estimated using \"\n",
    "                             \"the human ratings available for \"\n",
    "                             \"responses in the evaluation set. Note that the analyses in this \"\n",
    "                             \"section assume that the values \"\n",
    "                            \"in `{}` and `second_human_score_column` are independent scores \"\n",
    "                            \"from different raters or groups of raters. These analyses are \"\n",
    "                            \"not applicable to a situation where `{}` contains an average \"\n",
    "                            \"score from multiple raters\".format(label_column, label_column))\n",
    "\n",
    "        markdown_strs.append(\"#### Variance of human scores\")\n",
    "        markdown_strs.append(\"The table below shows variance of both sets of human scores \"\n",
    "                            \"for the whole evaluation set and for the subset of responses \"\n",
    "                            \"that were double-scored. Large differences in variance between \"\n",
    "                            \"the two human scores require further investigation.\")\n",
    "        display(Markdown('\\n'.join(markdown_strs)))\n",
    "        pd.options.display.width=10\n",
    "        column_order = ['N_total', 'N_double', 'h1_var (single)', 'h1_var (double)', 'h2_var (double)']\n",
    "        display(HTML('<span style=\"font-size:95%\">'+ df_human_variance[column_order].to_html(classes=['sortable'], \n",
    "                                                                                            escape=False,\n",
    "                                                                                            float_format=float_format_func) + '</span>'))\n",
    "    else:\n",
    "        markdown_strs.append(\"In this notebook the variance of true scores was \"\n",
    "                            \"estimated using the value of rater error variance \"\n",
    "                            \"supplied by the user ({})\".format(rater_error_variance))\n",
    "        display(Markdown('\\n'.join(markdown_strs)))\n",
    "        rater_variance_source = 'supplied'\n",
    "    \n",
    "    \n",
    "    markdown_strs = [\"#### Proportional reduction in mean squared error (PRMSE)\"]\n",
    "    markdown_strs.append(\"The table shows {} variance of human rater errors, \"\n",
    "                         \"true score variance, mean squared error (MSE) and \"\n",
    "                         \"proportional reduction in mean squared error (PRMSE) for \"\n",
    "                         \"predicting a true score with system score. As for other evaluations, \"\n",
    "                         \"these results are computed on the evaluation set. `raw_trim` scores \"\n",
    "                         \"are truncated to [{}, {}]. `raw_trim_round` scores are computed \"\n",
    "                         \"by first truncating and then rounding the predicted score. Scaled scores \"\n",
    "                         \"are computed by re-scaling the predicted scores using mean and standard \"\n",
    "                         \"deviation of human scores as observed on the training data and mean and \"\n",
    "                         \"standard deviation of machine scores as predicted for the training set.\".format(rater_variance_source,\n",
    "                                                                                                          min_score,\n",
    "                                                                                                          max_score))\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    pd.options.display.width=10\n",
    "    display(HTML('<span style=\"font-size:95%\">'+ df_prmse.to_html(classes=['sortable'], \n",
    "                                                               escape=False,\n",
    "                                                               float_format=float_format_func) + '</span>'))\n",
    "else:\n",
    "    markdown_strs.append(\"The configuration file did not specify \"\n",
    "                         \"`second_human_score_column` or `rater_error_variance`. \"\n",
    "                         \"At least one of these must be specified to compute \"\n",
    "                         \"evaluations against true scores.\")\n",
    "    display(Markdown('\\n'.join(markdown_strs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This notebook generates barplot with evaluation metrics for all groups specified in groups_eval variable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basic_metrics = {('wtkappa', 'trim'): [0.7],\n",
    "                 ('corr', 'trim'): [0.7],\n",
    "                 ('DSM', 'trim_round'): [0.1, -0.1],\n",
    "                 ('DSM', 'trim'): [0.1, -0.1],\n",
    "                 ('R2', 'trim'): [],\n",
    "                 ('RMSE', 'trim'): []}\n",
    "\n",
    "colprefix = 'scale' if use_scaled_predictions else 'raw'\n",
    "metrics = dict([('{}.{}_{}'.format(k[0], colprefix, k[1]), v) for k,v in basic_metrics.items()])\n",
    "num_metrics = len(metrics)\n",
    "\n",
    "for group in groups_eval:\n",
    "    display(Markdown('### Evaluation by {}'.format(group)))\n",
    "    \n",
    "    eval_group_file = join(output_dir, '{}_eval_by_{}.{}'.format(experiment_id, group, file_format))\n",
    "    df_eval_group_all = DataReader.read_from_file(eval_group_file, index_col=0)\n",
    "    \n",
    "    df_eval_group_all.index.name = group\n",
    "    df_eval_group_all.reset_index(inplace=True)\n",
    "    \n",
    "    # If we have threshold per group, apply it now. Keep \"All data\" in any case. \n",
    "    if group in min_n_per_group:\n",
    "        display(Markdown(\"The report only shows the results for groups with \"\n",
    "                         \"at least {} responses in the evaluation set.\".format(min_n_per_group[group])))\n",
    "        \n",
    "        df_eval_group = df_eval_group_all[(df_eval_group_all['N'] >= min_n_per_group[group]) |\n",
    "                                         (df_eval_group_all[group] == 'All data')].copy()\n",
    "    else:\n",
    "        df_eval_group = df_eval_group_all.copy()\n",
    "\n",
    "\n",
    "    # Define the order of the bars: put 'All data' first and 'No info' last.\n",
    "    group_levels = list(df_eval_group[group])\n",
    "    group_levels = [level for level in group_levels if level != 'All data']\n",
    "    \n",
    "    # We only want to show the report if we have anything other than All data\n",
    "    if len(group_levels) > 0:\n",
    "        \n",
    "        if 'No info' in group_levels:\n",
    "            bar_names = ['All data'] + [level for level in group_levels if level != 'No info'] + ['No info']\n",
    "        else:\n",
    "            bar_names = ['All data'] + group_levels\n",
    "\n",
    "        fig = plt.figure()\n",
    "        (figure_width, \n",
    "         figure_height, \n",
    "         num_rows, \n",
    "         num_columns, \n",
    "         wrapped_bar_names) = compute_subgroup_plot_params(bar_names, num_metrics)\n",
    "\n",
    "        fig.set_size_inches(figure_width, figure_height)\n",
    "        with sns.axes_style('white'), sns.plotting_context('notebook', font_scale=1.2):\n",
    "            for i, metric in enumerate(sorted(metrics.keys())):\n",
    "                df_plot = df_eval_group[[group, metric]]\n",
    "                ax = fig.add_subplot(num_rows, num_columns, i + 1)\n",
    "                for lineval in metrics[metric]:\n",
    "                    ax.axhline(y=float(lineval), linestyle='--', linewidth=0.5, color='black')\n",
    "                sns.barplot(x=df_plot[group], y=df_plot[metric], color='grey', ax=ax, order=bar_names)\n",
    "                ax.set_xticklabels(wrapped_bar_names, rotation=90) \n",
    "                ax.set_xlabel('')\n",
    "                ax.set_ylabel('')\n",
    "\n",
    "                # set the y-limits of the plots appropriately\n",
    "                if metric.startswith('corr') or metric.startswith('wtkappa'):\n",
    "                    if df_plot[metric].min() < 0:\n",
    "                        y_limits = (-1.0, 1.0)\n",
    "                        ax.axhline(y=0.0, linestyle='--', linewidth=0.5, color='black')\n",
    "                    else:\n",
    "                        y_limits = (0.0, 1.0)\n",
    "                    ax.set_ylim(y_limits)\n",
    "                elif metric.startswith('R2'):\n",
    "                    min_value = df_plot[metric].min()\n",
    "                    if min_value < 0:\n",
    "                        y_limits = (min_value - 0.1, 1.0)\n",
    "                        ax.axhline(y=0.0, linestyle='--', linewidth=0.5, color='black')\n",
    "                    else:\n",
    "                        y_limits = (0.0, 1.0)\n",
    "                    ax.set_ylim(y_limits)\n",
    "                elif metric.startswith('RMSE'):\n",
    "                    max_value = df_plot[metric].max()\n",
    "                    y_limits = (0.0, max(max_value + 0.1, 1.0))\n",
    "                    ax.set_ylim(y_limits)\n",
    "                elif metric.startswith('DSM'):\n",
    "                    min_value = df_plot[metric].min()\n",
    "                    if min_value < 0:\n",
    "                        ax.axhline(y=0.0, linestyle='--', linewidth=0.5, color='black')\n",
    "\n",
    "                # set the title\n",
    "                ax.set_title('{} by {}'.format(metric, group))\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "        imgfile = join(figure_dir, '{}_eval_by_{}.svg'.format(experiment_id, group))\n",
    "        plt.savefig(imgfile)\n",
    "\n",
    "        if use_thumbnails:\n",
    "            show_thumbnail(imgfile, next(id_generator))\n",
    "        else:\n",
    "            plt.show()\n",
    "            \n",
    "    else:\n",
    "        display(Markdown(\"None of the groups in {} had {} or more responses.\".format(group,\n",
    "                                                                                    min_n_per_group[group])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional fairness analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a note for ETS users\n",
    "from rsmtool import HAS_RSMEXTRA\n",
    "if HAS_RSMEXTRA:\n",
    "    from rsmextra.settings import fairness_note\n",
    "    display(Markdown(fairness_note))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section presents additional fairness analyses described in detail in [Loukina et al. (2019)](https://aclweb.org/anthology/papers/W/W19/W19-4401/).\n",
    "These analyses consider separately different definitions of fairness and can assist in further trouble-shooting \n",
    "the observed subgroup differences. The evaluations focuses on three dimensions:\n",
    "\n",
    "* **Outcome fairness** measures:  \n",
    "\n",
    "    - *Overall score accuracy*: whether the automated scores are equally accurate for each group. The metric shows how much of the variance in squared error $(S-H)^2$ is explained by subgroup membership.\n",
    "\n",
    "    -  *Overall score difference*: whether the automated scores are consistently different from human scores for members of a certain group. The metric shows how much of the variance in actual error $S-H$ is explained by subgroup membership. \n",
    "\n",
    "The differences in the outcome fairness measures might be due to different mean scores (different score distributions) across subgroups or due to differential treatment of different subgroups by the scoring engine or both. \n",
    "\n",
    "* **Process fairness** measures:\n",
    "\n",
    "    - *Conditional score difference*: whether the automated scoring model assigns different scores to members from different groups despite them having the same construct proficiency. The metric shows how much additional variance in actual error ($S-H$) is explained by subgroup membership after controlling for human score, which can be thought of as a reasonable proxy for proficiency. \n",
    "\n",
    "The differences in process fairness measures indicate differential treatment of different subgroups by the scoring model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and auxiliary function that we will need \n",
    "# for the plot later on\n",
    "def errplot(x, y, xerr, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    data = kwargs.pop(\"data\")\n",
    "    # let's remove color from kwargs\n",
    "    color = kwargs.pop('color')\n",
    "    data.plot(x=x, y=y, xerr=xerr,\n",
    "              kind=\"barh\", ax=ax,\n",
    "              color=colors,\n",
    "              **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check if we already created the merged file in another notebook\n",
    "\n",
    "try:\n",
    "    df_pred_preproc_merged\n",
    "except NameError:\n",
    "    df_pred_preproc_merged = pd.merge(df_pred_preproc, df_test_metadata, on = 'spkitemid')\n",
    "    \n",
    "# check which score we are using\n",
    "system_score_column = \"scale_trim\" if use_scaled_predictions else \"raw_trim\"\n",
    "\n",
    "for group in groups_eval:\n",
    "    \n",
    "    display(Markdown(\"### Additional fairness evaluation by {}\".format(group)))\n",
    "    \n",
    "    # run the actual analyses. We are currently doing this in the notebook\n",
    "    # so that it is easy to skip these if necessary. The notebook\n",
    "    # and analyses are set up in a way that will make it easy to move these\n",
    "    # in future to the main pipeline and read in the outputs here. \n",
    "    fit_dict, fairness_container = get_fairness_analyses(df_pred_preproc_merged,\n",
    "                                                         group,\n",
    "                                                         system_score_column)\n",
    "    \n",
    "    # write the results to disk so that we can consider them in tests\n",
    "    write_fairness_results(fit_dict,\n",
    "                           fairness_container,\n",
    "                           group,\n",
    "                           output_dir,\n",
    "                           experiment_id,\n",
    "                           file_format)\n",
    "    \n",
    "    # first show summary results\n",
    "    df_fairness = fairness_container['fairness_metrics_by_{}'.format(group)]\n",
    "\n",
    "    \n",
    "    display(Markdown(\"The summary table shows the overall score accuracy (OSA), overall score difference (OSD) \"\n",
    "                    \"and conditional score difference (CSD). The first row reports the percentage of variance, \"\n",
    "                    \" explained by group membership, the second row shows $p$ value. \"\n",
    "                     \"{} was used as a reference category. \"\n",
    "                    \"Larger values of R2 indicate larger differences between subgroups. \" \n",
    "                    \"Further detail about each model can be found in [intermediate \"\n",
    "                    \"output files](#Links-to-Intermediate-Files).\".format(df_fairness['base_category'].values[0])))\n",
    "    \n",
    "    display(HTML(df_fairness.loc[['R2', 'sig'],\n",
    "                                 ['Overall score accuracy',\n",
    "                                  'Overall score difference',\n",
    "                                  'Conditional score difference']].to_html(classes='sortable',\n",
    "                                                                     float_format=float_format_func)))\n",
    "\n",
    "    \n",
    "    Markdown_str = [(\"The plots show error estimates for different categories for each group \"\n",
    "                    \"(squared error for OSA, raw error for OSD, and conditional raw error for CSD) \"\n",
    "                    \"The estimates have been adjusted for the value of the group used as the Intercept. \"\n",
    "                    \"Black lines show 95% confidence intervals estimated by the model.\")]\n",
    "    \n",
    "    # if we only care about groups above threshold, identify those.\n",
    "    \n",
    "    category_counts = df_pred_preproc_merged[group].value_counts()        \n",
    "    \n",
    "    if group in min_n_per_group:\n",
    "        Markdown_str.append(\"While the models were fit to all data, the plots only show estimates for \"\n",
    "                            \"categories with more than {} members and the Intercept ({}).\".format(min_n_per_group[group],\n",
    "                                                                                                  df_fairness['base_category'].values[0]))\n",
    "        \n",
    "        groups_by_size = category_counts[category_counts >= min_n_per_group[group]].index\n",
    "        df_pred_preproc_selected = df_pred_preproc_merged[df_pred_preproc_merged[group].isin(groups_by_size)].copy()\n",
    "    else:\n",
    "        groups_by_size = category_counts.index\n",
    "        df_pred_preproc_selected = df_pred_preproc_merged.copy()\n",
    "    \n",
    "    display(Markdown('\\n'.join(Markdown_str)))\n",
    "   \n",
    "    \n",
    "    if len(groups_by_size) > 0:\n",
    "\n",
    "        # assemble all coefficients into a long data frame\n",
    "        all_coefs = []\n",
    "        for metrics in ['osa', 'csd', 'osd']:\n",
    "            df_metrics = fairness_container['estimates_{}_by_{}'.format(metrics, group)].copy()\n",
    "            # compute adjusted error estimates by adding the value of the Intercept\n",
    "            # to all non-Intercept values\n",
    "            non_index_cols = [r for r in df_metrics.index if not \"Intercept\" in r]\n",
    "            index_col = [r for r in df_metrics.index if \"Intercept\" in r]\n",
    "            df_metrics['error_estimate'] = df_metrics['estimate']\n",
    "            df_metrics.loc[non_index_cols,\n",
    "                          'error_estimate'] = df_metrics.loc[non_index_cols,\n",
    "                                                            'estimate'] + df_metrics.loc[index_col,\n",
    "                                                                                        'estimate'].values\n",
    "            # create a column for metrics\n",
    "            df_metrics['metrics'] = metrics\n",
    "            # only use groups with values above threshold and the intercept\n",
    "            df_metrics[group] = df_metrics.index\n",
    "            df_metrics_selected = df_metrics[df_metrics[group].isin(groups_by_size) |\n",
    "                                             (df_metrics[group] == index_col[0])]\n",
    "            all_coefs.append(df_metrics_selected)\n",
    "\n",
    "\n",
    "        # show coefficient plots\n",
    "        # define groups and color palette\n",
    "        colors = sns.color_palette(\"Greys_r\", len(groups_by_size))\n",
    "\n",
    "        df_coefs_all = pd.concat(all_coefs)\n",
    "\n",
    "        # compute the size of the confidence interval from the boundary\n",
    "        df_coefs_all['CI'] = np.abs(df_coefs_all['[0.025'] - df_coefs_all['estimate'])\n",
    "\n",
    "        # plot the coefficients\n",
    "        with sns.axes_style('whitegrid'), sns.plotting_context('notebook', font_scale=2):\n",
    "            g = sns.FacetGrid(df_coefs_all, col=\"metrics\",\n",
    "                              height=10, col_order = ['osa', 'osd', 'csd'])\n",
    "            g.map_dataframe(errplot, group, \"error_estimate\",  \"CI\").set_axis_labels(\"Error estimate\",\n",
    "                                                                                    group)\n",
    "\n",
    "            imgfile = join(figure_dir, '{}_fairness_estimates_{}.svg'.format(experiment_id, group))\n",
    "            plt.savefig(imgfile)\n",
    "            if use_thumbnails:\n",
    "                show_thumbnail(imgfile, next(id_generator))\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "        # Show the conditional score plot\n",
    "        markdown_str = [(\"The plot shows average {} system score for each \"\n",
    "                        \"group conditioned \"\n",
    "                         \"on human score.\".format(system_score_column))]\n",
    "        if group in min_n_per_group:\n",
    "            markdown_str.append(\"The plot only shows estimates for \"\n",
    "                               \"categories with more than {} members and the\"\n",
    "                                \"reference category ({}).\".format(min_n_per_group[group],\n",
    "                                                          df_fairness['base_category'].values[0]))\n",
    "        \n",
    "        display(Markdown('\\n'.join(markdown_str)))\n",
    "        \n",
    "\n",
    "\n",
    "        with sns.axes_style('whitegrid'), sns.plotting_context('notebook', font_scale=1.2):\n",
    "            p = sns.catplot(x='sc1', y=system_score_column,\n",
    "                            hue=group, \n",
    "                            hue_order = groups_by_size,\n",
    "                            palette=colors,\n",
    "                            legend_out=True,\n",
    "                            kind=\"point\",\n",
    "                            data=df_pred_preproc_selected)\n",
    "\n",
    "            #plt.tight_layout(h_pad=1.0)\n",
    "            imgfile = join(figure_dir, '{}_conditional_score_{}.svg'.format(experiment_id, group))\n",
    "            plt.savefig(imgfile)\n",
    "            if use_thumbnails:\n",
    "                show_thumbnail(imgfile, next(id_generator))\n",
    "            else:\n",
    "                plt.show()\n",
    "    else:\n",
    "        display(Markdown(\"None of the groups in {} had {} or more responses in the evaluation set.\".format(group,\n",
    "                                                                                     min_n_per_group[group])))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis\n",
    "\n",
    "PCA using scaled data and singular value decomposition. This is computed using processed features after the truncation of outliers and other transformations specified in feature config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_file = join(output_dir, '{}_pca.{}'.format(experiment_id, file_format))\n",
    "df_pca = DataReader.read_from_file(pca_file, index_col=0)\n",
    "df_pca.sort_index(inplace=True)\n",
    "HTML(df_pca.to_html(classes=['sortable'], float_format=float_format_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcavar_file = join(output_dir, '{}_pcavar.{}'.format(experiment_id, file_format))\n",
    "df_pcavar = DataReader.read_from_file(pcavar_file, index_col=0)\n",
    "df_pcavar.sort_index(inplace=True)\n",
    "HTML(df_pcavar.to_html(classes=['sortable'], float_format=float_format_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate the Scree plot\n",
    "with sns.axes_style('white'):\n",
    "    num_components = len(df_pcavar.columns)\n",
    "    labels = list(df_pcavar.columns)\n",
    "    ax = df_pcavar.transpose().plot(y='Eigenvalues', kind='line', \n",
    "                                    color='black', linestyle='dashed', marker='o', legend=False,\n",
    "                                    linewidth=1, use_index=True, xticks=range(num_components),\n",
    "                                    figsize=(11, 5), title='Scree Plot: Principal Component Analysis')\n",
    "    ax.set_ylabel('Variances')\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "    imgfile = join(figure_dir, '{}_pca.svg'.format(experiment_id))\n",
    "    plt.savefig(imgfile)\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(imgfile, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links to intermediate files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the hyperlinks below to see the intermediate experiment files generated as part of this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rsmtool.utils.notebook import show_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_files(output_dir, experiment_id, file_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "system_name = platform.system()\n",
    "\n",
    "# People might not know what 'Darwin' is, so we should replace that with 'Mac OS X'\n",
    "if system_name == 'Darwin':\n",
    "    system_name = 'Mac OS X'\n",
    "    \n",
    "# get the architecture\n",
    "architecture = platform.architecture()[0]\n",
    "\n",
    "# get the rsmtool version\n",
    "rsmtool_version_str = '.'.join(map(str, rsmtool_version))\n",
    "\n",
    "display(Markdown('This report was generated using rsmtool v{} on a '\n",
    "                 '{} computer running {}.'.format(rsmtool_version_str, \n",
    "                                                  architecture, \n",
    "                                                  system_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "package_names = '\\n'.join(sorted([\"%s==%s\" % (i.key, i.version) for i in pkg_resources.working_set]))\n",
    "display(HTML('<div id=\"packages\"><pre>{}</pre></div>'.format(package_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "// Code to dynamically generate table of contents at the top of the HTML file\n",
    "var tocEntries = ['<ul>'];\n",
    "var anchors = $('a.anchor-link');\n",
    "var headingTypes = $(anchors).parent().map(function() { return $(this).prop('tagName')});\n",
    "var headingTexts = $(anchors).parent().map(function() { return $(this).text()});\n",
    "var subList = false;\n",
    "\n",
    "$.each(anchors, function(i, anch) {\n",
    "    var hType = headingTypes[i];\n",
    "    var hText = headingTexts[i];\n",
    "    hText = hText.substr(0, hText.length - 1);\n",
    "    if (hType == 'H2') {\n",
    "        if (subList) {\n",
    "            tocEntries.push('</ul>')\n",
    "            subList = false;\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "    else if (hType == 'H3') {\n",
    "        if (!subList) {\n",
    "            subList = true;\n",
    "            tocEntries.push('<ul>')\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "});\n",
    "tocEntries.push('</ul>')\n",
    "$('#toc').html(tocEntries.join(' '))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}