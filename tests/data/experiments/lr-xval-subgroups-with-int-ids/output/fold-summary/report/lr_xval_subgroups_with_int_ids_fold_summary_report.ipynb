{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting options for the plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats={'retina', 'svg'}\n",
    "%config InlineBackend.rc={'savefig.dpi': 150}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import platform\n",
    "import time\n",
    "\n",
    "from collections import defaultdict as dd\n",
    "from functools import partial\n",
    "from os.path import abspath, dirname, exists, join\n",
    "from string import Template\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython import sys_info\n",
    "from IPython.display import display, HTML, Image, Javascript, Markdown, SVG\n",
    "\n",
    "from rsmtool.utils.files import (get_output_directory_extension,\n",
    "                                 parse_json_with_comments)\n",
    "from rsmtool.utils.notebook import (float_format_func,\n",
    "                                    int_or_float_format_func,\n",
    "                                    bold_highlighter,\n",
    "                                    color_highlighter,\n",
    "                                    show_thumbnail)\n",
    "\n",
    "from rsmtool.reader import DataReader\n",
    "from rsmtool.writer import DataWriter\n",
    "from rsmtool.version import VERSION as rsmtool_version\n",
    "\n",
    "# turn off interactive plotting\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_report_dir = os.environ.get('RSM_REPORT_DIR', None)\n",
    "if rsm_report_dir is None:\n",
    "    rsm_report_dir = os.getcwd()\n",
    "\n",
    "rsm_environ_config = join(rsm_report_dir, '.environ.json')\n",
    "if not exists(rsm_environ_config):\n",
    "    raise FileNotFoundError('The file {} cannot be located. '\n",
    "                            'Please make sure that either (1) '\n",
    "                            'you have set the correct directory with the `RSM_REPORT_DIR` '\n",
    "                            'environment variable, or (2) that your `.environ.json` '\n",
    "                            'file is in the same directory as your notebook.'.format(rsm_environ_config))\n",
    "    \n",
    "environ_config = parse_json_with_comments(rsm_environ_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "  div.prompt.output_prompt { \n",
    "    color: white; \n",
    "  }\n",
    "  \n",
    "  span.highlight_color {\n",
    "    color: red;\n",
    "  }\n",
    "  \n",
    "  span.highlight_bold {\n",
    "    font-weight: bold;  \n",
    "  }\n",
    "    \n",
    "  @media print {\n",
    "    @page {\n",
    "      size: landscape;\n",
    "      margin: 0cm 0cm 0cm 0cm;\n",
    "    }\n",
    "\n",
    "    * {\n",
    "      margin: 0px;\n",
    "      padding: 0px;\n",
    "    }\n",
    "\n",
    "    #toc {\n",
    "      display: none;\n",
    "    }\n",
    "\n",
    "    span.highlight_color, span.highlight_bold {\n",
    "        font-weight: bolder;\n",
    "        text-decoration: underline;\n",
    "    }\n",
    "\n",
    "    div.prompt.output_prompt {\n",
    "      display: none;\n",
    "    }\n",
    "    \n",
    "    h3#Python-packages, div#packages {\n",
    "      display: none;\n",
    "  }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you will need to set the following manually\n",
    "# if you are using this notebook interactively.\n",
    "summary_id = environ_config.get('SUMMARY_ID')\n",
    "description = environ_config.get('DESCRIPTION')\n",
    "jsons = environ_config.get('JSONS')\n",
    "output_dir = environ_config.get('OUTPUT_DIR')\n",
    "use_thumbnails = environ_config.get('USE_THUMBNAILS')\n",
    "file_format_summarize = environ_config.get('FILE_FORMAT')\n",
    "\n",
    "# groups for subgroup analysis.\n",
    "groups_desc = environ_config.get('GROUPS_FOR_DESCRIPTIVES') \n",
    "groups_eval = environ_config.get('GROUPS_FOR_EVALUATIONS') \n",
    "\n",
    "# javascript path\n",
    "javascript_path = environ_config.get(\"JAVASCRIPT_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize id generator for thumbnails\n",
    "id_generator = itertools.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(join(javascript_path, \"sort.js\"), \"r\", encoding=\"utf-8\") as sortf:\n",
    "    display(Javascript(data=sortf.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the information about all models\n",
    "model_list = []\n",
    "for (json_file, experiment_name) in jsons:\n",
    "    model_config = json.load(open(json_file))\n",
    "    model_id = model_config['experiment_id']\n",
    "    model_name = experiment_name if experiment_name else model_id\n",
    "    model_csvdir = dirname(json_file)\n",
    "    model_file_format = get_output_directory_extension(model_csvdir, model_id)\n",
    "    model_list.append((model_id, model_name, model_config, model_csvdir, model_file_format))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(\"This report presents the analysis for **{}**: {} \\n \".format(summary_id, description))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(time.strftime('%c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a matched list of model ids and descriptions\n",
    "models_and_desc = zip([model_name for (model_id, model_name, config, csvdir, model_file_format) in model_list],\n",
    "                      [config['description'] for (model_id, model_name, config, csvdir, file_format) in model_list])\n",
    "model_desc_list = '\\n\\n'.join(['**{}**: {}'.format(m, d) for (m, d) in models_and_desc])\n",
    "\n",
    "Markdown(\"The report compares the following models: \\n\\n {}\".format(model_desc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_thumbnails:\n",
    "    display(Markdown(\"\"\"***Note: Images in this report have been converted to \"\"\"\n",
    "                     \"\"\"clickable thumbnails***\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize_feature_correlations(model_list, file_suffix, header, file_format_summarize):\n",
    "    corrs = []\n",
    "    for (model_id, model_name, config, csvdir, file_format) in model_list:\n",
    "        corr_file = os.path.join(csvdir, '{}_{}.{}'.format(model_id, file_suffix, file_format))\n",
    "        if os.path.exists(corr_file):\n",
    "            model_corrs = DataReader.read_from_file(corr_file, index_col=0)\n",
    "            model_corrs.index = [model_name]\n",
    "            corrs.append(model_corrs)\n",
    "    if not len(corrs) == 0:\n",
    "        df_summ = pd.concat(corrs, sort=True)\n",
    "        display(header)\n",
    "        display(HTML(df_summ.to_html(index=True, classes = ['sortable'],\n",
    "                                     escape=False,\n",
    "                                     float_format=int_or_float_format_func)))\n",
    "\n",
    "        writer = DataWriter(summary_id)\n",
    "        writer.write_experiment_output(output_dir,\n",
    "                                       {file_suffix: df_summ},\n",
    "                                       index=True,\n",
    "                                       file_format=file_format_summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal and partial correlations\n",
    "\n",
    "The tables below shows correlations between truncated and standardized (if applicable) values of each feature against human score for each model. All correlations are computed on the training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = Markdown(\"####Marginal corelations against score\\n\\n\\n \"\n",
    "                  \"The table shows marginal correlations between each feature \"\n",
    "                  \"and the human score.\")\n",
    "\n",
    "summarize_feature_correlations(model_list, 'margcor_score_all_data', header, file_format_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = Markdown(\"####Partial correlations after controlling for all other variables\\n\\n\\n \"\n",
    "                  \"This table shows Pearson's correlation between each feature and human score after \"\n",
    "                  \"controlling for all other features\")\n",
    "\n",
    "summarize_feature_correlations(model_list, 'pcor_score_all_data', header, file_format_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = Markdown(\"####Partial correlations after controlling for length\\n\\n\\n \"\n",
    "                  \"This table shows Pearson's correlation between each feature and human score after \"\n",
    "                  \"controlling for length\")\n",
    "\n",
    "summarize_feature_correlations(model_list, 'pcor_score_no_length_all_data', header, file_format_summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows main model parameters for each experiment: the total number of features used in the model (linear models only), the number of features with negative coefficients (linear models only), the learner, and the label used to train the model.  For linear models, the second table shows standardized coefficients for all features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_models(model_list, file_format_summarize):\n",
    "    \n",
    "    writer = DataWriter(summary_id)\n",
    "    \n",
    "    summs = []\n",
    "    betas = []\n",
    "    for (model_id, model_name, config, csvdir, file_format) in model_list:\n",
    "        coef_file = join(csvdir, '{}_betas.{}'.format(model_id, file_format))\n",
    "        if exists(coef_file):\n",
    "            df_coefs = DataReader.read_from_file(coef_file)\n",
    "            model_summary = pd.DataFrame({'N features': [len(df_coefs)],\n",
    "                                          'N negative': len(df_coefs[df_coefs['standardized'] < 0]),\n",
    "                                          'learner': config['model'],\n",
    "                                          'train_label': config['train_label_column']},\n",
    "                                         index=[model_name])\n",
    "            summs.append(model_summary)\n",
    "            df_betas = pd.DataFrame({model_name : df_coefs['standardized'].values},\n",
    "                                     index = df_coefs['feature'].values)\n",
    "            betas.append(df_betas)\n",
    "        else:\n",
    "            if 'model' in config:\n",
    "                model_summary = pd.DataFrame({'N features': '-',\n",
    "                                              'N negative': '-',\n",
    "                                              'learner': config['model'],\n",
    "                                              'train_label': config['train_label_column']},\n",
    "                                            index=[model_name])\n",
    "                summs.append(model_summary)\n",
    "   \n",
    "    if not len(summs) == 0:\n",
    "        df_summ = pd.concat(summs, sort=True)\n",
    "        display(Markdown(\"## Model summary\"))\n",
    "        display(HTML(df_summ[['N features', 'N negative',\n",
    "                              'learner', 'train_label']].to_html(index=True, \n",
    "                                                                 classes = ['sortable'],\n",
    "                                                                 escape=False,\n",
    "                                                                 float_format=int_or_float_format_func)))\n",
    "\n",
    "        writer.write_experiment_output(output_dir,\n",
    "                                       {'model_summary': df_summ},\n",
    "                                       index=True,\n",
    "                                       file_format=file_format_summarize)\n",
    "        \n",
    "    if not len(betas) == 0:\n",
    "        df_betas_all = pd.concat(betas, axis=1, sort=True)\n",
    "        df_betas_all.fillna('-', inplace=True)\n",
    "        display(Markdown(\"## Standardized coefficients\"))\n",
    "        display(HTML(df_betas_all.to_html(index=True, \n",
    "                                          classes = ['sortable'],\n",
    "                                          escape=False,\n",
    "                                          float_format=int_or_float_format_func)))\n",
    "\n",
    "        writer.write_experiment_output(output_dir,\n",
    "                                       {'betas': df_betas_all},\n",
    "                                       index=True,\n",
    "                                       file_format=file_format_summarize)\n",
    "\n",
    "summarize_models(model_list, file_format_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize_model_fit(file_format_summarize):\n",
    "    fits = []\n",
    "    for (model_id, model_name, config, csvdir, file_format) in model_list:\n",
    "        model_fit_file = join(csvdir, '{}_model_fit.{}'.format(model_id, file_format))\n",
    "        if exists(model_fit_file):\n",
    "            fit = DataReader.read_from_file(model_fit_file)\n",
    "            fit.index = [model_name]\n",
    "            fits.append(fit)\n",
    "    if len(fits)>0:\n",
    "        df_fit = pd.concat(fits, sort=True)\n",
    "        display(Markdown(\"## Model fit\"))\n",
    "        display(HTML(df_fit[['N responses', 'N features',\n",
    "                             'R2','R2_adjusted']].to_html(index=True,\n",
    "                                                          classes=['sortable'],\n",
    "                                                          escape=False,\n",
    "                                                          float_format=int_or_float_format_func)))\n",
    "    \n",
    "        writer = DataWriter(summary_id)\n",
    "        writer.write_experiment_output(output_dir,\n",
    "                                       {'model_fit': df_fit},\n",
    "                                       index=True,\n",
    "                                       file_format=file_format_summarize)\n",
    "\n",
    "    \n",
    "summarize_model_fit(file_format_summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall association statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables in this section show the standard association metrics between human scores and different types of machine scores. These results are computed on the evaluation set. The scores for each model have been truncated to values indicated in `truncation range`. When indicated, scaled scores are computed by re-scaling the predicted scores using mean and standard deviation of human scores as observed on the training data and mean and standard deviation of machine scores as predicted for the training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_evals(model_list, file_format_summarize):\n",
    "\n",
    "    has_missing_trims = False\n",
    "\n",
    "    evals = []\n",
    "    for (model_id, model_name, config, csvdir, file_format) in model_list:\n",
    "        csv_file = os.path.join(csvdir, '{}_eval_short.{}'.format(model_id, file_format))\n",
    "        if os.path.exists(csv_file):\n",
    "            df_eval = DataReader.read_from_file(csv_file, index_col=0)\n",
    "            df_eval.index = [model_name]\n",
    "            \n",
    "            # figure out whether the score was scaled\n",
    "            df_eval['system score type'] = 'scale' if config.get('use_scaled_predictions') == True or config.get('scale_with') is not None else 'raw'        \n",
    "\n",
    "            # we want to display the truncation range, but this is slightly complicated\n",
    "            # we first check to see if the post-processing params file exists; if it does,\n",
    "            # we grab the trim_min and trim_max values from that file (which still could be None!)            \n",
    "            trim_min, trim_max = None, None\n",
    "            postproc_file = os.path.join(csvdir, '{}_postprocessing_params.{}'.format(model_id, file_format))\n",
    "            if os.path.exists(postproc_file):\n",
    "                df_postproc = DataReader.read_from_file(postproc_file)\n",
    "                trim_min = df_postproc['trim_min'].values[0]\n",
    "                trim_max = df_postproc['trim_max'].values[0] \n",
    "    \n",
    "            # if the trim_min or trim_max is still None, we then grab whatever is in the config\n",
    "            trim_min = config.get('trim_min') if trim_min is None else trim_min\n",
    "            trim_max = config.get('trim_max') if trim_max is None else trim_max\n",
    "            \n",
    "            # finally, we calculate the max and min scores; if we couldn't get any trim values,\n",
    "            # then we default these to `?` and the set `has_missing_trims=True`\n",
    "            if trim_min is None:\n",
    "                min_score, has_missing_trims = '?', True\n",
    "            else:\n",
    "                min_score = float(trim_min) - config.get('trim_tolerance', 0.4998)\n",
    "            if trim_max is None:\n",
    "                max_score, has_missing_trims = '?', True\n",
    "            else:\n",
    "                max_score = float(trim_max) + config.get('trim_tolerance', 0.4998)        \n",
    "\n",
    "            df_eval['truncation range'] = \"[{}, {}]\".format(min_score, max_score)\n",
    "            \n",
    "            # rename the columns to remove reference to scale/raw scores\n",
    "            new_column_names = [col.split('.')[0] if not 'round' in col \n",
    "                                else '{} (rounded)'.format(col.split('.')[0])\n",
    "                                for col in df_eval.columns ]\n",
    "            df_eval.columns = new_column_names\n",
    "            evals.append(df_eval)\n",
    "\n",
    "    if len(evals) > 0:\n",
    "        df_evals = pd.concat(evals, sort=True)\n",
    "    else:\n",
    "        df_evals = pd.DataFrame()\n",
    "    return df_evals, has_missing_trims\n",
    "\n",
    "df_eval, has_missing_trims = read_evals(model_list, file_format_summarize)\n",
    "\n",
    "if has_missing_trims:\n",
    "    display(Markdown('**Note:** The minimum and/or maximum scores after truncation could not be '\n",
    "                     'be computed in some cases. This is because `trim_min` and/or `trim_max` '\n",
    "                     'could not be found in either the configuration file or the postprocessing '\n",
    "                     'parameters file. Scores that could not be computed are shown as `?`.'))\n",
    "if not df_eval.empty:\n",
    "    writer = DataWriter(summary_id)\n",
    "    writer.write_experiment_output(output_dir,\n",
    "                                   {'eval_short': df_eval},\n",
    "                                   index=True,\n",
    "                                   file_format=file_format_summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive holistic score statistics\n",
    "\n",
    "The table shows distributional properties of human and system scores. SMD values lower then -0.15 or higher than 0.15 are <span class=\"highlight_color\">highlighted</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.width=10\n",
    "formatter = partial(color_highlighter, low=-0.15, high=0.15)\n",
    "if not df_eval.empty:\n",
    "     display(HTML(df_eval[['N', 'system score type', \"truncation range\", 'h_mean', 'h_sd', \n",
    "                           'sys_mean', 'sys_sd',  'SMD']].to_html(index=True,\n",
    "                                                                  classes=['sortable'],\n",
    "                                                                  escape=False,\n",
    "                                                                  formatters={'SMD': formatter},\n",
    "                                                                  float_format=int_or_float_format_func)))\n",
    "else:\n",
    "     display(Markdown(\"No information available for any of the models\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Association statistics\n",
    "\n",
    "The table shows the standard association metrics between human scores and machine scores. Note that some evaluations (`*_trim_round`) are based on rounded scores computed by first truncating and then rounding the predicted score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_eval.empty:\n",
    "    wtkappa_col = 'wtkappa' if 'wtkappa' in df_eval else 'wtkappa (rounded)'\n",
    "    display(HTML(df_eval[['N',\n",
    "                          'system score type',\n",
    "                          'corr', 'R2', 'RMSE',\n",
    "                          wtkappa_col,\n",
    "                          'kappa (rounded)',\n",
    "                          'exact_agr (rounded)',\n",
    "                          'adj_agr (rounded)']].to_html(index=True,\n",
    "                                                        classes=['sortable'],\n",
    "                                                        escape=False,\n",
    "                                                        float_format=int_or_float_format_func)))\n",
    "else:\n",
    "    display(Markdown(\"No information available for any of the models\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True score evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables in this section shows how well system scores can predict *true* scores. According to Test theory, a *true* score is a score that would have been obtained if there were no errors in measurement. While true scores cannot be observed, the variance of true scores and the prediction error can be estimated using observed human scores when multiple human ratings are available for a subset of responses. In this notebook, this variance and prediction error are estimated using human scores for responses in the evaluation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prmse_columns = ['N','N raters', 'N single', 'N multiple', \n",
    "                 'Variance of errors', 'True score var',\n",
    "                 'MSE true', 'PRMSE true']\n",
    "\n",
    "def read_true_score_evals(model_list, file_format_summarize):\n",
    "    true_score_evals = []\n",
    "    for (model_id, model_name, config, csvdir, file_format) in model_list:\n",
    "        csv_file = os.path.join(csvdir, '{}_true_score_eval.{}'.format(model_id, file_format))\n",
    "        if os.path.exists(csv_file):\n",
    "            df_true_score_eval_all = DataReader.read_from_file(csv_file, index_col=0)\n",
    "            # figure out whether the score was scaled\n",
    "            prefix = 'scale' if config.get('use_scaled_predictions') == True or config.get('scale_with') is not None else 'raw'        \n",
    "            # use the line that corresponds to the appropriate score (scaled or raw)\n",
    "            df_true_score_eval = df_true_score_eval_all.loc[['{}_trim'.format(prefix)]].copy()\n",
    "            df_true_score_eval['system score type'] = prefix\n",
    "            df_true_score_eval.index = [model_name]\n",
    "            true_score_evals.append(df_true_score_eval)          \n",
    "    if len(true_score_evals) > 0:\n",
    "        df_true_score_evals = pd.concat(true_score_evals, sort=True)\n",
    "    else:\n",
    "        df_true_score_evals = pd.DataFrame()\n",
    "    return(df_true_score_evals)\n",
    "\n",
    "df_true_score_eval = read_true_score_evals(model_list, file_format_summarize)\n",
    "if not df_true_score_eval.empty:\n",
    "    writer = DataWriter(summary_id)\n",
    "    writer.write_experiment_output(output_dir,\n",
    "                                   {'true_score_eval': df_true_score_eval},\n",
    "                                   index=True,\n",
    "                                   file_format=file_format_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_true_score_eval.empty:\n",
    "    markdown_strs = [\"#### Proportional reduction in mean squared error (PRMSE)\"]\n",
    "    markdown_strs.append(\"The table shows variance of human rater errors, \"\n",
    "                         \"true score variance, mean squared error (MSE) and \"\n",
    "                         \"proportional reduction in mean squared error (PRMSE) for \"\n",
    "                         \"predicting a true score with system score.\")\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    pd.options.display.width=10\n",
    "    df_prmse = df_true_score_eval[prmse_columns].copy()\n",
    "    df_prmse.replace({np.nan: '-'}, inplace=True)\n",
    "    display(HTML('<span style=\"font-size:95%\">'+ df_prmse.to_html(classes=['sortable'], \n",
    "                                                                  escape=False,\n",
    "                                                                  float_format=float_format_func) + '</span>'))\n",
    "else:\n",
    "    display(Markdown(\"No information available for any of the models\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links to intermediate files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the hyperlinks below to see the intermediate experiment files generated as part of this summary. \n",
    "\n",
    "**Note**: This only includes the intermediate files generated by `rsmsummarize`. It does not include links to the intermediate files generated by the original experiment(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rsmtool.utils.notebook import show_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_files(output_dir, summary_id, file_format_summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "system_name = platform.system()\n",
    "\n",
    "# People might not know what 'Darwin' is, so we should replace that with 'Mac OS X'\n",
    "if system_name == 'Darwin':\n",
    "    system_name = 'Mac OS X'\n",
    "    \n",
    "# get the architecture\n",
    "architecture = platform.architecture()[0]\n",
    "\n",
    "# get the rsmtool version\n",
    "rsmtool_version_str = '.'.join(map(str, rsmtool_version))\n",
    "\n",
    "display(Markdown('This report was generated using rsmtool v{} on a '\n",
    "                 '{} computer running {}.'.format(rsmtool_version_str, \n",
    "                                                  architecture, \n",
    "                                                  system_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "package_names = '\\n'.join(sorted([\"%s==%s\" % (i.key, i.version) for i in pkg_resources.working_set]))\n",
    "display(HTML('<div id=\"packages\"><pre>{}</pre></div>'.format(package_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "// Code to dynamically generate table of contents at the top of the HTML file\n",
    "var tocEntries = ['<ul>'];\n",
    "var anchors = $('a.anchor-link');\n",
    "var headingTypes = $(anchors).parent().map(function() { return $(this).prop('tagName')});\n",
    "var headingTexts = $(anchors).parent().map(function() { return $(this).text()});\n",
    "var subList = false;\n",
    "\n",
    "$.each(anchors, function(i, anch) {\n",
    "    var hType = headingTypes[i];\n",
    "    var hText = headingTexts[i];\n",
    "    hText = hText.substr(0, hText.length - 1);\n",
    "    if (hType == 'H2') {\n",
    "        if (subList) {\n",
    "            tocEntries.push('</ul>')\n",
    "            subList = false;\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "    else if (hType == 'H3') {\n",
    "        if (!subList) {\n",
    "            subList = true;\n",
    "            tocEntries.push('<ul>')\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "});\n",
    "tocEntries.push('</ul>')\n",
    "$('#toc').html(tocEntries.join(' '))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}