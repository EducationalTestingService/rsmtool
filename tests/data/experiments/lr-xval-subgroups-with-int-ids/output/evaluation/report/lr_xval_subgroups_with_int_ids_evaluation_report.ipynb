{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting options for the plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats={'retina', 'svg'}\n",
    "%config InlineBackend.rc={'savefig.dpi': 150}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import platform\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from functools import partial\n",
    "from os.path import abspath, relpath, exists, join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "from textwrap import wrap\n",
    "\n",
    "# allow older versions of pandas to work\n",
    "try:\n",
    "    from pandas.io.common import DtypeWarning\n",
    "except ImportError:\n",
    "    from pandas.errors import DtypeWarning\n",
    "\n",
    "from IPython import sys_info\n",
    "from IPython.display import display, HTML, Image, Javascript, Markdown, SVG\n",
    "from rsmtool.reader import DataReader\n",
    "from rsmtool.writer import DataWriter\n",
    "from rsmtool.utils.files import parse_json_with_comments\n",
    "from rsmtool.utils.notebook import (float_format_func,\n",
    "                                    int_or_float_format_func,\n",
    "                                    compute_subgroup_plot_params,\n",
    "                                    bold_highlighter,\n",
    "                                    color_highlighter,\n",
    "                                    show_thumbnail)\n",
    "\n",
    "from rsmtool.fairness_utils import (get_fairness_analyses,\n",
    "                                    write_fairness_results)\n",
    "\n",
    "from rsmtool.version import VERSION as rsmtool_version\n",
    "\n",
    "# turn off interactive plotting\n",
    "plt.ioff()\n",
    "\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_report_dir = os.environ.get('RSM_REPORT_DIR', None)\n",
    "if rsm_report_dir is None:\n",
    "    rsm_report_dir = os.getcwd()\n",
    "\n",
    "rsm_environ_config = join(rsm_report_dir, '.environ.json')\n",
    "if not exists(rsm_environ_config):\n",
    "    raise FileNotFoundError('The file {} cannot be located. '\n",
    "                            'Please make sure that either (1) '\n",
    "                            'you have set the correct directory with the `RSM_REPORT_DIR` '\n",
    "                            'environment variable, or (2) that your `.environ.json` '\n",
    "                            'file is in the same directory as your notebook.'.format(rsm_environ_config))\n",
    "    \n",
    "environ_config = parse_json_with_comments(rsm_environ_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "  div.prompt.output_prompt { \n",
    "    color: white; \n",
    "  }\n",
    "  \n",
    "  span.highlight_color {\n",
    "    color: red;\n",
    "  }\n",
    "  \n",
    "  span.highlight_bold {\n",
    "    font-weight: bold;  \n",
    "  }\n",
    "    \n",
    "  @media print {\n",
    "    @page {\n",
    "      size: landscape;\n",
    "      margin: 0cm 0cm 0cm 0cm;\n",
    "    }\n",
    "\n",
    "    * {\n",
    "      margin: 0px;\n",
    "      padding: 0px;\n",
    "    }\n",
    "\n",
    "    #toc {\n",
    "      display: none;\n",
    "    }\n",
    "\n",
    "    span.highlight_color, span.highlight_bold {\n",
    "        font-weight: bolder;\n",
    "        text-decoration: underline;\n",
    "    }\n",
    "\n",
    "    div.prompt.output_prompt {\n",
    "      display: none;\n",
    "    }\n",
    "    \n",
    "    h3#Python-packages, div#packages {\n",
    "      display: none;\n",
    "  }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you will need to set the following manually\n",
    "# if you are using this notebook interactively.\n",
    "experiment_id = environ_config.get('EXPERIMENT_ID')\n",
    "description = environ_config.get('DESCRIPTION')\n",
    "context = environ_config.get('CONTEXT')\n",
    "train_file_location = environ_config.get('TRAIN_FILE_LOCATION')\n",
    "test_file_location = environ_config.get('TEST_FILE_LOCATION')\n",
    "output_dir = environ_config.get('OUTPUT_DIR')\n",
    "figure_dir = environ_config.get('FIGURE_DIR')\n",
    "model_name = environ_config.get('MODEL_NAME')\n",
    "model_type = environ_config.get('MODEL_TYPE')\n",
    "skll_fixed_parameters = environ_config.get('SKLL_FIXED_PARAMETERS')\n",
    "skll_objective = environ_config.get('SKLL_OBJECTIVE')\n",
    "file_format = environ_config.get('FILE_FORMAT')\n",
    "length_column = environ_config.get('LENGTH_COLUMN')\n",
    "second_human_score_column = environ_config.get('H2_COLUMN')\n",
    "use_scaled_predictions = environ_config.get('SCALED')\n",
    "min_score = environ_config.get(\"MIN_SCORE\")\n",
    "max_score = environ_config.get(\"MAX_SCORE\")\n",
    "standardize_features = environ_config.get('STANDARDIZE_FEATURES')\n",
    "exclude_zero_scores = environ_config.get('EXCLUDE_ZEROS')\n",
    "feature_subset_file = environ_config.get('FEATURE_SUBSET_FILE', ' ')\n",
    "min_items = environ_config.get('MIN_ITEMS')\n",
    "use_thumbnails = environ_config.get('USE_THUMBNAILS')\n",
    "predict_expected_scores = environ_config.get('PREDICT_EXPECTED_SCORES')\n",
    "rater_error_variance = environ_config.get(\"RATER_ERROR_VARIANCE\")\n",
    "\n",
    "# groups for analysis by prompt or subgroup.\n",
    "groups_desc = environ_config.get('GROUPS_FOR_DESCRIPTIVES') \n",
    "groups_eval = environ_config.get('GROUPS_FOR_EVALUATIONS') \n",
    "\n",
    "# min number of n for group to be displayed in the report\n",
    "min_n_per_group = environ_config.get('MIN_N_PER_GROUP')\n",
    "\n",
    "if min_n_per_group is None:\n",
    "    min_n_per_group = {}\n",
    "\n",
    "# javascript path\n",
    "javascript_path = environ_config.get(\"JAVASCRIPT_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize counter for thumbnail IDs\n",
    "id_generator = itertools.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(javascript_path, \"sort.js\"), \"r\", encoding=\"utf-8\") as sortf:\n",
    "    display(Javascript(data=sortf.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown('''This report presents the analysis for **{}**: {}'''.format(experiment_id, description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = ''\n",
    "if use_thumbnails:\n",
    "    markdown_str += (\"\"\"\\n  - Images in this report have been converted to \"\"\"\n",
    "                     \"\"\"clickable thumbnails.\"\"\")\n",
    "if predict_expected_scores:\n",
    "    markdown_str += (\"\"\"\\n  - Predictions analyzed in this report are *expected scores*, \"\"\"\n",
    "                     \"\"\"i.e., probability-weighted averages over all score points.\"\"\")\n",
    "\n",
    "if markdown_str:\n",
    "    markdown_str = '**Notes**:' + markdown_str\n",
    "    display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(time.strftime('%c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training and testing features, both raw and pre-processed\n",
    "# Make sure that the `spkitemid` and `candidate` columns are read as strings \n",
    "# to preserve any leading zeros\n",
    "# We filter DtypeWarnings that pop up mostly in very large files\n",
    "\n",
    "string_columns = ['spkitemid', 'candidate']\n",
    "converter_dict = {column: str for column in string_columns}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore', category=DtypeWarning)\n",
    "    if exists(train_file_location):\n",
    "        df_train_orig = DataReader.read_from_file(train_file_location)\n",
    "\n",
    "    train_file = join(output_dir, '{}_train_features.{}'.format(experiment_id,\n",
    "                                                                file_format))\n",
    "    if exists(train_file):\n",
    "        df_train = DataReader.read_from_file(train_file, converters=converter_dict)\n",
    "\n",
    "    train_metadata_file = join(output_dir, '{}_train_metadata.{}'.format(experiment_id,\n",
    "                                                                         file_format))    \n",
    "    if exists(train_metadata_file):\n",
    "        df_train_metadata = DataReader.read_from_file(train_metadata_file, converters=converter_dict)\n",
    "\n",
    "    train_other_columns_file = join(output_dir, '{}_train_other_columns.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(train_other_columns_file):\n",
    "        df_train_other_columns = DataReader.read_from_file(train_other_columns_file, converters=converter_dict)\n",
    "\n",
    "    train_length_file = join(output_dir, '{}_train_response_lengths.{}'.format(experiment_id,\n",
    "                                                                               file_format))\n",
    "    if exists(train_length_file):\n",
    "        df_train_length = DataReader.read_from_file(train_length_file, converters=converter_dict)\n",
    "\n",
    "    train_excluded_file = join(output_dir, '{}_train_excluded_responses.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(train_excluded_file):\n",
    "        df_train_excluded = DataReader.read_from_file(train_excluded_file, converters=converter_dict)\n",
    "\n",
    "    train_responses_with_excluded_flags_file = join(output_dir, '{}_train_responses_with_excluded_flags.{}'.format(experiment_id,\n",
    "                                                                                                                   file_format))\n",
    "    if exists(train_responses_with_excluded_flags_file):\n",
    "        df_train_responses_with_excluded_flags = DataReader.read_from_file(train_responses_with_excluded_flags_file,\n",
    "                                                                           converters=converter_dict)\n",
    "\n",
    "    train_preproc_file = join(output_dir, '{}_train_preprocessed_features.{}'.format(experiment_id,\n",
    "                                                                                     file_format))    \n",
    "    if exists(train_preproc_file):\n",
    "        df_train_preproc = DataReader.read_from_file(train_preproc_file, converters=converter_dict)\n",
    "\n",
    "    if exists(test_file_location):\n",
    "        df_test_orig = DataReader.read_from_file(test_file_location)\n",
    "\n",
    "    test_file = join(output_dir, '{}_test_features.{}'.format(experiment_id,\n",
    "                                                              file_format))\n",
    "    if exists(test_file):\n",
    "        df_test = DataReader.read_from_file(test_file, converters=converter_dict)\n",
    "\n",
    "    test_metadata_file = join(output_dir, '{}_test_metadata.{}'.format(experiment_id,\n",
    "                                                                       file_format))    \n",
    "    if exists(test_metadata_file):\n",
    "        df_test_metadata = DataReader.read_from_file(test_metadata_file, converters=converter_dict)\n",
    "\n",
    "    test_other_columns_file = join(output_dir, '{}_test_other_columns.{}'.format(experiment_id,\n",
    "                                                                                 file_format))\n",
    "    if exists(test_other_columns_file):\n",
    "        df_test_other_columns = DataReader.read_from_file(test_other_columns_file, converters=converter_dict)\n",
    "\n",
    "    test_human_scores_file = join(output_dir, '{}_test_human_scores.{}'.format(experiment_id,\n",
    "                                                                               file_format))\n",
    "    if exists(test_human_scores_file):\n",
    "        df_test_human_scores = DataReader.read_from_file(test_human_scores_file, converters=converter_dict)\n",
    "\n",
    "    test_excluded_file = join(output_dir, '{}_test_excluded_responses.{}'.format(experiment_id,\n",
    "                                                                                 file_format))\n",
    "    if exists(test_excluded_file):\n",
    "        df_test_excluded = DataReader.read_from_file(test_excluded_file, converters=converter_dict)\n",
    "\n",
    "    test_responses_with_excluded_flags_file = join(output_dir, '{}_test_responses_with_excluded_flags.{}'.format(experiment_id,\n",
    "                                                                                                                 file_format))\n",
    "    if exists(test_responses_with_excluded_flags_file):\n",
    "        df_test_responses_with_excluded_flags = DataReader.read_from_file(test_responses_with_excluded_flags_file,\n",
    "                                                                          converters=converter_dict)\n",
    "\n",
    "    test_preproc_file = join(output_dir, '{}_test_preprocessed_features.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(test_preproc_file):\n",
    "        df_test_preproc = DataReader.read_from_file(test_preproc_file, converters=converter_dict)\n",
    "\n",
    "    pred_preproc_file = join(output_dir, '{}_pred_processed.{}'.format(experiment_id,\n",
    "                                                                       file_format))\n",
    "    if exists(pred_preproc_file):\n",
    "        df_pred_preproc = DataReader.read_from_file(pred_preproc_file, converters=converter_dict)\n",
    "\n",
    "    feature_file = join(output_dir, '{}_feature.{}'.format(experiment_id,\n",
    "                                                           file_format))\n",
    "    if exists(feature_file):\n",
    "        df_features = DataReader.read_from_file(feature_file, converters=converter_dict)\n",
    "        features_used = [c for c in df_features.feature.values]\n",
    "        # compute the longest feature name: we'll need if for the plots\n",
    "        longest_feature_name = max(map(len, features_used))\n",
    "\n",
    "    betas_file = join(output_dir, '{}_betas.{}'.format(experiment_id,\n",
    "                                                       file_format))\n",
    "    if exists(betas_file):\n",
    "        df_betas = DataReader.read_from_file(betas_file)\n",
    "\n",
    "    if exists(feature_subset_file):\n",
    "        df_feature_subset_specs = DataReader.read_from_file(feature_subset_file)\n",
    "    else:\n",
    "        df_feature_subset_specs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for continuous human scores in the evaluation set\n",
    "continuous_human_score = False\n",
    "\n",
    "if exists(pred_preproc_file):\n",
    "    if not df_pred_preproc['sc1'].equals(np.round(df_pred_preproc['sc1'])):\n",
    "        continuous_human_score = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    num_excluded_train = len(df_train_responses_with_excluded_flags)\n",
    "except NameError:\n",
    "    num_excluded_train = 0\n",
    "\n",
    "try:\n",
    "    num_excluded_test = len(df_test_responses_with_excluded_flags)\n",
    "except NameError:\n",
    "    num_excluded_test = 0\n",
    "\n",
    "if context == 'rsmtool':\n",
    "    pct_excluded_train = round(100*num_excluded_train/len(df_train_orig), 2)\n",
    "pct_excluded_test = round(100*num_excluded_test/len(df_test_orig), 2)\n",
    "\n",
    "if (num_excluded_train != 0 or num_excluded_test != 0):\n",
    "    display(Markdown(\"### Responses excluded due to flags\"))\n",
    "\n",
    "    display(Markdown(\"Total number of responses excluded due to flags:\"))\n",
    "    if context=='rsmtool':\n",
    "        display(Markdown(\"Training set: {} responses ({:.1f}% of the original {} responses)\".format(num_excluded_train, pct_excluded_train, len(df_train_orig))))\n",
    "    display(Markdown(\"Evaluation set: {} responses ({:.1f}% of the original {} responses)\".format(num_excluded_test, pct_excluded_test, len(df_test_orig))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses excluded due to non-numeric feature values or scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    num_missing_rows_train = len(df_train_excluded)\n",
    "except NameError:\n",
    "    num_missing_rows_train = 0\n",
    "\n",
    "if context == 'rsmtool':\n",
    "    pct_missing_rows_train = 100*num_missing_rows_train/len(df_train_orig)\n",
    "\n",
    "try:\n",
    "    num_missing_rows_test = len(df_test_excluded)\n",
    "except:\n",
    "    num_missing_rows_test = 0\n",
    "pct_missing_rows_test = 100*num_missing_rows_test/len(df_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excluded_candidates_note = Markdown(\"Note: if a candidate had less than {} responses left for analysis after applying all filters, \"\n",
    "                                    \"all responses from that \"\n",
    "                                    \"candidate were excluded from further analysis.\".format(min_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if context == 'rsmtool':\n",
    "    display(Markdown(\"**Training set**\"))\n",
    "    display(Markdown('Total number of excluded responses: {} ({:.1f}% of the original {})'.format(num_missing_rows_train, pct_missing_rows_train, len(df_train_orig))))\n",
    "    if num_missing_rows_train != 0:\n",
    "        train_excluded_analysis_file = join(output_dir, '{}_train_excluded_composition.{}'.format(experiment_id,\n",
    "                                                                                                  file_format))\n",
    "        df_train_excluded_analysis = DataReader.read_from_file(train_excluded_analysis_file)\n",
    "        display(HTML(df_train_excluded_analysis.to_html(classes=['sortable'], float_format=float_format_func, index=False))) \n",
    "        if min_items > 0:\n",
    "            display(excluded_candidates_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('**Evaluation set**'))\n",
    "display(Markdown('Total number of excluded responses: {} ({:.1f}% of the original {})'.format(num_missing_rows_test, pct_missing_rows_test, len(df_test_orig))))\n",
    "if num_missing_rows_test != 0:\n",
    "    test_excluded_analysis_file = join(output_dir, '{}_test_excluded_composition.{}'.format(experiment_id,\n",
    "                                                                                            file_format))\n",
    "    df_test_excluded_analysis = DataReader.read_from_file(test_excluded_analysis_file)\n",
    "    display(HTML(df_test_excluded_analysis.to_html(classes=['sortable'], float_format=float_format_func, index=False)))\n",
    "    if min_items > 0:\n",
    "        display(excluded_candidates_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of this report is based only on the responses that were not excluded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if context == 'rsmtool':\n",
    "    display(Markdown('### Composition of the training and evaluation sets'))\n",
    "elif context == 'rsmeval':\n",
    "    display(Markdown('### Composition of the evaluation set'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the table showing candidate (speaker), prompt \n",
    "# and responses stats for training and test\n",
    "\n",
    "# feature descriptives extra table\n",
    "data_composition_file = join(output_dir, '{}_data_composition.{}'.format(experiment_id, file_format))\n",
    "df_data_desc = DataReader.read_from_file(data_composition_file)\n",
    "display(HTML(df_data_desc.to_html(classes=['sortable'], float_format=float_format_func, index=False)))\n",
    "\n",
    "try:\n",
    "    num_double_scored_responses = len(df_test_human_scores[df_test_human_scores['sc2'].notnull()])\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    zeros_included_or_excluded = 'excluded' if exclude_zero_scores else 'included'\n",
    "    display(Markdown(\"Total number of double scored responses in the evaluation set\" \n",
    "                     \" used: {} (zeros {})\".format(num_double_scored_responses,\n",
    "                                                   zeros_included_or_excluded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This notebook displays data composition tables for all groups specified in groups_desc variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in groups_desc:\n",
    "    display(Markdown('### Number of responses by {}'.format(group)))\n",
    "    data_composition_by_file = join(output_dir, '{}_data_composition_by_{}.{}'.format(experiment_id,\n",
    "                                                                                      group,\n",
    "                                                                                      file_format))\n",
    "    df_data_composition_by_group = DataReader.read_from_file(data_composition_by_file)\n",
    "    display(HTML(df_data_composition_by_group.to_html(classes=['sortable'], float_format=float_format_func, index=False)))\n",
    "    if group in min_n_per_group:\n",
    "        display(Markdown(\"Note: the rest of this report will only display the results for groups with \"\n",
    "                         \"at least {} responses in the partition used for each set of analyses.\".format(min_n_per_group[group])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_strs = ['## Consistency']\n",
    "\n",
    "consistency_file = join(output_dir, '{}_consistency.{}'.format(experiment_id, file_format))\n",
    "degradation_file = join(output_dir, '{}_degradation.{}'.format(experiment_id, file_format))\n",
    "disattenuation_file = join(output_dir, '{}_disattenuated_correlations.{}'.format(experiment_id, file_format))\n",
    "eval_file = join(output_dir, '{}_eval.{}'.format(experiment_id,\n",
    "                                                 file_format))\n",
    "\n",
    "if exists(consistency_file) and exists(degradation_file) and exists(disattenuation_file):\n",
    "    df_consistency = DataReader.read_from_file(consistency_file, index_col=0)\n",
    "    df_degradation = DataReader.read_from_file(degradation_file, index_col=0)\n",
    "    df_dis_corrs = DataReader.read_from_file(disattenuation_file, index_col=0)\n",
    "    df_eval = DataReader.read_from_file(eval_file, index_col=0)\n",
    "\n",
    "    markdown_strs.append('*Note: this section assumes that the score used for evaluating machine scores '\n",
    "                         'is the score assigned by the first rater.*')\n",
    "    markdown_strs.append('### Human-human agreement')\n",
    "    markdown_strs.append(\"This table shows the human-human agreement on the \"\n",
    "                         \"double-scored evaluation data.\")\n",
    "    if continuous_human_score:\n",
    "        markdown_strs.append('For the computation of `kappa` and `wtkappa` '\n",
    "                             'human scores have beeen rounded to the nearest integer.')\n",
    "        \n",
    "    markdown_strs.append(\"The following are <span class='highlight_color'>highlighted </span>: \")\n",
    "    markdown_strs.append(' - Exact agreement (`exact_agr`) < 50%')\n",
    "    markdown_strs.append(' - Adjacent agreement (`adj_agr`) < 95%')\n",
    "    markdown_strs.append(' - Quadratic weighted kappa (`wtkappa`) < 0.7')\n",
    "    markdown_strs.append(' - Pearson correlation (`corr`) < 0.7')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    \n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_exact_agr = partial(color_highlighter, low=50, high=100)\n",
    "    formatter_adj_agr = partial(color_highlighter, low=95, high=100)\n",
    "    formatter_wtkappa_corr = partial(color_highlighter, low=0.7)\n",
    "    formatter_dict = {'exact_agr': formatter_exact_agr, \n",
    "                      'adj_agr': formatter_adj_agr,\n",
    "                      'wtkappa': formatter_wtkappa_corr, \n",
    "                      'corr': formatter_wtkappa_corr}\n",
    "    display(HTML(df_consistency.to_html(index=False,\n",
    "                                        escape=False,\n",
    "                                        float_format=float_format_func,\n",
    "                                        formatters=formatter_dict)))\n",
    "    \n",
    "    markdown_strs = ['### Degradation']\n",
    "    markdown_strs.append('The next table shows the degradation in the evaluation metrics '\n",
    "                         '(`diff`) when comparing the machine (`H-M`) to a second human (`H-H`). '\n",
    "                         'A positive degradation value indicates better human-machine performance. '\n",
    "                         'Note that the human-machine agreement is computed on the full '\n",
    "                         'dataset (to get a reliable estimate) whereas the human-human '\n",
    "                         'agreement is computed on the subset of responses that were double-scored.')\n",
    "    markdown_strs.append(\"\\nThe following degradation values are \"\n",
    "                         \"<span class='highlight_color'>highlighted</span>\")\n",
    "    markdown_strs.append(' - `corr` < -0.1')\n",
    "    markdown_strs.append(' - `wtkappa` < -0.1')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    df_eval_for_degradation = df_eval[df_degradation.columns].copy()\n",
    "    df_consistency_for_degradation = pd.concat([df_consistency]*len(df_eval), sort=True)\n",
    "    df_consistency_for_degradation = df_consistency_for_degradation[df_degradation.columns].copy()\n",
    "    df_consistency_for_degradation.index = df_eval_for_degradation.index\n",
    "\n",
    "    df_consistency_for_degradation['type'] = 'H-H'\n",
    "    df_eval_for_degradation['type'] = 'H-M'\n",
    "    df_degradation['type'] = 'diff'\n",
    "\n",
    "    df = pd.concat([df_consistency_for_degradation,\n",
    "                    df_eval_for_degradation,\n",
    "                    df_degradation], sort=True)\n",
    "    df = df[['type','corr', 'kappa', 'wtkappa', 'exact_agr', 'adj_agr', 'SMD']]\n",
    "    df = df.reset_index()\n",
    "    df = df.set_index(['index', 'type']).sort_index(level='index')\n",
    "    df.index.names = [None, None]\n",
    "    \n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_corr = partial(color_highlighter, low=-0.1, high=100)\n",
    "    formatter_wtkappa = partial(color_highlighter, low=-0.1, high=100)\n",
    "    formatter_dict = {'corr': formatter_corr, 'wtkappa': formatter_wtkappa}\n",
    "    display(HTML(df.to_html(float_format=float_format_func, \n",
    "                            formatters=formatter_dict, escape=False)))\n",
    "    \n",
    "    \n",
    "    markdown_strs = ['### Disattenuated correlations']\n",
    "    markdown_strs.append('The next table shows the correlations between human and machine scores, '\n",
    "                         'the correlations between two human scores, '  \n",
    "                         'and disattenuated correlations between human and machine scores computed as '\n",
    "                         'human-machine correlations divided by the square root of human-human '\n",
    "                         'correlation. '\n",
    "                         'Note that the human-machine correlation is computed on the full '\n",
    "                         'dataset (to get a reliable estimate) whereas the human-human '\n",
    "                         'correlation is computed on the subset of responses that were double-scored.')\n",
    "    markdown_strs.append(\"\\nThe following values are \"\n",
    "                         \"<span class='highlight_color'>highlighted</span>\")\n",
    "    markdown_strs.append(' - `disattenuated_corr` < -0.9')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_dis_corr = partial(color_highlighter, low=0.9)\n",
    "    formatter_dict = {'corr_disattenuated': formatter_dis_corr}\n",
    "    display(HTML(df_dis_corrs.to_html(index=True,\n",
    "                                      escape=False,\n",
    "                                      classes=['sortable'],\n",
    "                                      float_format=float_format_func,\n",
    "                                      formatters=formatter_dict)))\n",
    "else:  \n",
    "    markdown_strs.append(\"The configuration file did not specify \"\n",
    "                         \"`second_human_score_column` which is necessary to compute \"\n",
    "                         \"consistency metrics.\")\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall association statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = (\"The tables in this section show the standard association metrics between \"\n",
    "                \"*observed* human scores and different types of machine scores. \"\n",
    "                \"These results are computed on the evaluation set. `raw_trim` scores \"\n",
    "                \"are truncated to [{}, {}]. `raw_trim_round` scores are computed by first truncating \"\n",
    "                \"and then rounding the predicted score. Scaled scores are computed by re-scaling \"\n",
    "                \"the predicted scores using mean and standard deviation of human scores as observed \"\n",
    "                \"on the training data and mean and standard deviation of machine scores as predicted \"\n",
    "                \"for the training set.\".format(min_score, max_score))\n",
    "display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive holistic score statistics\n",
    "\n",
    "The table shows distributional properties of human and system scores. SMD values lower then -0.15 or higher than 0.15 are <span class=\"highlight_color\">highlighted</span>.\n",
    "\n",
    "*Please note that for raw scores, SMD values are likely to be affected by possible differences in scale.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_or_scaled = \"scaled\" if use_scaled_predictions else \"raw\"\n",
    "eval_file = join(output_dir, '{}_eval.{}'.format(experiment_id, file_format))\n",
    "df_eval = DataReader.read_from_file(eval_file, index_col=0)\n",
    "distribution_columns = ['N', 'h_mean', 'sys_mean', 'h_sd',  'sys_sd', 'h_min', 'sys_min', 'h_max', 'sys_max', 'SMD']\n",
    "association_columns = ['N'] + [column for column in df_eval.columns if not column in distribution_columns]\n",
    "df_distribution = df_eval[distribution_columns]\n",
    "df_association = df_eval[association_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.width=10\n",
    "formatter = partial(color_highlighter, low=-0.15, high=0.15)\n",
    "HTML('<span style=\"font-size:95%\">'+ df_distribution.to_html(classes=['sortable'], \n",
    "                                                             escape=False,\n",
    "                                                             formatters={'SMD': formatter},\n",
    "                                                             float_format=float_format_func) + '</span>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Association statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = ['The table shows the standard association metrics between human scores and machine scores.']\n",
    "if continuous_human_score:\n",
    "    markdown_str.append(\"Note that for computation of `kappa` both human and machine scores are rounded.\")\n",
    "else:\n",
    "    markdown_str.append(\"Note that for computation of `kappa` all machine scores are rounded.\")\n",
    "\n",
    "Markdown('\\n'.join(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.width=10\n",
    "HTML('<span style=\"font-size:95%\">'+ df_association.to_html(classes=['sortable'], \n",
    "                                                            escape=False,\n",
    "                                                            float_format=float_format_func) + '</span>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = [\"Confusion matrix using {}, trimmed, and rounded scores and human scores (rows=system, columns=human).\".format(raw_or_scaled)]\n",
    "\n",
    "if continuous_human_score:\n",
    "    markdown_str.append(\"Note: Human scores have beeen rounded to the nearest integer.\")\n",
    "            \n",
    "Markdown('\\n'.join(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confmat_file = join(output_dir, '{}_confMatrix.{}'.format(experiment_id, file_format))\n",
    "df_confmat = DataReader.read_from_file(confmat_file, index_col=0)\n",
    "df_confmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of human and machine scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_strs = [\"The histogram and the table below show the distibution of \"\n",
    "                 \"human scores and {}, trimmed, and rounded machine scores \"\n",
    "                 \"(as % of all responses).\".format(raw_or_scaled)]\n",
    "markdown_strs.append(\"Differences in the table between human and machine distributions \"\n",
    "                     \"larger than 5 percentage points are <span class='highlight_color'>highlighted</span>.\")\n",
    "if continuous_human_score:\n",
    "    markdown_strs.append(\"Note: Human scores have beeen rounded to the nearest integer.\")\n",
    "    \n",
    "display(Markdown('\\n'.join(markdown_strs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoredist_file = join(output_dir, '{}_score_dist.{}'.format(experiment_id, file_format))\n",
    "df_scoredist = DataReader.read_from_file(scoredist_file, index_col=0)\n",
    "df_scoredist_melted = pd.melt(df_scoredist, id_vars=['score'])\n",
    "df_scoredist_melted = df_scoredist_melted[df_scoredist_melted['variable'] != 'difference']\n",
    "\n",
    "# get the colors for the plot\n",
    "colors = sns.color_palette(\"Greys\", 2)\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "\n",
    "    # make a barplot without a legend since we will \n",
    "    # add one manually later\n",
    "    p = sns.catplot(x=\"score\", y=\"value\", hue=\"variable\", kind=\"bar\",\n",
    "                    palette=colors, data=df_scoredist_melted, \n",
    "                    height=3, aspect=2, legend=False)\n",
    "    p.set_axis_labels('score', '% of responses')\n",
    "    \n",
    "    # add a legend with the right colors\n",
    "    axis = p.axes[0][0]\n",
    "    legend = axis.legend(labels=('Human', 'Machine'), title='', frameon=True, fancybox=True)\n",
    "    legend.legendHandles[0].set_color(colors[0])\n",
    "    legend.legendHandles[1].set_color(colors[1])\n",
    "\n",
    "    imgfile = join(figure_dir, '{}_score_dist.svg'.format(experiment_id))\n",
    "    plt.savefig(imgfile)\n",
    "\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(imgfile, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatter = partial(color_highlighter, low=0, high=5, absolute=True)\n",
    "df_html = df_scoredist.to_html(classes=['sortable'], index=False, \n",
    "                               escape=False, formatters={'difference': formatter})\n",
    "display(HTML(df_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_strs = ['### True score evaluations']\n",
    "\n",
    "raw_or_scaled = \"scaled\" if use_scaled_predictions else \"raw\"\n",
    "true_eval_file = join(output_dir, '{}_true_score_eval.{}'.format(experiment_id, file_format))\n",
    "if exists(true_eval_file): \n",
    "    df_true_eval = DataReader.read_from_file(true_eval_file, index_col=0)\n",
    "    df_true_eval.replace({np.nan: '-'}, inplace=True)\n",
    "    prmse_columns = ['N','N raters', 'N single', 'N multiple', \n",
    "                     'Variance of errors', 'True score var',\n",
    "                     'MSE true', 'PRMSE true']\n",
    "    df_prmse = df_true_eval[prmse_columns]\n",
    "\n",
    "    markdown_strs.append(\"The tables in this section show how well system scores can \"\n",
    "                         \"predict *true* scores. According to Test theory, a *true* score \"\n",
    "                         \"is a score that would have been obtained if there were no errors \"\n",
    "                         \"in measurement. While true scores cannot be observed, the variance \"\n",
    "                         \"of true scores and the prediction error can be estimated using observed \"\n",
    "                         \"human scores when multiple human ratings are available for a subset of \"\n",
    "                         \"responses.\")\n",
    "\n",
    "    if rater_error_variance is None: \n",
    "        \n",
    "        rater_variance_source = 'estimated'\n",
    "        # if we estimated rater error variance from the data,\n",
    "        # we display the variance of the two human raters\n",
    "        # so that the user can verify there is no bias\n",
    "        # We get that data from existing analyses\n",
    "        df_human_variance = df_consistency[['N', 'h1_sd', 'h2_sd']].copy()\n",
    "        df_human_variance['N_double'] = df_human_variance['N']\n",
    "        df_human_variance['h1_var (double)'] = df_human_variance['h1_sd']**2\n",
    "        df_human_variance['h2_var (double)'] = df_human_variance['h2_sd']**2\n",
    "        df_human_variance['N_total'] = df_eval.iloc[0]['N']\n",
    "        df_human_variance['h1_var (single)'] = df_eval.iloc[0]['h_sd']**2\n",
    "        df_human_variance.index = ['human']\n",
    "\n",
    "\n",
    "        if context == 'rsmtool':\n",
    "            label_column = \"test_label_column\"\n",
    "        else:\n",
    "            label_column = \"human_score_column\"\n",
    "\n",
    "        markdown_strs.append(\"In this notebook the variance of true scores is estimated using \"\n",
    "                             \"the human ratings available for \"\n",
    "                             \"responses in the evaluation set. Note that the analyses in this \"\n",
    "                             \"section assume that the values \"\n",
    "                            \"in `{}` and `second_human_score_column` are independent scores \"\n",
    "                            \"from different raters or groups of raters. These analyses are \"\n",
    "                            \"not applicable to a situation where `{}` contains an average \"\n",
    "                            \"score from multiple raters\".format(label_column, label_column))\n",
    "\n",
    "        markdown_strs.append(\"#### Variance of human scores\")\n",
    "        markdown_strs.append(\"The table below shows variance of both sets of human scores \"\n",
    "                            \"for the whole evaluation set and for the subset of responses \"\n",
    "                            \"that were double-scored. Large differences in variance between \"\n",
    "                            \"the two human scores require further investigation.\")\n",
    "        display(Markdown('\\n'.join(markdown_strs)))\n",
    "        pd.options.display.width=10\n",
    "        column_order = ['N_total', 'N_double', 'h1_var (single)', 'h1_var (double)', 'h2_var (double)']\n",
    "        display(HTML('<span style=\"font-size:95%\">'+ df_human_variance[column_order].to_html(classes=['sortable'], \n",
    "                                                                                            escape=False,\n",
    "                                                                                            float_format=float_format_func) + '</span>'))\n",
    "    else:\n",
    "        markdown_strs.append(\"In this notebook the variance of true scores was \"\n",
    "                            \"estimated using the value of rater error variance \"\n",
    "                            \"supplied by the user ({})\".format(rater_error_variance))\n",
    "        display(Markdown('\\n'.join(markdown_strs)))\n",
    "        rater_variance_source = 'supplied'\n",
    "    \n",
    "    \n",
    "    markdown_strs = [\"#### Proportional reduction in mean squared error (PRMSE)\"]\n",
    "    markdown_strs.append(\"The table shows {} variance of human rater errors, \"\n",
    "                         \"true score variance, mean squared error (MSE) and \"\n",
    "                         \"proportional reduction in mean squared error (PRMSE) for \"\n",
    "                         \"predicting a true score with system score. As for other evaluations, \"\n",
    "                         \"these results are computed on the evaluation set. `raw_trim` scores \"\n",
    "                         \"are truncated to [{}, {}]. `raw_trim_round` scores are computed \"\n",
    "                         \"by first truncating and then rounding the predicted score. Scaled scores \"\n",
    "                         \"are computed by re-scaling the predicted scores using mean and standard \"\n",
    "                         \"deviation of human scores as observed on the training data and mean and \"\n",
    "                         \"standard deviation of machine scores as predicted for the training set.\".format(rater_variance_source,\n",
    "                                                                                                          min_score,\n",
    "                                                                                                          max_score))\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    pd.options.display.width=10\n",
    "    display(HTML('<span style=\"font-size:95%\">'+ df_prmse.to_html(classes=['sortable'], \n",
    "                                                               escape=False,\n",
    "                                                               float_format=float_format_func) + '</span>'))\n",
    "else:\n",
    "    markdown_strs.append(\"The configuration file did not specify \"\n",
    "                         \"`second_human_score_column` or `rater_error_variance`. \"\n",
    "                         \"At least one of these must be specified to compute \"\n",
    "                         \"evaluations against true scores.\")\n",
    "    display(Markdown('\\n'.join(markdown_strs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This notebook generates barplot with evaluation metrics for all groups specified in groups_eval variable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basic_metrics = {('wtkappa', 'trim'): [0.7],\n",
    "                 ('corr', 'trim'): [0.7],\n",
    "                 ('DSM', 'trim_round'): [0.1, -0.1],\n",
    "                 ('DSM', 'trim'): [0.1, -0.1],\n",
    "                 ('R2', 'trim'): [],\n",
    "                 ('RMSE', 'trim'): []}\n",
    "\n",
    "colprefix = 'scale' if use_scaled_predictions else 'raw'\n",
    "metrics = dict([('{}.{}_{}'.format(k[0], colprefix, k[1]), v) for k,v in basic_metrics.items()])\n",
    "num_metrics = len(metrics)\n",
    "\n",
    "for group in groups_eval:\n",
    "    display(Markdown('### Evaluation by {}'.format(group)))\n",
    "    \n",
    "    eval_group_file = join(output_dir, '{}_eval_by_{}.{}'.format(experiment_id, group, file_format))\n",
    "    df_eval_group_all = DataReader.read_from_file(eval_group_file, index_col=0)\n",
    "    \n",
    "    df_eval_group_all.index.name = group\n",
    "    df_eval_group_all.reset_index(inplace=True)\n",
    "    \n",
    "    # If we have threshold per group, apply it now. Keep \"All data\" in any case. \n",
    "    if group in min_n_per_group:\n",
    "        display(Markdown(\"The report only shows the results for groups with \"\n",
    "                         \"at least {} responses in the evaluation set.\".format(min_n_per_group[group])))\n",
    "        \n",
    "        df_eval_group = df_eval_group_all[(df_eval_group_all['N'] >= min_n_per_group[group]) |\n",
    "                                         (df_eval_group_all[group] == 'All data')].copy()\n",
    "    else:\n",
    "        df_eval_group = df_eval_group_all.copy()\n",
    "\n",
    "\n",
    "    # Define the order of the bars: put 'All data' first and 'No info' last.\n",
    "    group_levels = list(df_eval_group[group])\n",
    "    group_levels = [level for level in group_levels if level != 'All data']\n",
    "    \n",
    "    # We only want to show the report if we have anything other than All data\n",
    "    if len(group_levels) > 0:\n",
    "        \n",
    "        if 'No info' in group_levels:\n",
    "            bar_names = ['All data'] + [level for level in group_levels if level != 'No info'] + ['No info']\n",
    "        else:\n",
    "            bar_names = ['All data'] + group_levels\n",
    "\n",
    "        fig = plt.figure()\n",
    "        (figure_width, \n",
    "         figure_height, \n",
    "         num_rows, \n",
    "         num_columns, \n",
    "         wrapped_bar_names) = compute_subgroup_plot_params(bar_names, num_metrics)\n",
    "\n",
    "        fig.set_size_inches(figure_width, figure_height)\n",
    "        with sns.axes_style('white'), sns.plotting_context('notebook', font_scale=1.2):\n",
    "            for i, metric in enumerate(sorted(metrics.keys())):\n",
    "                df_plot = df_eval_group[[group, metric]]\n",
    "                ax = fig.add_subplot(num_rows, num_columns, i + 1)\n",
    "                for lineval in metrics[metric]:\n",
    "                    ax.axhline(y=float(lineval), linestyle='--', linewidth=0.5, color='black')\n",
    "                sns.barplot(x=df_plot[group], y=df_plot[metric], color='grey', ax=ax, order=bar_names)\n",
    "                ax.set_xticklabels(wrapped_bar_names, rotation=90) \n",
    "                ax.set_xlabel('')\n",
    "                ax.set_ylabel('')\n",
    "\n",
    "                # set the y-limits of the plots appropriately\n",
    "                if metric.startswith('corr') or metric.startswith('wtkappa'):\n",
    "                    if df_plot[metric].min() < 0:\n",
    "                        y_limits = (-1.0, 1.0)\n",
    "                        ax.axhline(y=0.0, linestyle='--', linewidth=0.5, color='black')\n",
    "                    else:\n",
    "                        y_limits = (0.0, 1.0)\n",
    "                    ax.set_ylim(y_limits)\n",
    "                elif metric.startswith('R2'):\n",
    "                    min_value = df_plot[metric].min()\n",
    "                    if min_value < 0:\n",
    "                        y_limits = (min_value - 0.1, 1.0)\n",
    "                        ax.axhline(y=0.0, linestyle='--', linewidth=0.5, color='black')\n",
    "                    else:\n",
    "                        y_limits = (0.0, 1.0)\n",
    "                    ax.set_ylim(y_limits)\n",
    "                elif metric.startswith('RMSE'):\n",
    "                    max_value = df_plot[metric].max()\n",
    "                    y_limits = (0.0, max(max_value + 0.1, 1.0))\n",
    "                    ax.set_ylim(y_limits)\n",
    "                elif metric.startswith('DSM'):\n",
    "                    min_value = df_plot[metric].min()\n",
    "                    if min_value < 0:\n",
    "                        ax.axhline(y=0.0, linestyle='--', linewidth=0.5, color='black')\n",
    "\n",
    "                # set the title\n",
    "                ax.set_title('{} by {}'.format(metric, group))\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "        imgfile = join(figure_dir, '{}_eval_by_{}.svg'.format(experiment_id, group))\n",
    "        plt.savefig(imgfile)\n",
    "\n",
    "        if use_thumbnails:\n",
    "            show_thumbnail(imgfile, next(id_generator))\n",
    "        else:\n",
    "            plt.show()\n",
    "            \n",
    "    else:\n",
    "        display(Markdown(\"None of the groups in {} had {} or more responses.\".format(group,\n",
    "                                                                                    min_n_per_group[group])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional fairness analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a note for ETS users\n",
    "from rsmtool import HAS_RSMEXTRA\n",
    "if HAS_RSMEXTRA:\n",
    "    from rsmextra.settings import fairness_note\n",
    "    display(Markdown(fairness_note))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section presents additional fairness analyses described in detail in [Loukina et al. (2019)](https://aclweb.org/anthology/papers/W/W19/W19-4401/).\n",
    "These analyses consider separately different definitions of fairness and can assist in further trouble-shooting \n",
    "the observed subgroup differences. The evaluations focuses on three dimensions:\n",
    "\n",
    "* **Outcome fairness** measures:  \n",
    "\n",
    "    - *Overall score accuracy*: whether the automated scores are equally accurate for each group. The metric shows how much of the variance in squared error $(S-H)^2$ is explained by subgroup membership.\n",
    "\n",
    "    -  *Overall score difference*: whether the automated scores are consistently different from human scores for members of a certain group. The metric shows how much of the variance in actual error $S-H$ is explained by subgroup membership. \n",
    "\n",
    "The differences in the outcome fairness measures might be due to different mean scores (different score distributions) across subgroups or due to differential treatment of different subgroups by the scoring engine or both. \n",
    "\n",
    "* **Process fairness** measures:\n",
    "\n",
    "    - *Conditional score difference*: whether the automated scoring model assigns different scores to members from different groups despite them having the same construct proficiency. The metric shows how much additional variance in actual error ($S-H$) is explained by subgroup membership after controlling for human score, which can be thought of as a reasonable proxy for proficiency. \n",
    "\n",
    "The differences in process fairness measures indicate differential treatment of different subgroups by the scoring model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and auxiliary function that we will need \n",
    "# for the plot later on\n",
    "def errplot(x, y, xerr, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    data = kwargs.pop(\"data\")\n",
    "    # let's remove color from kwargs\n",
    "    color = kwargs.pop('color')\n",
    "    data.plot(x=x, y=y, xerr=xerr,\n",
    "              kind=\"barh\", ax=ax,\n",
    "              color=colors,\n",
    "              **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check if we already created the merged file in another notebook\n",
    "\n",
    "try:\n",
    "    df_pred_preproc_merged\n",
    "except NameError:\n",
    "    df_pred_preproc_merged = pd.merge(df_pred_preproc, df_test_metadata, on = 'spkitemid')\n",
    "    \n",
    "# check which score we are using\n",
    "system_score_column = \"scale_trim\" if use_scaled_predictions else \"raw_trim\"\n",
    "\n",
    "for group in groups_eval:\n",
    "    \n",
    "    display(Markdown(\"### Additional fairness evaluation by {}\".format(group)))\n",
    "    \n",
    "    # run the actual analyses. We are currently doing this in the notebook\n",
    "    # so that it is easy to skip these if necessary. The notebook\n",
    "    # and analyses are set up in a way that will make it easy to move these\n",
    "    # in future to the main pipeline and read in the outputs here. \n",
    "    fit_dict, fairness_container = get_fairness_analyses(df_pred_preproc_merged,\n",
    "                                                         group,\n",
    "                                                         system_score_column)\n",
    "    \n",
    "    # write the results to disk so that we can consider them in tests\n",
    "    write_fairness_results(fit_dict,\n",
    "                           fairness_container,\n",
    "                           group,\n",
    "                           output_dir,\n",
    "                           experiment_id,\n",
    "                           file_format)\n",
    "    \n",
    "    # first show summary results\n",
    "    df_fairness = fairness_container['fairness_metrics_by_{}'.format(group)]\n",
    "\n",
    "    \n",
    "    display(Markdown(\"The summary table shows the overall score accuracy (OSA), overall score difference (OSD) \"\n",
    "                    \"and conditional score difference (CSD). The first row reports the percentage of variance, \"\n",
    "                    \" explained by group membership, the second row shows $p$ value. \"\n",
    "                     \"{} was used as a reference category. \"\n",
    "                    \"Larger values of R2 indicate larger differences between subgroups. \" \n",
    "                    \"Further detail about each model can be found in [intermediate \"\n",
    "                    \"output files](#Links-to-Intermediate-Files).\".format(df_fairness['base_category'].values[0])))\n",
    "    \n",
    "    display(HTML(df_fairness.loc[['R2', 'sig'],\n",
    "                                 ['Overall score accuracy',\n",
    "                                  'Overall score difference',\n",
    "                                  'Conditional score difference']].to_html(classes='sortable',\n",
    "                                                                     float_format=float_format_func)))\n",
    "\n",
    "    \n",
    "    Markdown_str = [(\"The plots show error estimates for different categories for each group \"\n",
    "                    \"(squared error for OSA, raw error for OSD, and conditional raw error for CSD) \"\n",
    "                    \"The estimates have been adjusted for the value of the group used as the Intercept. \"\n",
    "                    \"Black lines show 95% confidence intervals estimated by the model.\")]\n",
    "    \n",
    "    # if we only care about groups above threshold, identify those.\n",
    "    \n",
    "    category_counts = df_pred_preproc_merged[group].value_counts()        \n",
    "    \n",
    "    if group in min_n_per_group:\n",
    "        Markdown_str.append(\"While the models were fit to all data, the plots only show estimates for \"\n",
    "                            \"categories with more than {} members and the Intercept ({}).\".format(min_n_per_group[group],\n",
    "                                                                                                  df_fairness['base_category'].values[0]))\n",
    "        \n",
    "        groups_by_size = category_counts[category_counts >= min_n_per_group[group]].index\n",
    "        df_pred_preproc_selected = df_pred_preproc_merged[df_pred_preproc_merged[group].isin(groups_by_size)].copy()\n",
    "    else:\n",
    "        groups_by_size = category_counts.index\n",
    "        df_pred_preproc_selected = df_pred_preproc_merged.copy()\n",
    "    \n",
    "    display(Markdown('\\n'.join(Markdown_str)))\n",
    "   \n",
    "    \n",
    "    if len(groups_by_size) > 0:\n",
    "\n",
    "        # assemble all coefficients into a long data frame\n",
    "        all_coefs = []\n",
    "        for metrics in ['osa', 'csd', 'osd']:\n",
    "            df_metrics = fairness_container['estimates_{}_by_{}'.format(metrics, group)].copy()\n",
    "            # compute adjusted error estimates by adding the value of the Intercept\n",
    "            # to all non-Intercept values\n",
    "            non_index_cols = [r for r in df_metrics.index if not \"Intercept\" in r]\n",
    "            index_col = [r for r in df_metrics.index if \"Intercept\" in r]\n",
    "            df_metrics['error_estimate'] = df_metrics['estimate']\n",
    "            df_metrics.loc[non_index_cols,\n",
    "                          'error_estimate'] = df_metrics.loc[non_index_cols,\n",
    "                                                            'estimate'] + df_metrics.loc[index_col,\n",
    "                                                                                        'estimate'].values\n",
    "            # create a column for metrics\n",
    "            df_metrics['metrics'] = metrics\n",
    "            # only use groups with values above threshold and the intercept\n",
    "            df_metrics[group] = df_metrics.index\n",
    "            df_metrics_selected = df_metrics[df_metrics[group].isin(groups_by_size) |\n",
    "                                             (df_metrics[group] == index_col[0])]\n",
    "            all_coefs.append(df_metrics_selected)\n",
    "\n",
    "\n",
    "        # show coefficient plots\n",
    "        # define groups and color palette\n",
    "        colors = sns.color_palette(\"Greys_r\", len(groups_by_size))\n",
    "\n",
    "        df_coefs_all = pd.concat(all_coefs)\n",
    "\n",
    "        # compute the size of the confidence interval from the boundary\n",
    "        df_coefs_all['CI'] = np.abs(df_coefs_all['[0.025'] - df_coefs_all['estimate'])\n",
    "\n",
    "        # plot the coefficients\n",
    "        with sns.axes_style('whitegrid'), sns.plotting_context('notebook', font_scale=2):\n",
    "            g = sns.FacetGrid(df_coefs_all, col=\"metrics\",\n",
    "                              height=10, col_order = ['osa', 'osd', 'csd'])\n",
    "            g.map_dataframe(errplot, group, \"error_estimate\",  \"CI\").set_axis_labels(\"Error estimate\",\n",
    "                                                                                    group)\n",
    "\n",
    "            imgfile = join(figure_dir, '{}_fairness_estimates_{}.svg'.format(experiment_id, group))\n",
    "            plt.savefig(imgfile)\n",
    "            if use_thumbnails:\n",
    "                show_thumbnail(imgfile, next(id_generator))\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "        # Show the conditional score plot\n",
    "        markdown_str = [(\"The plot shows average {} system score for each \"\n",
    "                        \"group conditioned \"\n",
    "                         \"on human score.\".format(system_score_column))]\n",
    "        if group in min_n_per_group:\n",
    "            markdown_str.append(\"The plot only shows estimates for \"\n",
    "                               \"categories with more than {} members and the\"\n",
    "                                \"reference category ({}).\".format(min_n_per_group[group],\n",
    "                                                          df_fairness['base_category'].values[0]))\n",
    "        \n",
    "        display(Markdown('\\n'.join(markdown_str)))\n",
    "        \n",
    "\n",
    "\n",
    "        with sns.axes_style('whitegrid'), sns.plotting_context('notebook', font_scale=1.2):\n",
    "            p = sns.catplot(x='sc1', y=system_score_column,\n",
    "                            hue=group, \n",
    "                            hue_order = groups_by_size,\n",
    "                            palette=colors,\n",
    "                            legend_out=True,\n",
    "                            kind=\"point\",\n",
    "                            data=df_pred_preproc_selected)\n",
    "\n",
    "            #plt.tight_layout(h_pad=1.0)\n",
    "            imgfile = join(figure_dir, '{}_conditional_score_{}.svg'.format(experiment_id, group))\n",
    "            plt.savefig(imgfile)\n",
    "            if use_thumbnails:\n",
    "                show_thumbnail(imgfile, next(id_generator))\n",
    "            else:\n",
    "                plt.show()\n",
    "    else:\n",
    "        display(Markdown(\"None of the groups in {} had {} or more responses in the evaluation set.\".format(group,\n",
    "                                                                                     min_n_per_group[group])))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links to intermediate files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the hyperlinks below to see the intermediate experiment files generated as part of this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rsmtool.utils.notebook import show_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_files(output_dir, experiment_id, file_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "system_name = platform.system()\n",
    "\n",
    "# People might not know what 'Darwin' is, so we should replace that with 'Mac OS X'\n",
    "if system_name == 'Darwin':\n",
    "    system_name = 'Mac OS X'\n",
    "    \n",
    "# get the architecture\n",
    "architecture = platform.architecture()[0]\n",
    "\n",
    "# get the rsmtool version\n",
    "rsmtool_version_str = '.'.join(map(str, rsmtool_version))\n",
    "\n",
    "display(Markdown('This report was generated using rsmtool v{} on a '\n",
    "                 '{} computer running {}.'.format(rsmtool_version_str, \n",
    "                                                  architecture, \n",
    "                                                  system_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "package_names = '\\n'.join(sorted([\"%s==%s\" % (i.key, i.version) for i in pkg_resources.working_set]))\n",
    "display(HTML('<div id=\"packages\"><pre>{}</pre></div>'.format(package_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "// Code to dynamically generate table of contents at the top of the HTML file\n",
    "var tocEntries = ['<ul>'];\n",
    "var anchors = $('a.anchor-link');\n",
    "var headingTypes = $(anchors).parent().map(function() { return $(this).prop('tagName')});\n",
    "var headingTexts = $(anchors).parent().map(function() { return $(this).text()});\n",
    "var subList = false;\n",
    "\n",
    "$.each(anchors, function(i, anch) {\n",
    "    var hType = headingTypes[i];\n",
    "    var hText = headingTexts[i];\n",
    "    hText = hText.substr(0, hText.length - 1);\n",
    "    if (hType == 'H2') {\n",
    "        if (subList) {\n",
    "            tocEntries.push('</ul>')\n",
    "            subList = false;\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "    else if (hType == 'H3') {\n",
    "        if (!subList) {\n",
    "            subList = true;\n",
    "            tocEntries.push('<ul>')\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "});\n",
    "tocEntries.push('</ul>')\n",
    "$('#toc').html(tocEntries.join(' '))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}