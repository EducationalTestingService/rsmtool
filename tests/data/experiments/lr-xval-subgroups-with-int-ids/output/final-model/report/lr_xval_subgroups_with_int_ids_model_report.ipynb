{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting options for the plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats={'retina', 'svg'}\n",
    "%config InlineBackend.rc={'savefig.dpi': 150}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import platform\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from functools import partial\n",
    "from os.path import abspath, relpath, exists, join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "from textwrap import wrap\n",
    "\n",
    "# allow older versions of pandas to work\n",
    "try:\n",
    "    from pandas.io.common import DtypeWarning\n",
    "except ImportError:\n",
    "    from pandas.errors import DtypeWarning\n",
    "\n",
    "from IPython import sys_info\n",
    "from IPython.display import display, HTML, Image, Javascript, Markdown, SVG\n",
    "from rsmtool.reader import DataReader\n",
    "from rsmtool.writer import DataWriter\n",
    "from rsmtool.utils.files import parse_json_with_comments\n",
    "from rsmtool.utils.notebook import (float_format_func,\n",
    "                                    int_or_float_format_func,\n",
    "                                    compute_subgroup_plot_params,\n",
    "                                    bold_highlighter,\n",
    "                                    color_highlighter,\n",
    "                                    show_thumbnail)\n",
    "\n",
    "from rsmtool.fairness_utils import (get_fairness_analyses,\n",
    "                                    write_fairness_results)\n",
    "\n",
    "from rsmtool.version import VERSION as rsmtool_version\n",
    "\n",
    "# turn off interactive plotting\n",
    "plt.ioff()\n",
    "\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_report_dir = os.environ.get('RSM_REPORT_DIR', None)\n",
    "if rsm_report_dir is None:\n",
    "    rsm_report_dir = os.getcwd()\n",
    "\n",
    "rsm_environ_config = join(rsm_report_dir, '.environ.json')\n",
    "if not exists(rsm_environ_config):\n",
    "    raise FileNotFoundError('The file {} cannot be located. '\n",
    "                            'Please make sure that either (1) '\n",
    "                            'you have set the correct directory with the `RSM_REPORT_DIR` '\n",
    "                            'environment variable, or (2) that your `.environ.json` '\n",
    "                            'file is in the same directory as your notebook.'.format(rsm_environ_config))\n",
    "    \n",
    "environ_config = parse_json_with_comments(rsm_environ_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "  div.prompt.output_prompt { \n",
    "    color: white; \n",
    "  }\n",
    "  \n",
    "  span.highlight_color {\n",
    "    color: red;\n",
    "  }\n",
    "  \n",
    "  span.highlight_bold {\n",
    "    font-weight: bold;  \n",
    "  }\n",
    "    \n",
    "  @media print {\n",
    "    @page {\n",
    "      size: landscape;\n",
    "      margin: 0cm 0cm 0cm 0cm;\n",
    "    }\n",
    "\n",
    "    * {\n",
    "      margin: 0px;\n",
    "      padding: 0px;\n",
    "    }\n",
    "\n",
    "    #toc {\n",
    "      display: none;\n",
    "    }\n",
    "\n",
    "    span.highlight_color, span.highlight_bold {\n",
    "        font-weight: bolder;\n",
    "        text-decoration: underline;\n",
    "    }\n",
    "\n",
    "    div.prompt.output_prompt {\n",
    "      display: none;\n",
    "    }\n",
    "    \n",
    "    h3#Python-packages, div#packages {\n",
    "      display: none;\n",
    "  }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you will need to set the following manually\n",
    "# if you are using this notebook interactively.\n",
    "experiment_id = environ_config.get('EXPERIMENT_ID')\n",
    "description = environ_config.get('DESCRIPTION')\n",
    "context = environ_config.get('CONTEXT')\n",
    "train_file_location = environ_config.get('TRAIN_FILE_LOCATION')\n",
    "test_file_location = environ_config.get('TEST_FILE_LOCATION')\n",
    "output_dir = environ_config.get('OUTPUT_DIR')\n",
    "figure_dir = environ_config.get('FIGURE_DIR')\n",
    "model_name = environ_config.get('MODEL_NAME')\n",
    "model_type = environ_config.get('MODEL_TYPE')\n",
    "skll_fixed_parameters = environ_config.get('SKLL_FIXED_PARAMETERS')\n",
    "skll_objective = environ_config.get('SKLL_OBJECTIVE')\n",
    "file_format = environ_config.get('FILE_FORMAT')\n",
    "length_column = environ_config.get('LENGTH_COLUMN')\n",
    "second_human_score_column = environ_config.get('H2_COLUMN')\n",
    "use_scaled_predictions = environ_config.get('SCALED')\n",
    "min_score = environ_config.get(\"MIN_SCORE\")\n",
    "max_score = environ_config.get(\"MAX_SCORE\")\n",
    "standardize_features = environ_config.get('STANDARDIZE_FEATURES')\n",
    "exclude_zero_scores = environ_config.get('EXCLUDE_ZEROS')\n",
    "feature_subset_file = environ_config.get('FEATURE_SUBSET_FILE', ' ')\n",
    "min_items = environ_config.get('MIN_ITEMS')\n",
    "use_thumbnails = environ_config.get('USE_THUMBNAILS')\n",
    "predict_expected_scores = environ_config.get('PREDICT_EXPECTED_SCORES')\n",
    "rater_error_variance = environ_config.get(\"RATER_ERROR_VARIANCE\")\n",
    "\n",
    "# groups for analysis by prompt or subgroup.\n",
    "groups_desc = environ_config.get('GROUPS_FOR_DESCRIPTIVES') \n",
    "groups_eval = environ_config.get('GROUPS_FOR_EVALUATIONS') \n",
    "\n",
    "# min number of n for group to be displayed in the report\n",
    "min_n_per_group = environ_config.get('MIN_N_PER_GROUP')\n",
    "\n",
    "if min_n_per_group is None:\n",
    "    min_n_per_group = {}\n",
    "\n",
    "# javascript path\n",
    "javascript_path = environ_config.get(\"JAVASCRIPT_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize counter for thumbnail IDs\n",
    "id_generator = itertools.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(javascript_path, \"sort.js\"), \"r\", encoding=\"utf-8\") as sortf:\n",
    "    display(Javascript(data=sortf.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown('''This report presents the analysis for **{}**: {}'''.format(experiment_id, description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = ''\n",
    "if use_thumbnails:\n",
    "    markdown_str += (\"\"\"\\n  - Images in this report have been converted to \"\"\"\n",
    "                     \"\"\"clickable thumbnails.\"\"\")\n",
    "if predict_expected_scores:\n",
    "    markdown_str += (\"\"\"\\n  - Predictions analyzed in this report are *expected scores*, \"\"\"\n",
    "                     \"\"\"i.e., probability-weighted averages over all score points.\"\"\")\n",
    "\n",
    "if markdown_str:\n",
    "    markdown_str = '**Notes**:' + markdown_str\n",
    "    display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(time.strftime('%c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training and testing features, both raw and pre-processed\n",
    "# Make sure that the `spkitemid` and `candidate` columns are read as strings \n",
    "# to preserve any leading zeros\n",
    "# We filter DtypeWarnings that pop up mostly in very large files\n",
    "\n",
    "string_columns = ['spkitemid', 'candidate']\n",
    "converter_dict = {column: str for column in string_columns}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore', category=DtypeWarning)\n",
    "    if exists(train_file_location):\n",
    "        df_train_orig = DataReader.read_from_file(train_file_location)\n",
    "\n",
    "    train_file = join(output_dir, '{}_train_features.{}'.format(experiment_id,\n",
    "                                                                file_format))\n",
    "    if exists(train_file):\n",
    "        df_train = DataReader.read_from_file(train_file, converters=converter_dict)\n",
    "\n",
    "    train_metadata_file = join(output_dir, '{}_train_metadata.{}'.format(experiment_id,\n",
    "                                                                         file_format))    \n",
    "    if exists(train_metadata_file):\n",
    "        df_train_metadata = DataReader.read_from_file(train_metadata_file, converters=converter_dict)\n",
    "\n",
    "    train_other_columns_file = join(output_dir, '{}_train_other_columns.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(train_other_columns_file):\n",
    "        df_train_other_columns = DataReader.read_from_file(train_other_columns_file, converters=converter_dict)\n",
    "\n",
    "    train_length_file = join(output_dir, '{}_train_response_lengths.{}'.format(experiment_id,\n",
    "                                                                               file_format))\n",
    "    if exists(train_length_file):\n",
    "        df_train_length = DataReader.read_from_file(train_length_file, converters=converter_dict)\n",
    "\n",
    "    train_excluded_file = join(output_dir, '{}_train_excluded_responses.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(train_excluded_file):\n",
    "        df_train_excluded = DataReader.read_from_file(train_excluded_file, converters=converter_dict)\n",
    "\n",
    "    train_responses_with_excluded_flags_file = join(output_dir, '{}_train_responses_with_excluded_flags.{}'.format(experiment_id,\n",
    "                                                                                                                   file_format))\n",
    "    if exists(train_responses_with_excluded_flags_file):\n",
    "        df_train_responses_with_excluded_flags = DataReader.read_from_file(train_responses_with_excluded_flags_file,\n",
    "                                                                           converters=converter_dict)\n",
    "\n",
    "    train_preproc_file = join(output_dir, '{}_train_preprocessed_features.{}'.format(experiment_id,\n",
    "                                                                                     file_format))    \n",
    "    if exists(train_preproc_file):\n",
    "        df_train_preproc = DataReader.read_from_file(train_preproc_file, converters=converter_dict)\n",
    "\n",
    "    if exists(test_file_location):\n",
    "        df_test_orig = DataReader.read_from_file(test_file_location)\n",
    "\n",
    "    test_file = join(output_dir, '{}_test_features.{}'.format(experiment_id,\n",
    "                                                              file_format))\n",
    "    if exists(test_file):\n",
    "        df_test = DataReader.read_from_file(test_file, converters=converter_dict)\n",
    "\n",
    "    test_metadata_file = join(output_dir, '{}_test_metadata.{}'.format(experiment_id,\n",
    "                                                                       file_format))    \n",
    "    if exists(test_metadata_file):\n",
    "        df_test_metadata = DataReader.read_from_file(test_metadata_file, converters=converter_dict)\n",
    "\n",
    "    test_other_columns_file = join(output_dir, '{}_test_other_columns.{}'.format(experiment_id,\n",
    "                                                                                 file_format))\n",
    "    if exists(test_other_columns_file):\n",
    "        df_test_other_columns = DataReader.read_from_file(test_other_columns_file, converters=converter_dict)\n",
    "\n",
    "    test_human_scores_file = join(output_dir, '{}_test_human_scores.{}'.format(experiment_id,\n",
    "                                                                               file_format))\n",
    "    if exists(test_human_scores_file):\n",
    "        df_test_human_scores = DataReader.read_from_file(test_human_scores_file, converters=converter_dict)\n",
    "\n",
    "    test_excluded_file = join(output_dir, '{}_test_excluded_responses.{}'.format(experiment_id,\n",
    "                                                                                 file_format))\n",
    "    if exists(test_excluded_file):\n",
    "        df_test_excluded = DataReader.read_from_file(test_excluded_file, converters=converter_dict)\n",
    "\n",
    "    test_responses_with_excluded_flags_file = join(output_dir, '{}_test_responses_with_excluded_flags.{}'.format(experiment_id,\n",
    "                                                                                                                 file_format))\n",
    "    if exists(test_responses_with_excluded_flags_file):\n",
    "        df_test_responses_with_excluded_flags = DataReader.read_from_file(test_responses_with_excluded_flags_file,\n",
    "                                                                          converters=converter_dict)\n",
    "\n",
    "    test_preproc_file = join(output_dir, '{}_test_preprocessed_features.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(test_preproc_file):\n",
    "        df_test_preproc = DataReader.read_from_file(test_preproc_file, converters=converter_dict)\n",
    "\n",
    "    pred_preproc_file = join(output_dir, '{}_pred_processed.{}'.format(experiment_id,\n",
    "                                                                       file_format))\n",
    "    if exists(pred_preproc_file):\n",
    "        df_pred_preproc = DataReader.read_from_file(pred_preproc_file, converters=converter_dict)\n",
    "\n",
    "    feature_file = join(output_dir, '{}_feature.{}'.format(experiment_id,\n",
    "                                                           file_format))\n",
    "    if exists(feature_file):\n",
    "        df_features = DataReader.read_from_file(feature_file, converters=converter_dict)\n",
    "        features_used = [c for c in df_features.feature.values]\n",
    "        # compute the longest feature name: we'll need if for the plots\n",
    "        longest_feature_name = max(map(len, features_used))\n",
    "\n",
    "    betas_file = join(output_dir, '{}_betas.{}'.format(experiment_id,\n",
    "                                                       file_format))\n",
    "    if exists(betas_file):\n",
    "        df_betas = DataReader.read_from_file(betas_file)\n",
    "\n",
    "    if exists(feature_subset_file):\n",
    "        df_feature_subset_specs = DataReader.read_from_file(feature_subset_file)\n",
    "    else:\n",
    "        df_feature_subset_specs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for continuous human scores in the evaluation set\n",
    "continuous_human_score = False\n",
    "\n",
    "if exists(pred_preproc_file):\n",
    "    if not df_pred_preproc['sc1'].equals(np.round(df_pred_preproc['sc1'])):\n",
    "        continuous_human_score = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall descriptive feature statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are reported before transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature descriptives table\n",
    "desc_file = join(output_dir, '{}_feature_descriptives.{}'.format(experiment_id, file_format))\n",
    "\n",
    "df_desc = DataReader.read_from_file(desc_file, index_col=0)\n",
    "HTML(df_desc.to_html(classes=['sortable'], float_format=float_format_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prevalence of recoded cases\n",
    "\n",
    "This sections shows the number and percentage of cases truncated to mean +/- 4 SD for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outliers_file = join(output_dir, '{}_feature_outliers.{}'.format(experiment_id, file_format))\n",
    "df_outliers = DataReader.read_from_file(outliers_file, index_col=0)\n",
    "df_outliers.index.name = 'feature'\n",
    "df_outliers = df_outliers.reset_index()\n",
    "df_outliers = pd.melt(df_outliers, id_vars=['feature'])\n",
    "df_outliers = df_outliers[df_outliers.variable.str.contains(r'[ulb].*?perc')]\n",
    "\n",
    "\n",
    "# we need to increase the plot height if feature names are long\n",
    "if longest_feature_name > 10:\n",
    "    height = 3 + math.ceil((longest_feature_name - 10)/10)\n",
    "else:\n",
    "    height = 3\n",
    "    \n",
    "# we also need a higher aspect if we have more than 40 features\n",
    "# The aspect defines the final width of the plot (width=aspect*height).\n",
    "# We keep the width constant (9 for plots with many features or 6\n",
    "# for plots with few features) by dividing the expected width\n",
    "# by the height. \n",
    "aspect = 9/height if len(features_used) > 40 else 6/height\n",
    "\n",
    "\n",
    "# colors for the plot\n",
    "colors = sns.color_palette(\"Greys\", 3)\n",
    "\n",
    "# what's the largest value in the data frame\n",
    "maxperc = df_outliers['value'].max()\n",
    "\n",
    "# compute the limits for the graph\n",
    "limits = (0, max(2.5, maxperc))\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "    # create a barplot without a legend since we will manually\n",
    "    # add one later\n",
    "    p = sns.catplot(x=\"feature\", y=\"value\", hue=\"variable\", kind=\"bar\", \n",
    "                    palette=colors, data=df_outliers, height=height, \n",
    "                    aspect=aspect, legend=False)\n",
    "    p.set_axis_labels('', '% cases truncated\\nto mean +/- 4*sd')\n",
    "    p.set_xticklabels(rotation=90)\n",
    "    p.set(ylim=limits)\n",
    "\n",
    "    # add a line at 2%\n",
    "    axis = p.axes[0][0]\n",
    "    axis.axhline(y=2.0, linestyle='--', linewidth=1.5, color='black')\n",
    "\n",
    "    # add a legend with the right colors\n",
    "    legend=axis.legend(('both', 'lower', 'upper'), title='', frameon=True, fancybox=True, ncol=3)\n",
    "    legend.legendHandles[0].set_color(colors[0])\n",
    "    legend.legendHandles[1].set_color(colors[1])\n",
    "\n",
    "    # we want to try to force `tight_layout()`, but if this \n",
    "    # raises a warning, we don't want the entire notebook to fail\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "    imgfile = join(figure_dir, '{}_outliers.svg'.format(experiment_id))\n",
    "    plt.savefig(imgfile)\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(imgfile, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature value distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows additional statistics for the data. Quantiles are computed using type=3 method used in SAS. The mild outliers are defined as data points between [1.5, 3) \\* IQR away from the nearest quartile. Extreme outliers are the data points >= 3 * IQR away from the nearest quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature descriptives extra table\n",
    "desce_file = join(output_dir, '{}_feature_descriptivesExtra.{}'.format(experiment_id,\n",
    "                                                                       file_format))\n",
    "df_desce = DataReader.read_from_file(desce_file, index_col=0)\n",
    "HTML(df_desce.to_html(classes=['sortable'], float_format=float_format_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook generates boxplot with feature values for all groups specified in groups_desc variable. \n",
    "# Note that this analysis uses the raw feature values after filtering out the missing features and scores.\n",
    "\n",
    "# merge metadata and feature values\n",
    "df_train_merged = pd.merge(df_train, df_train_metadata, on = 'spkitemid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(features_used)\n",
    "\n",
    "if (num_features > 150):\n",
    "    display(Markdown('### Feature values by subgroup(s)'))\n",
    "    display(Markdown('Since the data has {} (> 150) features, boxplots with feature values for all groups '\n",
    "                     'will be skipped.'.format(num_features)))\n",
    "elif (30 < num_features <= 150 and not use_thumbnails):\n",
    "    display(Markdown('### Feature values by subgroup(s)'))\n",
    "    display(Markdown('Since the data has {} (> 30 but <= 150) features, you need to set `\"use_thumbnails\"` to `true` in your '\n",
    "                     'configuration file to generate boxplots with feature values for all groups.'.format(num_features)))\n",
    "else:\n",
    "    for group in groups_desc:\n",
    "        display(Markdown('### Feature values by {}'.format(group)))\n",
    "        display(Markdown('In all plots in this subsection the values are reported before '\n",
    "                         'transformations/truncation. The lines indicate the threshold for '\n",
    "                         'truncation (mean +/- 4*SD).'))\n",
    "\n",
    "        df_train_feats = df_train_merged[features_used + [group]]\n",
    "        \n",
    "        # if we have threshold set for this group, filter the data now\n",
    "        if group in min_n_per_group:\n",
    "            display(Markdown(\"The report only shows the results for groups with \"\n",
    "                             \"at least {} responses in the training set.\".format(min_n_per_group[group])))\n",
    "\n",
    "            category_counts = df_train_merged[group].value_counts()\n",
    "            selected_categories = category_counts[category_counts >= min_n_per_group[group]].index\n",
    "\n",
    "            df_train_feats_all = df_train_merged[df_train_merged[group].isin(selected_categories)].copy()\n",
    "        else:\n",
    "        \n",
    "            df_train_feats_all = df_train_merged.copy()\n",
    "        \n",
    "        if len(df_train_feats_all) > 0:\n",
    "        \n",
    "            df_train_feats_all[group] = 'All data'\n",
    "\n",
    "            df_train_combined = pd.concat([df_train_feats, df_train_feats_all], sort=True)\n",
    "            df_train_combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Define the order of the boxes: put 'All data' first and 'No info' last.\n",
    "            group_levels = sorted(list(df_train_feats[group].unique()))\n",
    "            if 'No info' in group_levels:\n",
    "                box_names = ['All data'] + [level for level in group_levels if level != 'No info'] + ['No info']\n",
    "            else:\n",
    "                box_names = ['All data'] + group_levels\n",
    "\n",
    "            # create the faceted boxplots\n",
    "            fig = plt.figure()\n",
    "            (figure_width, \n",
    "             figure_height, \n",
    "             num_rows, \n",
    "             num_columns, \n",
    "             wrapped_box_names) = compute_subgroup_plot_params(box_names, num_features)\n",
    "\n",
    "            fig.set_size_inches(figure_width, figure_height)\n",
    "            with sns.axes_style('white'), sns.plotting_context('notebook', font_scale=1.2):\n",
    "                for i, varname in enumerate(sorted(features_used)):\n",
    "                    df_plot = df_train_combined[[group, varname]]\n",
    "                    min_value = df_plot[varname].mean() - 4 * df_plot[varname].std()\n",
    "                    max_value = df_plot[varname].mean() + 4 * df_plot[varname].std()\n",
    "                    ax = fig.add_subplot(num_rows, num_columns, i + 1)\n",
    "                    ax.axhline(y=float(min_value), linestyle='--', linewidth=0.5, color='r')\n",
    "                    ax.axhline(y=float(max_value), linestyle='--', linewidth=0.5, color='r')\n",
    "                    sns.boxplot(x=df_plot[group], y=df_plot[varname], color='#b3b3b3', ax=ax, order=box_names)\n",
    "                    ax.set_xticklabels(wrapped_box_names, rotation=90) \n",
    "                    ax.set_xlabel('')\n",
    "                    ax.set_ylabel('')\n",
    "                    plot_title = '{} by {}'.format('\\n'.join(wrap(varname, 30)), group)\n",
    "                    ax.set_title(plot_title)\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "\n",
    "            # save the figure as an SVG file.\n",
    "            imgfile = join(figure_dir, '{}_feature_boxplot_by_{}.svg'.format(experiment_id, group))\n",
    "            plt.savefig(imgfile)\n",
    "            if use_thumbnails:\n",
    "                show_thumbnail(imgfile, next(id_generator))\n",
    "            else:\n",
    "                # needed so that the figures are shown after the heading and not at the end of the cell\n",
    "                plt.show()\n",
    "        else:\n",
    "            display(Markdown(\"None of the groups in {} had {} or more responses.\".format(group,\n",
    "                                                                                         min_n_per_group[group])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature distributions and inter-feature correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set distributions\n",
    "\n",
    "The following plot shows the distributions of the feature values in \n",
    "the training set, after transformation (if applicable), truncation \n",
    "and standardization (if applicable). The line shows the kernel density estimate. The \n",
    "human score (`sc1`) is also included. \n",
    "\n",
    "Response length (`length`) is included if you specified `length_column` in the config file, unless\n",
    "the column had missing values or a standard deviation <= 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = features_used + ['sc1', 'spkitemid']\n",
    "df_train_preproc_selected_features = df_train_preproc[selected_columns]\n",
    "try:\n",
    "    df_train_preproc_selected_features = df_train_preproc_selected_features.merge(df_train_length, on='spkitemid')\n",
    "except NameError:\n",
    "    column_order = sorted(features_used) + ['sc1']\n",
    "else:\n",
    "    column_order = sorted(features_used) + ['sc1', 'length']\n",
    "\n",
    "df_train_preproc_melted = pd.melt(df_train_preproc_selected_features, id_vars=['spkitemid'])\n",
    "df_train_preproc_melted = df_train_preproc_melted[['variable', 'value']]\n",
    "\n",
    "# we need to reduce col_wrap and increase width if the feature names are too long\n",
    "if longest_feature_name > 10:\n",
    "    col_wrap = 2\n",
    "    # adjust height to allow for wrapping really long names. We allow 0.25 in per line\n",
    "    height = 2+(math.ceil(longest_feature_name/30)*0.25)\n",
    "    aspect = 4/height\n",
    "else:\n",
    "    col_wrap = 3\n",
    "    aspect = 1\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    with sns.axes_style('white'):\n",
    "        g = sns.FacetGrid(col='variable', data=df_train_preproc_melted, col_wrap=col_wrap, \n",
    "                          col_order=column_order, sharex=False, sharey=False, height=height, \n",
    "                          aspect=aspect)\n",
    "        g.map(sns.distplot, \"value\", color=\"grey\", kde=False)\n",
    "        for ax, cname in zip(g.axes, g.col_names):\n",
    "            labels = ax.get_xticks()\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_xticklabels(labels, rotation=90)\n",
    "            plot_title = '\\n'.join(wrap(str(cname), 30))\n",
    "            ax.set_title(plot_title)\n",
    "\n",
    "        plt.tight_layout(h_pad=1.0)\n",
    "        imgfile = join(figure_dir, '{}_distrib.svg'.format(experiment_id))\n",
    "        plt.savefig(imgfile)\n",
    "        if use_thumbnails:\n",
    "            show_thumbnail(imgfile, next(id_generator))\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-feature correlations\n",
    "\n",
    "The following table shows the Pearson correlations between all the training features\n",
    "after transformation (if applicable), truncation and standardization (if applicable). The human score \n",
    "(`sc1`) is also included. \n",
    "\n",
    "Response length (`length`) is included if \n",
    "you specified `length_column` in the config file, unless the column had missing \n",
    "values or a standard deviation <= 0. \n",
    "\n",
    "The following values are <span class=\"highlight_color\">highlighted</span>:\n",
    "- inter-feature correlations above 0.7, and\n",
    "- `sc1`-feature correlations lower than 0.1 or higher than 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cors_file = join(output_dir, '{}_cors_processed.{}'.format(experiment_id,\n",
    "                                                           file_format))\n",
    "df_cors = DataReader.read_from_file(cors_file, index_col=0)\n",
    "if 'length' in df_cors.columns:\n",
    "    feature_columns = sorted([c for c in df_cors.columns if c not in ['sc1', 'length']])\n",
    "    order = ['sc1', 'length'] + feature_columns\n",
    "else:\n",
    "    feature_columns = sorted([c for c in df_cors.columns if c != 'sc1'])\n",
    "    order = ['sc1'] + feature_columns\n",
    "    \n",
    "df_cors = df_cors.reindex(index=order, columns=order)\n",
    "\n",
    "# wrap the column names if the feature names are very long\n",
    "if longest_feature_name > 10:\n",
    "    column_names = ['\\n'.join(wrap(c, 10)) for c in order]\n",
    "else:\n",
    "    column_names = order\n",
    "    \n",
    "df_cors.columns = column_names\n",
    "\n",
    "# apply two different formatting to the columns according\n",
    "# to two different thresholds. The first one highlights all\n",
    "# inter-feature correlations > 0.7 (so, not including sc1)\n",
    "# and the second highlights all sc1-X correlations lower\n",
    "# than 0.1 and higher than 0.7. We will use red for the\n",
    "# first formatting and blue for the second one. \n",
    "formatter1 = partial(color_highlighter, low=-1, high=0.7)\n",
    "formatter2 = partial(color_highlighter, low=0.1, high=0.7)\n",
    "\n",
    "formatter_dict = {c: formatter1 for c in column_names if not c == 'sc1'}\n",
    "formatter_dict.update({'sc1': formatter2})\n",
    "\n",
    "HTML(df_cors.to_html(classes=['sortable'], formatters=formatter_dict, escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal and partial correlations\n",
    "\n",
    "The plot below shows correlations between truncated and standardized (if applicable) values of each feature against human score. The first bar (`Marginal`) in each case shows Pearson's correlation. The second bar (`Partial - all`) shows partial correlations after controlling for all other variables. If you specified `length_column` in the config file, a third bar (`Partial - length`) will show partial correlations of each feature against the human score after controlling for length. The dotted lines correspond to *r* = 0.1 and *r* = 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in and merge the score correlations \n",
    "margcor_file = join(output_dir, '{}_margcor_score_all_data.{}'.format(experiment_id, file_format))\n",
    "pcor_file = join(output_dir, '{}_pcor_score_all_data.{}'.format(experiment_id, file_format))\n",
    "\n",
    "df_margcor = DataReader.read_from_file(margcor_file, index_col=0)\n",
    "df_pcor = DataReader.read_from_file(pcor_file, index_col=0)\n",
    "\n",
    "# check if we have length partial correlations\n",
    "pcor_no_length_file = join(output_dir, '{}_pcor_score_no_length_all_data.{}'.format(experiment_id,\n",
    "                                                                                    file_format))\n",
    "with_length = exists(pcor_no_length_file)\n",
    "if with_length:\n",
    "    df_pcor_no_length = DataReader.read_from_file(pcor_no_length_file, index_col=0)\n",
    "    df_mpcor = pd.DataFrame([df_margcor.loc['All data'], \n",
    "                             df_pcor.loc['All data'], \n",
    "                             df_pcor_no_length.loc['All data']]).transpose()\n",
    "    df_mpcor.columns = ['marginal', 'partial_all', 'partial_length']\n",
    "    num_entries = 3\n",
    "    labels = ('Marginal', 'Partial - all', 'Partial - length')\n",
    "\n",
    "else:\n",
    "    df_mpcor = pd.DataFrame([df_margcor.loc['All data'], \n",
    "                             df_pcor.loc['All data']]).transpose()\n",
    "    df_mpcor.columns = ['marginal', 'partial_all']\n",
    "    num_entries = 2\n",
    "    labels = ('Marginal', 'Partial (all)')\n",
    "\n",
    "df_mpcor.index.name = 'feature'\n",
    "df_mpcor = df_mpcor.reset_index()\n",
    "df_mpcor = pd.melt(df_mpcor, id_vars=['feature'])\n",
    "\n",
    "# we need to change the plot height if the feature names are long\n",
    "if longest_feature_name > 10:\n",
    "    height = 3 + math.ceil((longest_feature_name - 10)/10)\n",
    "else:\n",
    "    height = 3\n",
    "        \n",
    "# we need a higher aspect if we have more than 40 features\n",
    "aspect = 9/height if len(features_used) > 40 else 6/height\n",
    "\n",
    "\n",
    "# get the colors for the plot\n",
    "colors = sns.color_palette(\"Greys\", num_entries)\n",
    "\n",
    "# check for any negative correlations\n",
    "limits = (0, 1)\n",
    "if len(df_mpcor[df_mpcor.value < 0]):\n",
    "    limits = (-1, 1)\n",
    "\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "\n",
    "    # generate a bar plot but without the legend since we will\n",
    "    # manually add one later\n",
    "    p = sns.catplot(x=\"feature\", y=\"value\", hue=\"variable\", kind=\"bar\",\n",
    "                    palette=colors, data=df_mpcor, height=height, \n",
    "                    aspect=aspect, legend=False)\n",
    "    p.set_axis_labels('', 'Correlation with score')\n",
    "    p.set_xticklabels(rotation=90)\n",
    "    p.set(ylim=limits)\n",
    "    \n",
    "    # add a line at 0.1 and 0.7\n",
    "    axis = p.axes[0][0]\n",
    "    axis.axhline(y=0.1, linestyle='--', linewidth=0.5, color='black');\n",
    "    axis.axhline(y=0.7, linestyle='--', linewidth=0.5, color='black');\n",
    "\n",
    "    # create the legend manually with the right colors\n",
    "    legend = axis.legend(labels=labels, title='', frameon=True, \n",
    "                         fancybox=True, ncol=num_entries)\n",
    "    for i in range(num_entries):\n",
    "        legend.legendHandles[i].set_color(colors[i]);\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "    imgfile = join(figure_dir, '{}_cors_score.svg'.format(experiment_id))\n",
    "    plt.savefig(imgfile)\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(imgfile, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_margcor_file = join(output_dir, '{}_margcor_length_all_data.{}'.format(experiment_id,\n",
    "                                                                           file_format))\n",
    "len_pcor_file = join(output_dir, '{}_pcor_length_all_data.{}'.format(experiment_id,\n",
    "                                                                     file_format))\n",
    "if exists(len_margcor_file) and exists(len_pcor_file):\n",
    "\n",
    "    if standardize_features:\n",
    "        display(Markdown(\"The plot below shows the same correlations between truncated and \"\n",
    "                         \"standardized values of each feature against length.\"))\n",
    "    else:\n",
    "        display(Markdown(\"The plot below shows the same correlations between truncated and \"\n",
    "                         \"un-standardized values of each feature against length.\"))\n",
    "\n",
    "    df_margcor = DataReader.read_from_file(len_margcor_file, index_col=0)\n",
    "    df_pcor = DataReader.read_from_file(len_pcor_file, index_col=0)\n",
    "    df_mpcor = pd.DataFrame([df_margcor.loc['All data'], df_pcor.loc['All data']]).transpose()\n",
    "    df_mpcor.index.name = 'feature'\n",
    "    df_mpcor.columns = ['marginal', 'partial']\n",
    "    df_mpcor = df_mpcor.reset_index()\n",
    "    df_mpcor = pd.melt(df_mpcor, id_vars=['feature'])\n",
    "\n",
    "    # we need to change the plot height if the feature names are long\n",
    "    if longest_feature_name > 10:\n",
    "        height = 3 + math.ceil((longest_feature_name - 10)/10)\n",
    "    else:\n",
    "        height = 3\n",
    "        \n",
    "    # we need a higher aspect if we have more than 40 features\n",
    "    aspect = 9/height if len(features_used) > 40 else 6/height\n",
    "\n",
    "\n",
    "    # check for any negative correlations\n",
    "    limits = (0, 1)\n",
    "    if len(df_mpcor[df_mpcor.value < 0]):\n",
    "        limits = (-1, 1)\n",
    "\n",
    "    # get the colors for the plot\n",
    "    colors = sns.color_palette(\"Greys\", 2)\n",
    "        \n",
    "    with sns.axes_style('whitegrid'):\n",
    "        \n",
    "        # create a barplot but without the legend since\n",
    "        # we will manually add one later\n",
    "        p = sns.catplot(x=\"feature\", y=\"value\", hue=\"variable\", kind=\"bar\",\n",
    "                        palette=colors, data=df_mpcor, height=height, \n",
    "                        aspect=aspect, legend=False)\n",
    "        p.set_axis_labels('', 'Correlation with length')\n",
    "        p.set_xticklabels(rotation=90)\n",
    "        p.set(ylim=limits)\n",
    "\n",
    "        # create the legend manually with the right colors\n",
    "        axis = p.axes[0][0]\n",
    "        legend = axis.legend(labels=('Marginal', 'Partial  - all'), title='', \n",
    "                             frameon=True, fancybox=True, ncol=2)\n",
    "        legend.legendHandles[0].set_color(colors[0]);\n",
    "        legend.legendHandles[1].set_color(colors[1]);\n",
    "        imgfile = join(figure_dir, '{}_cors_length.svg'.format(experiment_id))\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "        plt.savefig(imgfile) \n",
    "        if use_thumbnails:\n",
    "            show_thumbnail(imgfile, next(id_generator))\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(groups_desc) > 0:\n",
    "    markdown_str = [\"## Differential feature functioning\"]\n",
    "    markdown_str.append(\"This section shows differential feature functioning (DFF) plots \"\n",
    "                        \"for all features and subgroups. The features are shown after applying \"\n",
    "                        \"transformations (if applicable) and truncation of outliers.\")\n",
    "    display(Markdown('\\n'.join(markdown_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check if we already created the merged file in another notebook\n",
    "\n",
    "try:\n",
    "    df_train_preproc_merged\n",
    "except NameError:\n",
    "    df_train_preproc_merged = pd.merge(df_train_preproc, df_train_metadata, on = 'spkitemid')\n",
    "\n",
    "for group in groups_desc:\n",
    "    display(Markdown(\"### DFF by {}\".format(group)))\n",
    "    \n",
    "    if group in min_n_per_group:\n",
    "        display(Markdown(\"The report only shows the results for groups with \"\n",
    "                         \"at least {} responses in the training set.\".format(min_n_per_group[group])))\n",
    "        \n",
    "        category_counts = df_train_preproc_merged[group].value_counts()\n",
    "        selected_categories = category_counts[category_counts >= min_n_per_group[group]].index\n",
    "        \n",
    "        df_train_preproc_selected = df_train_preproc_merged[df_train_preproc_merged[group].isin(selected_categories)].copy()\n",
    "    else:\n",
    "        df_train_preproc_selected = df_train_preproc_merged.copy()\n",
    "    \n",
    "    \n",
    "    if len(df_train_preproc_selected) > 0:\n",
    "        \n",
    "        # we need to reduce col_wrap and increase width if the feature names are too long\n",
    "        if longest_feature_name > 10:\n",
    "            col_wrap = 2\n",
    "            # adjust height to allow for wrapping really long names. We allow 0.25 in per line\n",
    "            height = 2+(math.ceil(longest_feature_name/30)*0.25)\n",
    "            aspect = 5/height\n",
    "            # show legend near the second plot in the grid\n",
    "            plot_with_legend = 1\n",
    "        else:\n",
    "            height=3\n",
    "            col_wrap = 3\n",
    "            aspect = 1\n",
    "            # show the legend near the third plot in the grid\n",
    "            plot_with_legend = 2\n",
    "        \n",
    "        selected_columns = ['spkitemid', 'sc1'] + features_used + [group]\n",
    "        df_melted = pd.melt(df_train_preproc_selected[selected_columns], id_vars=['spkitemid', 'sc1', group], var_name='feature')\n",
    "        group_values = sorted(df_melted[group].unique())\n",
    "        colors = sns.color_palette(\"Greys\", len(group_values))\n",
    "        with sns.axes_style('whitegrid'), sns.plotting_context('notebook', font_scale=1.2):\n",
    "            p = sns.catplot(x='sc1', y='value', hue=group, hue_order = group_values,\n",
    "                            col='feature', col_wrap=col_wrap, height=height, aspect=aspect,\n",
    "                            scale=0.6,\n",
    "                            palette=colors,\n",
    "                            sharey=False, sharex=False, legend=False, kind=\"point\",\n",
    "                            data=df_melted)\n",
    "\n",
    "            for i, axis in enumerate(p.axes):\n",
    "                axis.set_xlabel('score')\n",
    "                if i == plot_with_legend:\n",
    "                    legend = axis.legend(group_values, title=group, \n",
    "                                         frameon=True, fancybox=True, \n",
    "                                         ncol=1, fontsize=10,\n",
    "                                         loc='upper right', bbox_to_anchor=(1.75, 1))\n",
    "                    for j in range(len(group_values)):\n",
    "                        legend.legendHandles[j].set_color(colors[j])\n",
    "                    plt.setp(legend.get_title(), fontsize='x-small')\n",
    "            \n",
    "            for ax, cname in zip(p.axes, p.col_names):\n",
    "                ax.set_title('\\n'.join(wrap(str(cname), 30)))\n",
    "\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "            imgfile = join(figure_dir, '{}_dff_{}.svg'.format(experiment_id, group))\n",
    "            plt.savefig(imgfile)\n",
    "            if use_thumbnails:\n",
    "                show_thumbnail(imgfile, next(id_generator))\n",
    "            else:\n",
    "                plt.show()\n",
    "    else:\n",
    "        display(Markdown(\"None of the groups in {} had {} or more responses.\".format(group,\n",
    "                                                                                    min_n_per_group[group])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Markdown('Model used: **{}**'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Markdown('Number of features in model: **{}**'.format(len(features_used)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "builtin_ols_models = ['LinearRegression',\n",
    "                      'EqualWeightsLR',\n",
    "                      'RebalancedLR',\n",
    "                      'NNLR',\n",
    "                      'LassoFixedLambdaThenNNLR',\n",
    "                      'LassoFixedLambdaThenLR',\n",
    "                      'PositiveLassoCVThenLR',\n",
    "                      'WeightedLeastSquares']\n",
    "\n",
    "builtin_lasso_models = ['LassoFixedLambda',\n",
    "                        'PositiveLassoCV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we first just show a summary of the OLS model and the main model parameters\n",
    "if model_name in builtin_ols_models:\n",
    "    display(Markdown('### Model summary'))\n",
    "    summary_file = join(output_dir, '{}_ols_summary.txt'.format(experiment_id))\n",
    "    with open(summary_file, 'r') as summf:\n",
    "        model_summary = summf.read()\n",
    "        print(model_summary)\n",
    "     \n",
    "    display(Markdown('### Model fit'))\n",
    "    df_fit = DataReader.read_from_file(join(output_dir, '{}_model_fit.{}'.format(experiment_id,\n",
    "                                                                                 file_format)))\n",
    "    display(HTML(df_fit.to_html(index=False,\n",
    "                                float_format=float_format_func)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Standardized and relative regression coefficients (betas)\n",
    "\n",
    "The relative coefficients are intended to show relative contribution of different feature and their primary purpose is to indentify whether one of the features has an unproportionate effect over the final score. They are computed as standardized/(sum of absolute values of standardized coefficients). \n",
    "\n",
    "Negative standardized coefficients are <span class=\"highlight_color\">highlighted</span>.\n",
    "\n",
    "**Note**: if the model contains negative coefficients, relative values will not sum up to one and their interpretation is generally questionable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_str = \"\"\"\n",
    "**Note**: The coefficients were estimated using LASSO regression. Unlike OLS (standard) linear regression, lasso estimation is based on an optimization routine and therefore the exact estimates may differ across different systems. \"\"\"\n",
    "\n",
    "if model_name in builtin_lasso_models:\n",
    "    display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_betas.sort_values(by='feature', inplace=True)\n",
    "display(HTML(df_betas.to_html(classes=['sortable'], \n",
    "                              index=False, \n",
    "                              escape=False,\n",
    "                              float_format=float_format_func,\n",
    "                              formatters={'standardized': color_highlighter})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here are the same values, shown graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_betas_sorted = df_betas.sort_values(by='standardized', ascending=False)\n",
    "df_betas_sorted.reset_index(drop=True, inplace=True)\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(8, 3)\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "grey_colors = sns.color_palette('Greys', len(features_used))[::-1]\n",
    "with sns.axes_style('whitegrid'):\n",
    "    ax1=fig.add_subplot(121)\n",
    "    sns.barplot(x=\"feature\", y=\"standardized\", data=df_betas_sorted, \n",
    "                order=df_betas_sorted['feature'].values,\n",
    "                palette=sns.color_palette(\"Greys\", 1), ax=ax1)\n",
    "    ax1.set_xticklabels(df_betas_sorted['feature'].values, rotation=90)\n",
    "    ax1.set_title('Values of standardized coefficients')\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_ylabel('')\n",
    "    # no pie chart if we have more than 15 features,\n",
    "    # if the feature names are long (pie chart looks ugly)\n",
    "    # or if there are any negative coefficients.\n",
    "    if len(features_used) <= 15 and longest_feature_name <= 10 and (df_betas_sorted['relative']>=0).all():\n",
    "        ax2=fig.add_subplot(133, aspect=True)\n",
    "        ax2.pie(abs(df_betas_sorted['relative'].values), colors=grey_colors, \n",
    "            labels=df_betas_sorted['feature'].values, normalize=True)\n",
    "        ax2.set_title('Proportional contribution of each feature')\n",
    "    else:\n",
    "        fig.set_size_inches(len(features_used), 3)\n",
    "    betas_file = join(figure_dir, '{}_betas.svg'.format(experiment_id))\n",
    "    plt.savefig(betas_file)\n",
    "\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(betas_file, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if model_name in builtin_ols_models:\n",
    "    display(Markdown('### Model diagnostics'))\n",
    "    display(Markdown(\"These are standard plots for model diagnostics for the main model. All information is computed based on the training set.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the OLS model file and create the diagnostics plots\n",
    "if model_name in builtin_ols_models:\n",
    "    ols_file = join(output_dir, '{}.ols'.format(experiment_id))\n",
    "    model = pickle.load(open(ols_file, 'rb'))\n",
    "    model_predictions = model.predict()\n",
    "\n",
    "    with sns.axes_style('white'):\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        f.set_size_inches((10, 4))\n",
    "        \n",
    "        ###\n",
    "        # for now, we do not show the influence plot since it can be slow to generate\n",
    "        ###\n",
    "        # sm.graphics.influence_plot(model.sm_ols, criterion=\"cooks\", size=10, ax=ax1)\n",
    "        # ax1.set_title('Residuals vs. Leverage', fontsize=16)\n",
    "        # ax1.set_xlabel('Leverage', fontsize=16)\n",
    "        # ax1.set_ylabel('Standardized Residuals', fontsize=16)\n",
    "\n",
    "        sm.qqplot(model.resid, stats.norm, fit=True, line='q', ax=ax1)\n",
    "        ax1.set_title('Normal Q-Q Plot', fontsize=16)\n",
    "        ax1.set_xlabel('Theoretical Quantiles', fontsize=16)\n",
    "        ax1.set_ylabel('Sample Quantiles', fontsize=16)\n",
    "\n",
    "        ax2.scatter(model_predictions, model.resid)\n",
    "        ax2.set_xlabel('Fitted values', fontsize=16)\n",
    "        ax2.set_ylabel('Residuals', fontsize=16)\n",
    "        ax2.set_title('Residuals vs. Fitted', fontsize=16)\n",
    "\n",
    "        imgfile = join(figure_dir, '{}_ols_diagnostic_plots.png'.format(experiment_id))\n",
    "        plt.savefig(imgfile)\n",
    "\n",
    "        if use_thumbnails:\n",
    "            show_thumbnail(imgfile, next(id_generator))\n",
    "        else:\n",
    "            display(Image(imgfile))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links to intermediate files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the hyperlinks below to see the intermediate experiment files generated as part of this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rsmtool.utils.notebook import show_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_files(output_dir, experiment_id, file_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "system_name = platform.system()\n",
    "\n",
    "# People might not know what 'Darwin' is, so we should replace that with 'Mac OS X'\n",
    "if system_name == 'Darwin':\n",
    "    system_name = 'Mac OS X'\n",
    "    \n",
    "# get the architecture\n",
    "architecture = platform.architecture()[0]\n",
    "\n",
    "# get the rsmtool version\n",
    "rsmtool_version_str = '.'.join(map(str, rsmtool_version))\n",
    "\n",
    "display(Markdown('This report was generated using rsmtool v{} on a '\n",
    "                 '{} computer running {}.'.format(rsmtool_version_str, \n",
    "                                                  architecture, \n",
    "                                                  system_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "package_names = '\\n'.join(sorted([\"%s==%s\" % (i.key, i.version) for i in pkg_resources.working_set]))\n",
    "display(HTML('<div id=\"packages\"><pre>{}</pre></div>'.format(package_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "// Code to dynamically generate table of contents at the top of the HTML file\n",
    "var tocEntries = ['<ul>'];\n",
    "var anchors = $('a.anchor-link');\n",
    "var headingTypes = $(anchors).parent().map(function() { return $(this).prop('tagName')});\n",
    "var headingTexts = $(anchors).parent().map(function() { return $(this).text()});\n",
    "var subList = false;\n",
    "\n",
    "$.each(anchors, function(i, anch) {\n",
    "    var hType = headingTypes[i];\n",
    "    var hText = headingTexts[i];\n",
    "    hText = hText.substr(0, hText.length - 1);\n",
    "    if (hType == 'H2') {\n",
    "        if (subList) {\n",
    "            tocEntries.push('</ul>')\n",
    "            subList = false;\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "    else if (hType == 'H3') {\n",
    "        if (!subList) {\n",
    "            subList = true;\n",
    "            tocEntries.push('<ul>')\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "});\n",
    "tocEntries.push('</ul>')\n",
    "$('#toc').html(tocEntries.join(' '))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}