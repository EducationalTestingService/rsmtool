# rsmeval

`rsmeval` evaluates the predictions obtained from an external source and creates a report with all standard evaluations.

Most common use cases:

* Evaluate predictions generated by an external system such as SKLL, WEKA etc. 

* Evaluate new predictions generated by RSMPredict

## Input

`rsmeval` requires the following input:

* File with predictions. The files must be in .csv format. Each row should correspond to a single response and contain the numeric system score predicted for this response. In addition there should be a column with a unique id and a column with human score for each response. The column names are defined in the configuration file. 

* Configuration file in .json format with the settings for `rsmeval`. See [config_file_eval.md](config_file_eval.md) for further information.

## Output

`rsmeval` produces three folders with the following content: 

* `figure/`: figures generated by the analyses

* `output/`:  evaluation outputs in .csv format (see [output_csv.md](output_csv.md) for more information).

* `report/`: evaluation report as .html file and .ipnb python notebook. 

## Usage

`rsmeval config_file [output_directory]`

* `config_file` - the path to the configuration file in .json file with the description of the data and the model. 

* `output-directory` - the directory where `rsmeval` will save its output. 
Default: current folder

* `--force` - by default, `rsmeval` will raise an Exception if you specify an output directory that already contains a directory called `output`. This is to ensure that an older experiment does not interfere with the current one. However, if you specify this flag, `rsmeval` assumes that you are an advanced user who knows what they are doing and only outputs a warning. 
