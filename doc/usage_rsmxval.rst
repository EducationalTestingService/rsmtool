.. _usage_rsmxval:

``rsmxval`` - Run cross-validation experiments
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

RSMTool provides the ``rsmxval`` command-line utility to run cross-validation experiments with scoring models. Why would one want to use cross-validation rather than just using the simple train-and-evaluate loop provided by the ``rsmtool`` utility. Using cross-validation can provide more accurate estimates of scoring model performance since those estimates are averaged over *multiple* train-test splits that are randomly induced from the data. Using a single train-test split may lead to biased estimates of performance since those estimates will depend on the specific characteristics of that split. Using cross-validation is more likely to provide estimates of how well the scoring model will generalize to unseen test data, and more easily flag problems with overfitting and selection bias, if any. 

Cross-validation experiments in RSMTool consist of the following steps:

1. The given training data file is first shuffled randomly (with a fixed seed for reproducibility) and then split into the requested number of folds.  It is also possible for the user to provide a CSV file containing a pre-determined set of folds, e.g., from another part of the data pipeline.

2. For each fold (or train-test split), ``rsmtool`` is run to train a scoring model on the training split and evaluated on the test split. All of the outputs for each of the ``rsmtool`` runs is saved on disk and represents the per-fold performance. 

3. The predictions generated by ``rsmtool`` for each of the folds are all combined into a single file which is then used as input for ``rsmeval``. The output of this evaluation run is saved to disk and provides a more accurate estimate of the predictive performance of a scoring model trained on the given data. 

4. A summary report comparing all of the folds is generated by running ``rsmsummarize`` on all of the fold directories created in the Step 1 and its output is also saved to disk. This summary output can be useful to see if the performance for any of the folds stands out for any reason which could point to a potential problem. 

5. Finally, a scoring model is trained on the complete training data file using ``rsmtool`` which also generates report that contains only the feature and model descriptives. The model is what will most likely be deployed for inference assuming the analyses produced in this step and Steps 1--4 meet the stakeholders' requirements.

.. include:: tutorial_rsmxval.rst

Input
"""""

```rsmxval`` requires a single argument to run an experiment: the path to :ref:`a configuration file <config_file_rsmxval>`. It can also take an output directory as an optional second argument. If the latter is not specified, ``rsmeval`` will use the current directory as the output directory. 

Here are all the arguments to the ``rsmxval`` command-line script.

.. program:: rsmxval

.. option:: config_file

    The :ref:`JSON configuration file <config_file_rsmxval>` for this cross-validation experiment.

.. option:: output_dir (optional)

    The output directory where all the sub-directories and files for this cross-validation experiment will be stored. If a non-empty directory with the same name already exists, an error will be raised.

.. option:: -h, --help

    Show help message and exist.

.. option:: -V, --version

    Show version number and exit.

.. include:: config_rsmxval.rst

.. _output_dirs_rsmxval:

Output
""""""

``rsmxval`` produces a set of folders in the output directory.

folds
~~~~~
This folder contains the output of each of the per-fold ``rsmtool`` experiments. It contains as many sub-folders as the number of specified folds, named ``01``, ``02``, ``03``, etc. Each of these numbered sub-folders contains the output of one ``rsmtool`` experiment conducted using the training split of that fold as the training data and the test split as the evaluation data. Each of the sub-folders contains the :ref:`output directories produced by rsmtool <output_dirs_rsmtool>`. The report for each fold lives in the ``report`` sub-directory, e.g., the report for the first fold is found at``folds/01/report/<experiment_id>_fold01_report.html``, and so on. The messages that are usually printed out by ``rsmtool`` to the screen are instead logged to a file and saved to disk as, e.g., ``folds/01/rsmtool.log``.

evaluation
~~~~~~~~~~
This folder contains the output of the ``rsmeval`` evaluation experiment that uses the cross-validated predictions from each fold. This folder contains the :ref:`output directories produced by rsmeval <output_dirs_rsmeval>`. The evaluation report can be found at ``evaluation/report/<experiment_id>_evaluation_report.html``. The messages that are usually printed out by ``rsmeval`` to the screen are instead logged to a file and saved to disk as ``evaluation/rsmeval.log``.

fold-summary
~~~~~~~~~~~~
This folder contains the output of the ``rsmsummarize`` experiment that provides a quick summary of all of the folds in a single, easy-to-glance-at report. The folder contains the :ref:`output directories produced by rsmsummarize <output_dirs_rsmsummarize>`. The summary report can be found at ``fold-summary/report/<experiment_id>_fold_summary_report.html``. The messages that are usually printed out by ``rsmsummarize`` to the screen are instead logged to a file and saved to disk as ``fold-summary/rsmsummarize.log``.

final-model
~~~~~~~~~~~
This folder contains the output of the ``rsmtool`` experiment that trains a model on the full training data and provides a report showing the feature and model descriptives. It contains the :ref:`output directories produced by rsmtool <output_dirs_rsmtool>`. The primary artifacts of this experiment are the report (``final-model/report/<experiment_id>_model_report.html``) and the final trained model (``final-model/output/<experiment_id>_model.model``). The messages that are usually printed out by ``rsmtool`` to the screen are instead logged to a file and saved to disk as ``final-model/rsmtool.log``.

.. note:: 
    
    Every ``rsmtool`` experiment requires both a training and an evaluation set. However, in this step, we are using the full training data to train the model and ``rsmxval`` does not use a separate test set. Therefore, we simply randomly sample 10% of the full training data as a dummy test set to make sure that ``rsmtool`` runs successfully. The report in this step *only* contains the model and feature descriptives and, therefore, does not use this dummy test set at all. Users should ignore any intermediate files under the ``final-model/output`` and ``final-model/figure`` sub-directories that are derived from this dummy test set. If needed, the data used as the dummy test set can be found at ``final-model/dummy_test.csv`` (or in the :ref:`chosen format <input_file_format>`). 


In addition to these folders, ``rsmxval`` will also save a copy of the :ref:`configuration file <config_file_rsmxval>` in the output directory at the same-level as the above folders. Fields not specified in the original configuration file will be pre-populated with default values. 
